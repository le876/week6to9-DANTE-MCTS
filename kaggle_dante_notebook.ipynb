{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DANTE Rosenbrock ä¼˜åŒ–ä»»åŠ¡ - Kaggleç‰ˆæœ¬\n",
        "\n",
        "## é¡¹ç›®æ¦‚è¿°\n",
        "åŸºäºCNNä»£ç†æ¨¡å‹çš„DANTEä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºè§£å†³20ç»´Rosenbrockå‡½æ•°ä¼˜åŒ–é—®é¢˜ã€‚\n",
        "ä¸»è¦ç›®æ ‡æ˜¯è°ƒè¯•å’Œä¼˜åŒ–NTEæœç´¢å™¨ä¸­çš„åŠ¨æ€æ¢ç´¢å› å­å‚æ•°ã€‚\n",
        "\n",
        "## å½“å‰é…ç½® (å·²æ¢å¤åˆ°åŸé¡¹ç›®è®¾ç½®)\n",
        "- **ç»´åº¦**: 20D Rosenbrockå‡½æ•°\n",
        "- **åˆå§‹æ ·æœ¬**: 800ä¸ª\n",
        "- **è¿­ä»£æ¬¡æ•°**: 400æ¬¡ä¸»åŠ¨å­¦ä¹  (åŸé¡¹ç›®å®Œæ•´è®¾ç½®)\n",
        "- **æ¯æ¬¡è¿­ä»£æ ·æœ¬**: 20ä¸ªæ–°æ ·æœ¬\n",
        "- **CNNè®­ç»ƒè½®æ•°**: 200 epochs (åŸé¡¹ç›®è®¾ç½®)\n",
        "- **é‡ç‚¹è°ƒè¯•**: `dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)` (åŸé¡¹ç›®éªŒè¯çš„æœ€ä½³å…¬å¼)\n",
        "\n",
        "## P100æ˜¾å¡ä¼˜åŒ–\n",
        "- ä¸“é—¨é’ˆå¯¹Kaggle P100æ˜¾å¡è¿›è¡Œäº†æ€§èƒ½ä¼˜åŒ–\n",
        "- è™šæ‹ŸGPUå†…å­˜é™åˆ¶: 15GB\n",
        "- æ··åˆç²¾åº¦è®­ç»ƒ: mixed_float16\n",
        "- XLAç¼–è¯‘ä¼˜åŒ–: å·²å¯ç”¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: ç¯å¢ƒè®¾ç½®ä¸GPUé…ç½®\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import torch\n",
        "import random\n",
        "from scipy.stats import pearsonr\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "import psutil\n",
        "\n",
        "# GPUé…ç½®ä¼˜åŒ–\n",
        "print(\"=== GPUç¯å¢ƒé…ç½® ===\")\n",
        "\n",
        "# TensorFlow GPUè®¾ç½®\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # å¯ç”¨å†…å­˜å¢é•¿\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"TensorFlowæ£€æµ‹åˆ°{len(gpus)}ä¸ªGPUè®¾å¤‡\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"  GPU {i}: {gpu}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPUè®¾ç½®é”™è¯¯: {e}\")\n",
        "else:\n",
        "    print(\"æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPU\")\n",
        "\n",
        "# PyTorch GPUè®¾ç½®\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(f\"PyTorchå°†ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "    print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
        "    print(f\"GPUè®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(f\"PyTorchå°†ä½¿ç”¨CPU\")\n",
        "\n",
        "# å¯è§†åŒ–è®¾ç½®\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 150\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'  # ä½¿ç”¨é»˜è®¤å­—ä½“é¿å…ä¸­æ–‡é—®é¢˜\n",
        "\n",
        "# é…ç½®æ—¥å¿—è®°å½•å™¨\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"ç¯å¢ƒé…ç½®å®Œæˆ\")\n",
        "\n",
        "# Kaggleç¯å¢ƒè·¯å¾„è®¾ç½® - æ›´æ–°ä¸ºå®é™…æ•°æ®é›†è·¯å¾„\n",
        "DATA_PATH = \"/kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw\"\n",
        "RESULTS_PATH = \"/kaggle/working/results\"\n",
        "\n",
        "# åˆ›å»ºç»“æœç›®å½•\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "logger.info(f\"æ•°æ®è·¯å¾„: {DATA_PATH}\")\n",
        "logger.info(f\"ç»“æœè·¯å¾„: {RESULTS_PATH}\")\n",
        "\n",
        "# éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "data_files = [\n",
        "    \"Rosenbrock_x_train.npy\",\n",
        "    \"Rosenbrock_y_train.npy\", \n",
        "    \"Rosenbrock_x_test.npy\",\n",
        "    \"Rosenbrock_y_test.npy\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== æ•°æ®æ–‡ä»¶æ£€æŸ¥ ===\")\n",
        "for file in data_files:\n",
        "    file_path = os.path.join(DATA_PATH, file)\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"âœ… {file} - æ‰¾åˆ°\")\n",
        "        if file.endswith('.npy'):\n",
        "            data_shape = np.load(file_path).shape\n",
        "            print(f\"   æ•°æ®å½¢çŠ¶: {data_shape}\")\n",
        "    else:\n",
        "        print(f\"âŒ {file} - æœªæ‰¾åˆ°\")\n",
        "        logger.warning(f\"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}\")\n",
        "\n",
        "print(\"\\n=== ç¯å¢ƒé…ç½®å®Œæˆ ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: æ ¸å¿ƒå·¥å…·å‡½æ•° (utils.py)\n",
        "import numpy as np\n",
        "\n",
        "def rosenbrock_function(x):\n",
        "    \"\"\"\n",
        "    Calculates the Rosenbrock function value for a given input vector x.\n",
        "    The function is typically evaluated on the hypercube xi âˆˆ [-5, 10] for all i, \n",
        "    although it may be evaluated for xi âˆˆ [-2.048, 2.048] as well.\n",
        "    For our DANTE optimization, we usually normalize inputs to [0,1] or work within specific bounds.\n",
        "    Args:\n",
        "        x (np.ndarray): Input vector of shape (n,) or (batch_size, n).\n",
        "    Returns:\n",
        "        float or np.ndarray: Rosenbrock function value(s).\n",
        "    \"\"\"\n",
        "    if not isinstance(x, np.ndarray):\n",
        "        x = np.array(x)\n",
        "\n",
        "    if x.ndim == 1:\n",
        "        # Single input vector\n",
        "        n = len(x)\n",
        "        if n < 2:\n",
        "            raise ValueError(\"Rosenbrock function requires at least 2 dimensions.\")\n",
        "        sum_val = 0\n",
        "        for i in range(n - 1):\n",
        "            sum_val += 100 * (x[i+1] - x[i]**2)**2 + (x[i] - 1)**2\n",
        "        return sum_val\n",
        "    elif x.ndim == 2:\n",
        "        # Batch of input vectors\n",
        "        n = x.shape[1]\n",
        "        if n < 2:\n",
        "            raise ValueError(\"Rosenbrock function requires at least 2 dimensions for each vector in the batch.\")\n",
        "        results = [] \n",
        "        for i in range(x.shape[0]):\n",
        "            sum_val = 0\n",
        "            for j in range(n - 1):\n",
        "                sum_val += 100 * (x[i, j+1] - x[i, j]**2)**2 + (x[i, j] - 1)**2\n",
        "            results.append(sum_val)\n",
        "        return np.array(results)\n",
        "    else:\n",
        "        raise ValueError(\"Input x must be a 1D or 2D NumPy array.\")\n",
        "\n",
        "\n",
        "def get_rosenbrock_bounds(dimension):\n",
        "    \"\"\"\n",
        "    Returns the typical bounds for the Rosenbrock function for a given dimension.\n",
        "    Often [-5, 10] or [-2.048, 2.048]. Let's use [-5, 10] for now.\n",
        "    \"\"\"\n",
        "    return [(-5.0, 10.0)] * dimension\n",
        "\n",
        "# å¤–éƒ¨è¯„ä¼°å™¨\n",
        "def rosenbrock_evaluator(x_vector):\n",
        "    \"\"\"è®¡ç®—ç»™å®šå‘é‡xçš„Rosenbrockå‡½æ•°å€¼ã€‚\"\"\"\n",
        "    return rosenbrock_function(x_vector)\n",
        "\n",
        "# å®šä¹‰LOG_EPSILONå¸¸é‡\n",
        "LOG_EPSILON = 1.0  # ä½¿ç”¨10ä¸ºåº•çš„å¯¹æ•°å˜æ¢: log10(y + 1)\n",
        "\n",
        "# NTEç›¸å…³å¸¸é‡\n",
        "def safe_power10(log_values):\n",
        "    \"\"\"å®‰å…¨åœ°è®¡ç®—10çš„å¹‚ï¼Œé¿å…æº¢å‡º\"\"\"\n",
        "    # é™åˆ¶è¾“å…¥èŒƒå›´ä»¥é¿å…æº¢å‡º\n",
        "    log_values_clipped = np.clip(log_values, -10, 10)\n",
        "    return 10.0 ** log_values_clipped - LOG_EPSILON\n",
        "\n",
        "NTE_C0 = 1\n",
        "NTE_ROLLOUT_ROUNDS = 100\n",
        "\n",
        "logger.info(\"æ ¸å¿ƒå·¥å…·å‡½æ•°åŠ è½½å®Œæˆ\")\n",
        "print(\"æµ‹è¯•Rosenbrockå‡½æ•°...\")\n",
        "test_x = np.array([1, 1, 1])\n",
        "test_result = rosenbrock_function(test_x)\n",
        "print(f\"Rosenbrock([1,1,1]) = {test_result} (æœŸæœ›å€¼: 0)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: æ•°æ®ç®¡ç†æ¨¡å— (data_manager.py)\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self, initial_X=None, initial_y=None, dimension=None, data_file_path=None):\n",
        "        \"\"\"\n",
        "        Initializes the DataManager.\n",
        "        Args:\n",
        "            initial_X (np.ndarray, optional): Initial X data. Defaults to None.\n",
        "            initial_y (np.ndarray, optional): Initial y data (original scale). Defaults to None.\n",
        "            dimension (int, optional): Dimension of the X data.\n",
        "            data_file_path (str, optional): Path to an .npz file to load data from.\n",
        "        \"\"\"\n",
        "        self.X_data = np.array([])\n",
        "        self.y_data_orig = np.array([])  # Stores original y values\n",
        "        self.y_data_log = np.array([])   # Stores log-transformed y values\n",
        "        self.dimension = dimension\n",
        "        self.mean_X = None\n",
        "        self.std_X = None\n",
        "        \n",
        "        # æ·»åŠ åŸŸèŒƒå›´å±æ€§ï¼Œç”¨äºNTEæœç´¢\n",
        "        self.x_min = -2.048  # Rosenbrockå‡½æ•°çš„é»˜è®¤ä¸‹é™\n",
        "        self.x_max = 2.048   # Rosenbrockå‡½æ•°çš„é»˜è®¤ä¸Šé™\n",
        "        \n",
        "        # æ·»åŠ æ ·æœ¬æƒé‡å±æ€§\n",
        "        self.last_batch_weights = None\n",
        "        \n",
        "        # æ·»åŠ æ•°æ®ç¼“å­˜\n",
        "        self._normalized_cache = {}\n",
        "        self._cache_hash = None\n",
        "\n",
        "        if data_file_path and os.path.exists(data_file_path):\n",
        "            logger.info(f\"Loading data from {data_file_path}\")\n",
        "            try:\n",
        "                data = np.load(data_file_path)\n",
        "                self.X_data = data.get('X_data', np.array([]))\n",
        "                loaded_y_orig = data.get('y_data', np.array([])) \n",
        "                \n",
        "                if self.X_data.ndim == 1 and self.X_data.shape[0] > 0:\n",
        "                    self.X_data = self.X_data.reshape(1, -1)\n",
        "                if loaded_y_orig.ndim == 1 and loaded_y_orig.shape[0] > 0:\n",
        "                     loaded_y_orig = loaded_y_orig.reshape(-1,1)\n",
        "\n",
        "                self.y_data_orig = loaded_y_orig\n",
        "                if self.y_data_orig.shape[0] > 0:\n",
        "                    self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "                else:\n",
        "                    self.y_data_log = np.empty((0,1))\n",
        "\n",
        "                self.mean_X = data.get('mean_X', None)\n",
        "                self.std_X = data.get('std_X', None)\n",
        "                \n",
        "                if self.X_data.shape[0] > 0:\n",
        "                    self.dimension = self.X_data.shape[1]\n",
        "                    logger.info(f\"Loaded {self.X_data.shape[0]} samples.\")\n",
        "                    if self.mean_X is not None and self.std_X is not None:\n",
        "                        logger.info(\"Loaded existing normalization parameters for X.\")\n",
        "                    else:\n",
        "                        logger.info(\"Normalization parameters not found, will compute them.\")\n",
        "                        self.normalize_X()\n",
        "                else:\n",
        "                    logger.warning(f\"No data found in {data_file_path} or data is empty.\")\n",
        "                    if initial_X is not None and initial_y is not None:\n",
        "                        logger.info(\"Falling back to provided initial_X and initial_y.\")\n",
        "                        self.X_data = initial_X.copy()\n",
        "                        self.y_data_orig = initial_y.copy()\n",
        "                        if self.y_data_orig.ndim == 1: \n",
        "                            self.y_data_orig = self.y_data_orig.reshape(-1, 1)\n",
        "                        self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "                        if self.dimension is None and self.X_data.shape[0] > 0:\n",
        "                            self.dimension = self.X_data.shape[1]\n",
        "                        self.normalize_X()\n",
        "                    elif self.dimension is None:\n",
        "                        raise ValueError(\"Dimension must be provided if not loading from file and no initial data.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading data from {data_file_path}: {e}\")\n",
        "                if initial_X is not None and initial_y is not None:\n",
        "                    logger.info(\"Falling back to provided initial_X and initial_y due to loading error.\")\n",
        "                    self.X_data = initial_X.copy()\n",
        "                    self.y_data_orig = initial_y.copy()\n",
        "                    if self.y_data_orig.ndim == 1: \n",
        "                        self.y_data_orig = self.y_data_orig.reshape(-1, 1)\n",
        "                    self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "                    if self.dimension is None and self.X_data.shape[0] > 0:\n",
        "                        self.dimension = self.X_data.shape[1]\n",
        "                    self.normalize_X()\n",
        "                elif self.dimension is None:\n",
        "                    raise ValueError(\"Dimension must be provided if loading fails and no initial data.\")\n",
        "        \n",
        "        elif initial_X is not None and initial_y is not None:\n",
        "            logger.info(\"Initializing with provided initial_X and initial_y.\")\n",
        "            self.X_data = initial_X.copy()\n",
        "            self.y_data_orig = initial_y.copy()\n",
        "            if self.X_data.ndim == 1: \n",
        "                self.X_data = self.X_data.reshape(1, -1)\n",
        "            if self.y_data_orig.ndim == 1: \n",
        "                self.y_data_orig = self.y_data_orig.reshape(-1, 1)\n",
        "            self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "            if self.dimension is None and self.X_data.shape[0] > 0:\n",
        "                 self.dimension = self.X_data.shape[1]\n",
        "            self.normalize_X()\n",
        "        elif self.dimension is None:\n",
        "             raise ValueError(\"Dimension must be provided if no data source is specified.\")\n",
        "        else:\n",
        "            logger.info(f\"DataManager initialized empty. Dimension set to {self.dimension}.\")\n",
        "            self.X_data = np.empty((0, self.dimension if self.dimension else 0))\n",
        "            self.y_data_orig = np.empty((0, 1))\n",
        "            self.y_data_log = np.empty((0, 1))\n",
        "            \n",
        "        self._update_best()\n",
        "\n",
        "    def normalize_X(self):\n",
        "        \"\"\"æ”¹è¿›çš„è§„èŒƒåŒ–æ–¹æ³•ï¼Œä½¿ç”¨é²æ£’æ€§ç¼©æ”¾ä»¥å¤„ç†å¼‚å¸¸å€¼\"\"\"\n",
        "        if self.X_data.shape[0] > 1:\n",
        "            self.mean_X = np.mean(self.X_data, axis=0)\n",
        "            self.std_X = np.std(self.X_data, axis=0)\n",
        "            \n",
        "            # å¤„ç†è¿‘é›¶æ–¹å·®çš„ç‰¹å¾\n",
        "            epsilon = 1e-6\n",
        "            self.std_X = np.where(self.std_X < epsilon, 1.0, self.std_X)\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦é²æ£’æ€§å¤„ç†çš„æç«¯å€¼\n",
        "            X_normalized = (self.X_data - self.mean_X) / self.std_X\n",
        "            extremes = np.abs(X_normalized) > 5.0\n",
        "            \n",
        "            if np.any(extremes):\n",
        "                # ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•å¤„ç†æç«¯å€¼\n",
        "                q1 = np.percentile(self.X_data, 25, axis=0)\n",
        "                q3 = np.percentile(self.X_data, 75, axis=0)\n",
        "                iqr = q3 - q1\n",
        "                \n",
        "                # å°†IQRä¸º0çš„ç»´åº¦è®¾ä¸º1ï¼Œé˜²æ­¢é™¤é›¶\n",
        "                iqr = np.where(iqr < epsilon, 1.0, iqr)\n",
        "                \n",
        "                # åŸºäºIQRçš„è§„èŒƒåŒ–å‚æ•°\n",
        "                self.mean_X = np.median(self.X_data, axis=0)\n",
        "                self.std_X = iqr / 1.35\n",
        "                \n",
        "                logger.info(\"ä½¿ç”¨é²æ£’æ€§è§„èŒƒåŒ– (åŸºäºåˆ†ä½æ•°) å¤„ç†æç«¯å€¼\")\n",
        "            else:\n",
        "                logger.info(\"ä½¿ç”¨æ ‡å‡†è§„èŒƒåŒ– (å‡å€¼å’Œæ ‡å‡†å·®)\")\n",
        "        elif self.X_data.shape[0] == 1:\n",
        "            self.mean_X = self.X_data[0]\n",
        "            self.std_X = np.ones_like(self.X_data[0])\n",
        "            logger.info(\"X normalization parameters set for single data point.\")\n",
        "        else:\n",
        "            self.mean_X = None\n",
        "            self.std_X = None\n",
        "            logger.info(\"No data to calculate X normalization parameters.\")\n",
        "            \n",
        "    def get_normalized_X(self, X_samples):\n",
        "        \"\"\"ä½¿ç”¨å­˜å‚¨çš„å‚æ•°å¯¹Xæ ·æœ¬è¿›è¡Œè§„èŒƒåŒ–ï¼Œæ”¯æŒç¼“å­˜ä¼˜åŒ–\"\"\"\n",
        "        # ç”Ÿæˆæ•°æ®å“ˆå¸Œç”¨äºç¼“å­˜\n",
        "        data_hash = hashlib.md5(X_samples.tobytes()).hexdigest()\n",
        "        \n",
        "        # æ£€æŸ¥ç¼“å­˜\n",
        "        if data_hash in self._normalized_cache:\n",
        "            logger.debug(\"ä½¿ç”¨ç¼“å­˜çš„è§„èŒƒåŒ–æ•°æ®\")\n",
        "            return self._normalized_cache[data_hash]\n",
        "        \n",
        "        if self.mean_X is None or self.std_X is None:\n",
        "            if self.X_data.shape[0] > 0:\n",
        "                 logger.warning(\"Normalization params not available, attempting to calculate them now.\")\n",
        "                 self.normalize_X()\n",
        "                 if self.mean_X is None or self.std_X is None:\n",
        "                    raise ValueError(\"Normalization parameters could not be calculated.\")\n",
        "            else:\n",
        "                logger.warning(\"No stored normalization parameters. Normalizing based on input samples.\")\n",
        "                if X_samples.shape[0] > 1:\n",
        "                    mean_temp = np.mean(X_samples, axis=0)\n",
        "                    std_temp = np.std(X_samples, axis=0)\n",
        "                    std_temp[std_temp < 1e-6] = 1.0\n",
        "                    normalized = (X_samples - mean_temp) / std_temp\n",
        "                else:\n",
        "                    normalized = np.zeros_like(X_samples)\n",
        "                \n",
        "                # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰\n",
        "                if len(self._normalized_cache) < 10:\n",
        "                    self._normalized_cache[data_hash] = normalized.copy()\n",
        "                return normalized\n",
        "\n",
        "        # åº”ç”¨è§„èŒƒåŒ–ï¼Œå¹¶æˆªæ–­æç«¯å€¼åˆ°åˆç†èŒƒå›´\n",
        "        normalized = (X_samples - self.mean_X) / self.std_X\n",
        "        normalized = np.clip(normalized, -10.0, 10.0)\n",
        "        \n",
        "        # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰\n",
        "        if len(self._normalized_cache) < 10:\n",
        "            self._normalized_cache[data_hash] = normalized.copy()\n",
        "        \n",
        "        return normalized\n",
        "\n",
        "    def get_train_data_for_cnn(self):\n",
        "        if self.X_data.shape[0] == 0:\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "        \n",
        "        normalized_X = self.get_normalized_X(self.X_data)\n",
        "        \n",
        "        if self.dimension != 20:\n",
        "            raise ValueError(f\"DataManager expected dimension 20 for CNN reshape, got {self.dimension}\")\n",
        "        \n",
        "        reshaped_X = normalized_X.reshape(-1, 1, 4, 5)\n",
        "        \n",
        "        # Return log-transformed y for training\n",
        "        return torch.tensor(reshaped_X, dtype=torch.float32), torch.tensor(self.y_data_log, dtype=torch.float32)\n",
        "\n",
        "    def add_samples(self, new_X, new_y_orig, re_normalize=False, shuffle=True):\n",
        "        \"\"\"æ·»åŠ æ–°æ ·æœ¬åˆ°æ•°æ®é›†ä¸­\"\"\"\n",
        "        if new_X.ndim == 1: \n",
        "            new_X = new_X.reshape(1, -1)\n",
        "        if new_y_orig.ndim == 1: \n",
        "            new_y_orig = new_y_orig.reshape(-1, 1)\n",
        "        \n",
        "        new_y_log = np.log10(new_y_orig + LOG_EPSILON)\n",
        "\n",
        "        if self.X_data.shape[0] == 0:\n",
        "            self.X_data = new_X.copy()\n",
        "            self.y_data_orig = new_y_orig.copy()\n",
        "            self.y_data_log = new_y_log.copy()\n",
        "            if self.dimension is None: \n",
        "                self.dimension = new_X.shape[1]\n",
        "        else:\n",
        "            if new_X.shape[1] != self.dimension:\n",
        "                raise ValueError(f\"New X samples have dimension {new_X.shape[1]}, expected {self.dimension}\")\n",
        "            \n",
        "            # å…ˆåˆå¹¶æ•°æ®\n",
        "            self.X_data = np.vstack((self.X_data, new_X))\n",
        "            self.y_data_orig = np.vstack((self.y_data_orig, new_y_orig))\n",
        "            self.y_data_log = np.vstack((self.y_data_log, new_y_log))\n",
        "            \n",
        "            # å¦‚æœéœ€è¦æ‰“ä¹±ï¼Œåˆ™ç”Ÿæˆéšæœºæ’åˆ—\n",
        "            if shuffle:\n",
        "                n_samples = self.X_data.shape[0]\n",
        "                permutation = np.random.permutation(n_samples)\n",
        "                self.X_data = self.X_data[permutation]\n",
        "                self.y_data_orig = self.y_data_orig[permutation]\n",
        "                self.y_data_log = self.y_data_log[permutation]\n",
        "                logger.info(f\"æ•°æ®é›†å·²æ‰“ä¹±ï¼Œç¡®ä¿æ–°æ ·æœ¬éšæœºåˆ†å¸ƒ\")\n",
        "        \n",
        "        if re_normalize or self.mean_X is None:\n",
        "            self.normalize_X()\n",
        "            \n",
        "        self._update_best()\n",
        "        logger.info(f\"Added {new_X.shape[0]} new samples. Total samples: {self.X_data.shape[0]}\")\n",
        "\n",
        "    def _update_best(self):\n",
        "        if self.y_data_orig.shape[0] > 0:\n",
        "            min_idx = np.argmin(self.y_data_orig)\n",
        "            self.best_y = self.y_data_orig[min_idx, 0]\n",
        "            self.best_x = self.X_data[min_idx]\n",
        "        else:\n",
        "            self.best_y = float('inf')\n",
        "            self.best_x = None\n",
        "\n",
        "    def get_current_best(self):\n",
        "        return self.best_x, self.best_y\n",
        "\n",
        "    def save_data(self, file_path):\n",
        "        \"\"\"ä¿å­˜å½“å‰æ•°æ®å’Œè§„èŒƒåŒ–å‚æ•°åˆ°.npzæ–‡ä»¶\"\"\"\n",
        "        try:\n",
        "            save_dict = {\n",
        "                'X_data': self.X_data,\n",
        "                'y_data': self.y_data_orig\n",
        "            }\n",
        "            if self.mean_X is not None:\n",
        "                save_dict['mean_X'] = self.mean_X\n",
        "            if self.std_X is not None:\n",
        "                save_dict['std_X'] = self.std_X\n",
        "            \n",
        "            np.savez(file_path, **save_dict)\n",
        "            logger.info(f\"DataManager state saved to {file_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving DataManager state to {file_path}: {e}\")\n",
        "\n",
        "    @property\n",
        "    def num_samples(self):\n",
        "        return self.X_data.shape[0]\n",
        "\n",
        "    def get_sorted_data_by_y(self, top_n=None):\n",
        "        \"\"\"è·å–æŒ‰yå€¼æ’åºçš„æ•°æ®ï¼ˆå‡åºï¼Œå³æœ€ä¼˜å…ˆï¼‰\"\"\"\n",
        "        if self.X_data.shape[0] == 0:\n",
        "            return np.array([]), np.array([])\n",
        "            \n",
        "        sorted_indices = np.argsort(self.y_data_orig, axis=0).flatten()\n",
        "        \n",
        "        sorted_X = self.X_data[sorted_indices]\n",
        "        sorted_y_orig = self.y_data_orig[sorted_indices]\n",
        "        \n",
        "        if top_n is not None:\n",
        "            return sorted_X[:top_n], sorted_y_orig[:top_n]\n",
        "        else:\n",
        "            return sorted_X, sorted_y_orig\n",
        "    \n",
        "    def get_best_samples_range(self, n_best=60):\n",
        "        \"\"\"è·å–æœ€ä¼˜çš„Nä¸ªæ ·æœ¬çš„åŸå§‹Yå€¼çš„èŒƒå›´\"\"\"\n",
        "        if self.num_samples == 0:\n",
        "            return float('nan'), float('nan')\n",
        "        \n",
        "        _, sorted_y_orig = self.get_sorted_data_by_y(top_n=n_best)\n",
        "        \n",
        "        if sorted_y_orig.shape[0] == 0:\n",
        "            return float('nan'), float('nan')\n",
        "            \n",
        "        min_y = np.min(sorted_y_orig)\n",
        "        max_y = np.max(sorted_y_orig)\n",
        "        return min_y, max_y\n",
        "\n",
        "    def get_all_y_orig(self):\n",
        "        \"\"\"Returns all original y data.\"\"\"\n",
        "        return self.y_data_orig.copy()\n",
        "\n",
        "    def get_all_y_log(self):\n",
        "        \"\"\"Returns all log-transformed y data.\"\"\"\n",
        "        return self.y_data_log.copy()\n",
        "\n",
        "    def get_all_x_orig(self):\n",
        "        \"\"\"Returns all original X data.\"\"\"\n",
        "        return self.X_data.copy()\n",
        "    \n",
        "    def clear_cache(self):\n",
        "        \"\"\"æ¸…ç†æ•°æ®ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜\"\"\"\n",
        "        self._normalized_cache.clear()\n",
        "        self._cache_hash = None\n",
        "        logger.info(\"å·²æ¸…ç†DataManagerç¼“å­˜\")\n",
        "\n",
        "logger.info(\"DataManagerç±»åŠ è½½å®Œæˆ\")\n",
        "print(\"DataManageræ¨¡å—å·²å‡†å¤‡å°±ç»ª\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: æ€§èƒ½ä¼˜åŒ–æ¨¡å— (ç®€åŒ–ç‰ˆ)\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "class PerformanceOptimizer:\n",
        "    \"\"\"æ€§èƒ½ä¼˜åŒ–å™¨ï¼Œè´Ÿè´£ä¼˜åŒ–TensorFlowè®­ç»ƒæ€§èƒ½\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cpu_count = psutil.cpu_count(logical=True)\n",
        "        self.physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "        self.memory_cache = {}\n",
        "        self.is_configured = False\n",
        "        logger.info(f\"æ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•°: ç‰©ç†={self.physical_cpu_count}, é€»è¾‘={self.cpu_count}\")\n",
        "    \n",
        "    def configure_tensorflow_performance(self):\n",
        "        \"\"\"é…ç½®TensorFlowæ€§èƒ½è®¾ç½®\"\"\"\n",
        "        if self.is_configured:\n",
        "            return\n",
        "            \n",
        "        logger.info(\"å¼€å§‹é…ç½®TensorFlowæ€§èƒ½ä¼˜åŒ–...\")\n",
        "        \n",
        "        # é…ç½®GPUå†…å­˜å¢é•¿\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                logger.info(f\"å·²é…ç½®GPUå†…å­˜å¢é•¿ï¼Œæ£€æµ‹åˆ°{len(gpus)}ä¸ªGPU\")\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPUå†…å­˜é…ç½®å¤±è´¥: {e}\")\n",
        "        \n",
        "        # é…ç½®CPUå¹¶è¡Œåº¦\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(self.cpu_count)\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(self.physical_cpu_count)\n",
        "        \n",
        "        # é…ç½®ç¯å¢ƒå˜é‡\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "        os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "        \n",
        "        self.is_configured = True\n",
        "        logger.info(\"TensorFlowæ€§èƒ½ä¼˜åŒ–é…ç½®å®Œæˆ\")\n",
        "    \n",
        "    def create_optimized_dataset(self, X_data, y_data, batch_size=32, shuffle=True, cache=True):\n",
        "        \"\"\"åˆ›å»ºä¼˜åŒ–çš„tf.data.Dataset\"\"\"\n",
        "        logger.info(f\"åˆ›å»ºä¼˜åŒ–çš„æ•°æ®é›†ï¼Œæ ·æœ¬æ•°: {len(X_data)}, æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
        "        \n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
        "        \n",
        "        if cache and len(X_data) < 50000:\n",
        "            dataset = dataset.cache()\n",
        "            logger.info(\"å·²å¯ç”¨æ•°æ®é›†å†…å­˜ç¼“å­˜\")\n",
        "        \n",
        "        if shuffle:\n",
        "            buffer_size = min(len(X_data), 10000)\n",
        "            dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "        \n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "        return dataset\n",
        "    \n",
        "    def clear_memory_cache(self):\n",
        "        \"\"\"æ¸…ç†å†…å­˜ç¼“å­˜\"\"\"\n",
        "        self.memory_cache.clear()\n",
        "        gc.collect()\n",
        "        logger.info(\"å·²æ¸…ç†å†…å­˜ç¼“å­˜\")\n",
        "    \n",
        "    def cache_data(self, key, data):\n",
        "        \"\"\"ç¼“å­˜æ•°æ®åˆ°å†…å­˜\"\"\"\n",
        "        self.memory_cache[key] = data\n",
        "    \n",
        "    def get_cached_data(self, key):\n",
        "        \"\"\"ä»ç¼“å­˜è·å–æ•°æ®\"\"\"\n",
        "        return self.memory_cache.get(key, None)\n",
        "    \n",
        "    def optimize_memory_usage(self):\n",
        "        \"\"\"ä¼˜åŒ–å†…å­˜ä½¿ç”¨\"\"\"\n",
        "        gc.collect()\n",
        "        if tf.config.experimental.list_physical_devices('GPU'):\n",
        "            try:\n",
        "                tf.keras.backend.clear_session()\n",
        "            except:\n",
        "                pass\n",
        "        logger.info(\"å·²æ‰§è¡Œå†…å­˜ä¼˜åŒ–æ¸…ç†\")\n",
        "    \n",
        "    def monitor_performance(self, stage_name=\"\"):\n",
        "        \"\"\"ç›‘æ§æ€§èƒ½æŒ‡æ ‡\"\"\"\n",
        "        try:\n",
        "            process = psutil.Process()\n",
        "            memory_info = process.memory_info()\n",
        "            memory_mb = memory_info.rss / 1024 / 1024\n",
        "            logger.info(f\"[{stage_name}] å†…å­˜ä½¿ç”¨: {memory_mb:.1f}MB\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"æ€§èƒ½ç›‘æ§å¤±è´¥: {e}\")\n",
        "\n",
        "# åˆ›å»ºå…¨å±€æ€§èƒ½ä¼˜åŒ–å™¨å®ä¾‹\n",
        "performance_optimizer = PerformanceOptimizer()\n",
        "\n",
        "logger.info(\"æ€§èƒ½ä¼˜åŒ–æ¨¡å—åŠ è½½å®Œæˆ\")\n",
        "print(\"æ€§èƒ½ä¼˜åŒ–æ¨¡å—å·²å‡†å¤‡å°±ç»ª\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: NTE Searcher (å®Œæ•´ç‰ˆ) - å…³é”®è°ƒè¯•æ¨¡å—\n",
        "\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "from typing import List, Tuple, Dict, Any, Union, Callable\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# NTEæœç´¢å™¨å¸¸é‡\n",
        "NTE_C0 = 1\n",
        "NTE_ROLLOUT_ROUNDS = 100\n",
        "VALIDATION_BATCH_SIZE_NTE = 20\n",
        "\n",
        "def safe_power10(log_values, clip_min=-30, clip_max=25):\n",
        "    \"\"\"å®‰å…¨åœ°è®¡ç®—10çš„å¹‚ï¼Œé¿å…æ•°å€¼æº¢å‡º\"\"\"\n",
        "    clipped_values = np.clip(log_values, clip_min, clip_max)\n",
        "    return 10.0 ** clipped_values\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    \"\"\"NTEæœç´¢æ ‘çš„èŠ‚ç‚¹\"\"\"\n",
        "    state: Tuple[float, ...]  # çŠ¶æ€å‘é‡ï¼ˆå…ƒç»„ç”¨äºå“ˆå¸Œï¼‰\n",
        "    value_pred: float = float('inf')  # æ›¿ä»£æ¨¡å‹é¢„æµ‹å€¼\n",
        "    visit_count: int = 0  # è®¿é—®æ¬¡æ•°\n",
        "\n",
        "class NTESearcher:\n",
        "    \"\"\"Neural Tree Expansionæœç´¢å™¨ - å®Œæ•´å®ç°ï¼ŒåŒ…å«æ‰€æœ‰åŸå§‹åŠŸèƒ½\"\"\"\n",
        "    \n",
        "    def __init__(self, dimension, domain=None, domain_mins=None, domain_maxs=None, \n",
        "                 c0=NTE_C0, rollout_rounds=NTE_ROLLOUT_ROUNDS, \n",
        "                 validation_batch_size=VALIDATION_BATCH_SIZE_NTE,\n",
        "                 rho_scaling_factor_heuristic=lambda min_y: 1.0):\n",
        "        \"\"\"åˆå§‹åŒ–NTEæœç´¢å™¨\"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.base_c0 = c0\n",
        "        self.rollout_rounds = rollout_rounds\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "        self.rho_scaling_factor_heuristic = rho_scaling_factor_heuristic\n",
        "        \n",
        "        # å¤„ç†åŸŸè¾¹ç•Œ - ä¿æŒå‘åå…¼å®¹\n",
        "        if domain_mins is not None and domain_maxs is not None:\n",
        "            self.domain_mins = np.array(domain_mins)\n",
        "            self.domain_maxs = np.array(domain_maxs)\n",
        "        elif domain is not None:\n",
        "            domain_array = np.array(domain)\n",
        "            self.domain_mins = domain_array[:, 0]\n",
        "            self.domain_maxs = domain_array[:, 1]\n",
        "        else:\n",
        "            self.domain_mins = np.array([-2.048] * dimension)\n",
        "            self.domain_maxs = np.array([2.048] * dimension)\n",
        "            \n",
        "        # çŠ¶æ€è¿½è¸ªå˜é‡\n",
        "        self.last_move_successful = False\n",
        "        self.last_move_deterministic = False\n",
        "        self.last_move_type = None\n",
        "        self.last_move_direction = None\n",
        "        self.last_move_dim_index = -1\n",
        "\n",
        "print(\"âœ“ NTE Searcher (å®Œæ•´ç‰ˆç¬¬1éƒ¨åˆ†) å·²åŠ è½½\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: NTE Searcher å®Œæ•´å®ç° - æ··åˆæ‰©å±•ç­–ç•¥\n",
        "\n",
        "def _calculate_dynamic_search_space(self, all_states=None, all_values=None, iteration=1):\n",
        "    \"\"\"è®¡ç®—åŠ¨æ€æœç´¢ç©ºé—´ï¼Œè€ƒè™‘å·²çŸ¥æ ·æœ¬åˆ†å¸ƒå’Œè¿­ä»£è¿›åº¦\"\"\"\n",
        "    if all_states is None or len(all_states) == 0:\n",
        "        return self.domain_mins.copy(), self.domain_maxs.copy()\n",
        "        \n",
        "    # åŸºäºå½“å‰æ•°æ®é›†åŠ¨æ€è°ƒæ•´æœç´¢èŒƒå›´\n",
        "    data_mins = np.min(all_states, axis=0)\n",
        "    data_maxs = np.max(all_states, axis=0)\n",
        "    data_ranges = data_maxs - data_mins\n",
        "    \n",
        "    # è®¡ç®—æ‰©å±•å› å­ï¼šæ—©æœŸè¿­ä»£æ›´å¹¿æ³›æ¢ç´¢ï¼ŒåæœŸæ›´é›†ä¸­\n",
        "    base_expansion = 0.2\n",
        "    iteration_factor = max(0.1, 1.0 - 0.01 * iteration)\n",
        "    expansion_factor = base_expansion * iteration_factor\n",
        "    \n",
        "    # åŸºäºå‡½æ•°å€¼åˆ†å¸ƒè°ƒæ•´æœç´¢åŒºåŸŸ\n",
        "    if all_values is not None and len(all_values) > 0:\n",
        "        # æ‰¾åˆ°æœ€ä¼˜10%çš„æ ·æœ¬ï¼Œé‡ç‚¹å…³æ³¨è¿™äº›åŒºåŸŸ\n",
        "        sorted_indices = np.argsort(all_values.flatten())\n",
        "        top_10_percent = max(1, len(sorted_indices) // 10)\n",
        "        best_samples = all_states[sorted_indices[:top_10_percent]]\n",
        "        \n",
        "        if len(best_samples) > 0:\n",
        "            best_mins = np.min(best_samples, axis=0)\n",
        "            best_maxs = np.max(best_samples, axis=0)\n",
        "            best_ranges = best_maxs - best_mins\n",
        "            \n",
        "            # åœ¨æœ€ä¼˜åŒºåŸŸå‘¨å›´æ‰©å±•\n",
        "            best_expansion = 0.5  # åœ¨æœ€ä¼˜åŒºåŸŸå‘¨å›´æ‰©å±•50%\n",
        "            extended_best_mins = best_mins - best_expansion * best_ranges\n",
        "            extended_best_maxs = best_maxs + best_expansion * best_ranges\n",
        "            \n",
        "            # åˆå¹¶å…¨å±€æœç´¢åŒºåŸŸå’Œæœ€ä¼˜åŒºåŸŸ\n",
        "            extended_mins = np.minimum(data_mins - expansion_factor * data_ranges, extended_best_mins)\n",
        "            extended_maxs = np.maximum(data_maxs + expansion_factor * data_ranges, extended_best_maxs)\n",
        "        else:\n",
        "            extended_mins = data_mins - expansion_factor * data_ranges\n",
        "            extended_maxs = data_maxs + expansion_factor * data_ranges\n",
        "    else:\n",
        "        extended_mins = data_mins - expansion_factor * data_ranges\n",
        "        extended_maxs = data_maxs + expansion_factor * data_ranges\n",
        "    \n",
        "    # ç¡®ä¿åœ¨åŸå§‹åŸŸå†…\n",
        "    dynamic_mins = np.maximum(extended_mins, self.domain_mins)\n",
        "    dynamic_maxs = np.minimum(extended_maxs, self.domain_maxs)\n",
        "    \n",
        "    # ç¡®ä¿æœç´¢åŒºåŸŸä¸ä¼šé€€åŒ–ä¸ºç‚¹\n",
        "    for i in range(self.dimension):\n",
        "        if dynamic_maxs[i] - dynamic_mins[i] < 0.01:\n",
        "            center = (dynamic_mins[i] + dynamic_maxs[i]) / 2\n",
        "            dynamic_mins[i] = max(self.domain_mins[i], center - 0.005)\n",
        "            dynamic_maxs[i] = min(self.domain_maxs[i], center + 0.005)\n",
        "    \n",
        "    return dynamic_mins, dynamic_maxs\n",
        "\n",
        "def _mixed_expansion(self, state_vector, dynamic_factor=1.0, all_states=None, \n",
        "                    all_values=None, iteration=1, pre_computed_bounds=None):\n",
        "    \"\"\"å®Œæ•´çš„æ··åˆæ‰©å±•ç­–ç•¥ï¼ŒåŒ…æ‹¬éšæœºç§»åŠ¨å’Œç¡®å®šæ€§ç§»åŠ¨\"\"\"\n",
        "    leaf_states = []\n",
        "    current_state = state_vector.copy()\n",
        "\n",
        "    if self.dimension <= 0:\n",
        "        return []\n",
        "\n",
        "    # è®¡ç®—åŠ¨æ€æœç´¢åŒºé—´\n",
        "    if pre_computed_bounds is not None:\n",
        "        dynamic_mins, dynamic_maxs = pre_computed_bounds\n",
        "    else:\n",
        "        dynamic_mins, dynamic_maxs = self._calculate_dynamic_search_space(all_states, all_values, iteration)\n",
        "    \n",
        "    # è®¡ç®—è‡ªé€‚åº”æ­¥é•¿\n",
        "    base_step = 0.05\n",
        "    adaptive_step = base_step * (0.5 + 0.5 * (dynamic_factor - 1.0))\n",
        "    \n",
        "    # è°ƒæ•´æ¢ç´¢æ¦‚ç‡\n",
        "    stochastic_prob = 0.67  # é»˜è®¤67%æ¦‚ç‡ä½¿ç”¨éšæœºç§»åŠ¨\n",
        "    \n",
        "    if self.last_move_successful and self.last_move_deterministic:\n",
        "        stochastic_prob = 0.0  # å¦‚æœä¸Šæ¬¡ç¡®å®šæ€§ç§»åŠ¨æˆåŠŸï¼Œç»§ç»­ä½¿ç”¨ç¡®å®šæ€§ç§»åŠ¨\n",
        "        logger.debug(\"ä¸Šæ¬¡ç¡®å®šæ€§ç§»åŠ¨æˆåŠŸï¼Œç»§ç»­ä½¿ç”¨ç¡®å®šæ€§ç§»åŠ¨(100%)\")\n",
        "    else:\n",
        "        logger.debug(f\"ä½¿ç”¨æ ‡å‡†æ¦‚ç‡: {stochastic_prob*100:.1f}%éšæœºç§»åŠ¨, {(1-stochastic_prob)*100:.1f}%ç¡®å®šæ€§ç§»åŠ¨\")\n",
        "    \n",
        "    # æ··åˆæ‰©å±•ç­–ç•¥ï¼šç”Ÿæˆç»´åº¦æ•°é‡çš„å¶èŠ‚ç‚¹\n",
        "    for _ in range(self.dimension): \n",
        "        new_state = current_state.copy()\n",
        "        \n",
        "        use_stochastic = random.random() < stochastic_prob\n",
        "        \n",
        "        if use_stochastic: \n",
        "            # éšæœºç§»åŠ¨\n",
        "            move_type = random.choices(\n",
        "                ['single_var', 'd_div_5_vars', 'd_div_10_vars'],\n",
        "                weights=[0.5, 0.25, 0.25],\n",
        "                k=1\n",
        "            )[0]\n",
        "            \n",
        "            self.last_move_deterministic = False\n",
        "            \n",
        "            if move_type == 'single_var':\n",
        "                # éšæœºæ”¹å˜1ä¸ªå˜é‡\n",
        "                idx = random.randint(0, self.dimension - 1)\n",
        "                new_state[idx] = random.uniform(dynamic_mins[idx], dynamic_maxs[idx])\n",
        "                \n",
        "            elif move_type == 'd_div_5_vars':\n",
        "                # éšæœºæ”¹å˜d/5ä¸ªå˜é‡\n",
        "                num_vars_to_change = max(1, int(self.dimension / 5))\n",
        "                indices = random.sample(range(self.dimension), num_vars_to_change)\n",
        "                for idx in indices:\n",
        "                    new_state[idx] = random.uniform(dynamic_mins[idx], dynamic_maxs[idx])\n",
        "                    \n",
        "            elif move_type == 'd_div_10_vars':\n",
        "                # éšæœºæ”¹å˜d/10ä¸ªå˜é‡\n",
        "                num_vars_to_change = max(1, int(self.dimension / 10))\n",
        "                indices = random.sample(range(self.dimension), num_vars_to_change)\n",
        "                for idx in indices:\n",
        "                    new_state[idx] = random.uniform(dynamic_mins[idx], dynamic_maxs[idx])\n",
        "        else:\n",
        "            # ç¡®å®šæ€§ç§»åŠ¨\n",
        "            self.last_move_deterministic = True\n",
        "            self.last_move_type = \"multi_dim_step\"\n",
        "            \n",
        "            # é€‰æ‹©ä¿®æ”¹çš„ç»´åº¦æ•°é‡\n",
        "            dim_choice = random.choices(\n",
        "                ['single_dim', 'd_div_5_dims', 'd_div_10_dims', 'd_div_2_dims'],\n",
        "                weights=[0.91, 0.03, 0.03, 0.03],\n",
        "                k=1\n",
        "            )[0]\n",
        "            \n",
        "            # ç¡®å®šè¦ä¿®æ”¹çš„ç»´åº¦ç´¢å¼•\n",
        "            if dim_choice == 'single_dim':\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    indices = [self.last_move_dim_index]\n",
        "                else:\n",
        "                    indices = [random.randint(0, self.dimension - 1)]\n",
        "                logger.debug(f\"ç¡®å®šæ€§ç§»åŠ¨: ä¿®æ”¹å•ä¸ªç»´åº¦\")\n",
        "                \n",
        "            elif dim_choice == 'd_div_5_dims':\n",
        "                num_dims = max(4, int(self.dimension / 5))\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    remaining_dims = [i for i in range(self.dimension) if i != self.last_move_dim_index]\n",
        "                    additional_dims = random.sample(remaining_dims, min(num_dims - 1, len(remaining_dims)))\n",
        "                    indices = [self.last_move_dim_index] + additional_dims\n",
        "                else:\n",
        "                    indices = random.sample(range(self.dimension), num_dims)\n",
        "                logger.debug(f\"ç¡®å®šæ€§ç§»åŠ¨: ä¿®æ”¹d/5={num_dims}ä¸ªç»´åº¦\")\n",
        "                \n",
        "            elif dim_choice == 'd_div_10_dims':\n",
        "                num_dims = max(20, int(self.dimension / 10))\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    remaining_dims = [i for i in range(self.dimension) if i != self.last_move_dim_index]\n",
        "                    additional_dims = random.sample(remaining_dims, min(num_dims - 1, len(remaining_dims)))\n",
        "                    indices = [self.last_move_dim_index] + additional_dims\n",
        "                else:\n",
        "                    indices = random.sample(range(self.dimension), num_dims)\n",
        "                logger.debug(f\"ç¡®å®šæ€§ç§»åŠ¨: ä¿®æ”¹d/10={num_dims}ä¸ªç»´åº¦\")\n",
        "            \n",
        "            elif dim_choice == 'd_div_2_dims':\n",
        "                num_dims = 2\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    remaining_dims = [i for i in range(self.dimension) if i != self.last_move_dim_index]\n",
        "                    additional_dims = random.sample(remaining_dims, min(num_dims - 1, len(remaining_dims)))\n",
        "                    indices = [self.last_move_dim_index] + additional_dims\n",
        "                else:\n",
        "                    indices = random.sample(range(self.dimension), num_dims)\n",
        "                logger.debug(f\"ç¡®å®šæ€§ç§»åŠ¨: ä¿®æ”¹2ä¸ªç»´åº¦\")\n",
        "            \n",
        "            # è®°å½•ç¬¬ä¸€ä¸ªç»´åº¦\n",
        "            if indices:\n",
        "                self.last_move_dim_index = indices[0]\n",
        "            \n",
        "            # å¯¹é€‰å®šçš„ç»´åº¦è¿›è¡Œå°æ­¥è°ƒæ•´\n",
        "            for idx in indices:\n",
        "                if self.last_move_successful and idx == self.last_move_dim_index:\n",
        "                    direction = self.last_move_direction\n",
        "                else:\n",
        "                    direction = random.choice([-1, 1])\n",
        "                \n",
        "                if idx == indices[0]:\n",
        "                    self.last_move_direction = direction\n",
        "                \n",
        "                step_size = 0.1 \n",
        "                new_state[idx] = current_state[idx] + direction * step_size\n",
        "        \n",
        "        # ç¡®ä¿çŠ¶æ€åœ¨åŠ¨æ€æœç´¢åŒºé—´å†…\n",
        "        np.clip(new_state, dynamic_mins, dynamic_maxs, out=new_state)\n",
        "        leaf_states.append(new_state)\n",
        "\n",
        "    # ç¡®ä¿ç”Ÿæˆè¶³å¤Ÿçš„å¶èŠ‚ç‚¹\n",
        "    if not leaf_states: \n",
        "        rand_state = np.array([random.uniform(dynamic_mins[i], dynamic_maxs[i]) for i in range(self.dimension)])\n",
        "        return [rand_state] * self.dimension if self.dimension > 0 else []\n",
        "    \n",
        "    while len(leaf_states) < self.dimension and self.dimension > 0:\n",
        "        leaf_states.append(leaf_states[-1].copy())\n",
        "\n",
        "    return leaf_states[:self.dimension]\n",
        "\n",
        "# å°†æ–¹æ³•æ·»åŠ åˆ°NTESearcherç±»ä¸­\n",
        "NTESearcher._calculate_dynamic_search_space = _calculate_dynamic_search_space\n",
        "NTESearcher._mixed_expansion = _mixed_expansion\n",
        "\n",
        "print(\"âœ“ NTE Searcher æ ¸å¿ƒæ‰©å±•æ–¹æ³•å·²æ·»åŠ \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: ç¯å¢ƒé…ç½®éªŒè¯ä¸åŸºæœ¬åŠŸèƒ½æµ‹è¯•\n",
        "import traceback\n",
        "\n",
        "def comprehensive_environment_test():\n",
        "    \"\"\"æ‰§è¡Œå…¨é¢çš„ç¯å¢ƒé…ç½®éªŒè¯å’ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•\"\"\"\n",
        "    print(\"ğŸš€ å¼€å§‹æ‰§è¡ŒDANTEç¯å¢ƒé…ç½®éªŒè¯...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    test_results = {\n",
        "        'gpu_config': False,\n",
        "        'data_loading': False,\n",
        "        'model_creation': False,\n",
        "        'model_training': False,\n",
        "        'nte_search': False,\n",
        "        'visualization': False\n",
        "    }\n",
        "    \n",
        "    # 1. GPUé…ç½®éªŒè¯\n",
        "    print(\"ğŸ“Š 1. GPUé…ç½®éªŒè¯\")\n",
        "    try:\n",
        "        print(f\"   PyTorch CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"   GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}\")\n",
        "            print(f\"   å½“å‰GPU: {torch.cuda.get_device_name()}\")\n",
        "            print(f\"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "        \n",
        "        print(f\"   TensorFlow GPUå¯ç”¨: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            print(f\"   TensorFlow GPUæ•°é‡: {len(tf.config.list_physical_devices('GPU'))}\")\n",
        "        \n",
        "        test_results['gpu_config'] = True\n",
        "        print(\"   âœ… GPUé…ç½®éªŒè¯é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ GPUé…ç½®éªŒè¯å¤±è´¥: {e}\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 2. æ•°æ®åŠ è½½ä¸ç®¡ç†éªŒè¯\n",
        "    print(\"ğŸ“ 2. æ•°æ®åŠ è½½ä¸ç®¡ç†éªŒè¯\")\n",
        "    try:\n",
        "        # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
        "        test_dimension = 20\n",
        "        test_samples = 100\n",
        "        \n",
        "        print(f\"   ç”Ÿæˆæµ‹è¯•æ•°æ®: {test_samples}ä¸ªæ ·æœ¬, {test_dimension}ç»´\")\n",
        "        X_test = np.random.uniform(-2.048, 2.048, (test_samples, test_dimension))\n",
        "        y_test = np.array([rosenbrock_function(x) for x in X_test]).reshape(-1, 1)\n",
        "        \n",
        "        print(f\"   Rosenbrockå‡½æ•°å€¼èŒƒå›´: {y_test.min():.2e} - {y_test.max():.2e}\")\n",
        "        \n",
        "        # æµ‹è¯•DataManager\n",
        "        data_manager = DataManager(\n",
        "            initial_X=X_test[:80], \n",
        "            initial_y=y_test[:80], \n",
        "            dimension=test_dimension\n",
        "        )\n",
        "        \n",
        "        print(f\"   DataManageråˆå§‹åŒ–æˆåŠŸ: {data_manager.num_samples}ä¸ªæ ·æœ¬\")\n",
        "        print(f\"   æ•°æ®æ ‡å‡†åŒ–æµ‹è¯•: XèŒƒå›´{data_manager.X_normalized.min():.3f}-{data_manager.X_normalized.max():.3f}\")\n",
        "        print(f\"   å¯¹æ•°å˜æ¢æµ‹è¯•: y_logèŒƒå›´{data_manager.y_log.min():.3f}-{data_manager.y_log.max():.3f}\")\n",
        "        \n",
        "        test_results['data_loading'] = True\n",
        "        print(\"   âœ… æ•°æ®åŠ è½½ä¸ç®¡ç†éªŒè¯é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ æ•°æ®åŠ è½½ä¸ç®¡ç†éªŒè¯å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 3. CNNæ¨¡å‹åˆ›å»ºéªŒè¯\n",
        "    print(\"ğŸ§  3. CNNæ¨¡å‹åˆ›å»ºéªŒè¯\")\n",
        "    try:\n",
        "        cnn_model = CNN1DSurrogate(input_dim=test_dimension)\n",
        "        print(f\"   CNN1Dæ¨¡å‹åˆ›å»ºæˆåŠŸ\")\n",
        "        print(f\"   æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in cnn_model.model.parameters() if p.requires_grad):,}\")\n",
        "        \n",
        "        # æµ‹è¯•é¢„æµ‹åŠŸèƒ½\n",
        "        test_input = X_test[:5]\n",
        "        predictions = cnn_model.predict(test_input)\n",
        "        print(f\"   æ¨¡å‹é¢„æµ‹æµ‹è¯•: è¾“å…¥{test_input.shape} -> è¾“å‡º{predictions.shape}\")\n",
        "        print(f\"   é¢„æµ‹å€¼èŒƒå›´: {predictions.min():.3f} - {predictions.max():.3f}\")\n",
        "        \n",
        "        test_results['model_creation'] = True\n",
        "        print(\"   âœ… CNNæ¨¡å‹åˆ›å»ºéªŒè¯é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ CNNæ¨¡å‹åˆ›å»ºéªŒè¯å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 4. æ¨¡å‹è®­ç»ƒéªŒè¯ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰\n",
        "    print(\"ğŸƒ 4. æ¨¡å‹è®­ç»ƒéªŒè¯ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰\")\n",
        "    try:\n",
        "        # å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆä»…5ä¸ªepochï¼‰\n",
        "        print(\"   å¼€å§‹å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ5ä¸ªepochsï¼‰...\")\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=5,\n",
        "            batch_size=16,\n",
        "            validation_split=0.2,\n",
        "            patience=10,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        final_loss = training_history['loss'][-1]\n",
        "        print(f\"   è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {final_loss:.4f}\")\n",
        "        \n",
        "        # æµ‹è¯•è®­ç»ƒåçš„é¢„æµ‹\n",
        "        post_train_predictions = cnn_model.predict(test_input)\n",
        "        print(f\"   è®­ç»ƒåé¢„æµ‹å€¼èŒƒå›´: {post_train_predictions.min():.3f} - {post_train_predictions.max():.3f}\")\n",
        "        \n",
        "        test_results['model_training'] = True\n",
        "        print(\"   âœ… æ¨¡å‹è®­ç»ƒéªŒè¯é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ æ¨¡å‹è®­ç»ƒéªŒè¯å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 5. NTEæœç´¢éªŒè¯\n",
        "    print(\"ğŸ” 5. NTEæœç´¢éªŒè¯\")\n",
        "    try:\n",
        "        # åˆ›å»ºNTEæœç´¢å™¨\n",
        "        nte_searcher = NTESearcher(\n",
        "            dimension=test_dimension,\n",
        "            domain=[(-2.048, 2.048) for _ in range(test_dimension)],\n",
        "            c0=1,\n",
        "            rollout_rounds=10,  # å‡å°‘è½®æ¬¡ä»¥åŠ å¿«æµ‹è¯•\n",
        "            validation_batch_size=5   # å‡å°‘æ‰¹æ¬¡å¤§å°\n",
        "        )\n",
        "        \n",
        "        print(f\"   NTESearcheråˆ›å»ºæˆåŠŸ\")\n",
        "        \n",
        "        # åˆ›å»ºç®€åŒ–çš„surrogate wrapperç”¨äºæµ‹è¯•\n",
        "        class TestSurrogateWrapper:\n",
        "            def __init__(self, model):\n",
        "                self.model = model\n",
        "            \n",
        "            def predict(self, X_batch):\n",
        "                if X_batch.shape[0] == 0:\n",
        "                    return np.array([])\n",
        "                return self.model.predict(X_batch)\n",
        "        \n",
        "        wrapper = TestSurrogateWrapper(cnn_model)\n",
        "        current_best_x, current_best_y = data_manager.get_current_best()\n",
        "        \n",
        "        print(f\"   å¼€å§‹NTEæœç´¢æµ‹è¯•ï¼ˆå°è§„æ¨¡ï¼‰...\")\n",
        "        print(f\"   å½“å‰æœ€ä¼˜å€¼: {current_best_y:.4e}\")\n",
        "        \n",
        "        # æ‰§è¡Œç®€åŒ–çš„NTEæœç´¢\n",
        "        search_result = nte_searcher.search(\n",
        "            surrogate_model=wrapper,\n",
        "            current_best_state=current_best_x,\n",
        "            min_y_observed=current_best_y,\n",
        "            dynamic_exploration_factor=1.0,\n",
        "            all_states=data_manager.X_data,\n",
        "            all_values=data_manager.get_all_y_orig(),\n",
        "            previous_iteration_samples=None,\n",
        "            current_iteration=0,\n",
        "            return_ducb_trends=True\n",
        "        )\n",
        "        \n",
        "        candidates, ducb_trends = search_result\n",
        "        print(f\"   NTEæœç´¢å®Œæˆ: æ‰¾åˆ°{len(candidates)}ä¸ªå€™é€‰ç‚¹\")\n",
        "        \n",
        "        if len(candidates) > 0:\n",
        "            # è¯„ä¼°å€™é€‰ç‚¹\n",
        "            candidate_values = np.array([rosenbrock_function(x) for x in candidates])\n",
        "            print(f\"   å€™é€‰ç‚¹çœŸå€¼èŒƒå›´: {candidate_values.min():.4e} - {candidate_values.max():.4e}\")\n",
        "            print(f\"   æœ€ä½³å€™é€‰ç‚¹å€¼: {candidate_values.min():.4e}\")\n",
        "        \n",
        "        test_results['nte_search'] = True\n",
        "        print(\"   âœ… NTEæœç´¢éªŒè¯é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ NTEæœç´¢éªŒè¯å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 6. å¯è§†åŒ–åŠŸèƒ½éªŒè¯\n",
        "    print(\"ğŸ“ˆ 6. å¯è§†åŒ–åŠŸèƒ½éªŒè¯\")\n",
        "    try:\n",
        "        # æµ‹è¯•åŸºæœ¬å¯è§†åŒ–åŠŸèƒ½\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        # åˆ›å»ºæµ‹è¯•å›¾å½¢\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        test_x = np.linspace(0, 10, 50)\n",
        "        test_y = np.sin(test_x)\n",
        "        plt.plot(test_x, test_y, 'b-', label='Test Plot')\n",
        "        plt.xlabel('X Values')\n",
        "        plt.ylabel('Y Values')\n",
        "        plt.title('Visualization Test')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # åœ¨Kaggleç¯å¢ƒä¸­æ˜¾ç¤ºå›¾å½¢\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        print(\"   âœ… åŸºæœ¬å¯è§†åŒ–åŠŸèƒ½æ­£å¸¸\")\n",
        "        \n",
        "        # æµ‹è¯•plot_utilsä¸­çš„å‡½æ•°\n",
        "        if len(candidates) > 0:\n",
        "            try:\n",
        "                # æµ‹è¯•é¢„æµ‹vsçœŸå€¼å›¾\n",
        "                pred_vs_true_metrics = plot_prediction_vs_truth(\n",
        "                    cnn_model.predict(candidates), \n",
        "                    candidate_values, \n",
        "                    \"/kaggle/working\", \n",
        "                    iteration=0,\n",
        "                    filename=\"test_pred_vs_true.png\"\n",
        "                )\n",
        "                print(f\"   é¢„æµ‹vsçœŸå€¼å›¾æµ‹è¯•æˆåŠŸ: RMSE={pred_vs_true_metrics[1]:.4e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   é«˜çº§å¯è§†åŒ–æµ‹è¯•è­¦å‘Š: {e}\")\n",
        "        \n",
        "        test_results['visualization'] = True\n",
        "        print(\"   âœ… å¯è§†åŒ–åŠŸèƒ½éªŒè¯é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ å¯è§†åŒ–åŠŸèƒ½éªŒè¯å¤±è´¥: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    print(\"ğŸ”§ ç¯å¢ƒé…ç½®éªŒè¯å®Œæˆ!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # è¾“å‡ºæµ‹è¯•æ€»ç»“\n",
        "    passed_tests = sum(test_results.values())\n",
        "    total_tests = len(test_results)\n",
        "    \n",
        "    print(f\"ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»: {passed_tests}/{total_tests} é¡¹æµ‹è¯•é€šè¿‡\")\n",
        "    print()\n",
        "    \n",
        "    for test_name, passed in test_results.items():\n",
        "        status = \"âœ…\" if passed else \"âŒ\"\n",
        "        print(f\"   {status} {test_name.replace('_', ' ').title()}\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    if passed_tests == total_tests:\n",
        "        print(\"ğŸ‰ æ‰€æœ‰ç¯å¢ƒé…ç½®éªŒè¯é€šè¿‡ï¼DANTEå·²å‡†å¤‡å¥½è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"âš ï¸  æœ‰ {total_tests - passed_tests} é¡¹æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®åå†è¿›è¡Œå®Œæ•´è®­ç»ƒã€‚\")\n",
        "        return False\n",
        "\n",
        "# æ‰§è¡Œç¯å¢ƒéªŒè¯\n",
        "print(\"å¼€å§‹DANTEç¯å¢ƒé…ç½®éªŒè¯...\")\n",
        "validation_success = comprehensive_environment_test()\n",
        "\n",
        "if validation_success:\n",
        "    print(\"\\nğŸš€ ç¯å¢ƒéªŒè¯æˆåŠŸï¼å¯ä»¥å¼€å§‹å®Œæ•´çš„DANTEè®­ç»ƒæµç¨‹ã€‚\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  ç¯å¢ƒéªŒè¯æœªå®Œå…¨é€šè¿‡ï¼Œå»ºè®®å…ˆè§£å†³é—®é¢˜åå†è¿›è¡Œè®­ç»ƒã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: ä¸»è¦DANTEè®­ç»ƒæ‰§è¡Œé€»è¾‘\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gc\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# é…ç½®æ—¥å¿—è®°å½•å™¨\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def calculate_pearson_correlation(model, data_manager):\n",
        "    \"\"\"è®¡ç®—æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    if data_manager.num_samples < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        X_data = data_manager.get_all_x_orig()\n",
        "        y_data_orig = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        predictions_log = model.predict(X_data)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_data_orig) < 2:\n",
        "            logger.warning(\"æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°\")\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_data_orig)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_best_samples(model, data_manager, n_best=60):\n",
        "    \"\"\"è®¡ç®—æ¨¡å‹åœ¨æœ€ä½³æ ·æœ¬ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    all_X = data_manager.get_all_x_orig()\n",
        "    all_y_orig = data_manager.get_all_y_orig()\n",
        "    \n",
        "    if all_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "    \n",
        "    # æŒ‰yå€¼æ’åºï¼ˆå‡åºï¼‰å–æœ€ä½³æ ·æœ¬\n",
        "    sorted_indices = np.argsort(all_y_orig.flatten())\n",
        "    n_best = min(n_best, len(sorted_indices))\n",
        "    sorted_X = all_X[sorted_indices[:n_best]]\n",
        "    sorted_y_orig = all_y_orig[sorted_indices[:n_best]]\n",
        "    \n",
        "    min_y_orig_for_range = np.min(sorted_y_orig)\n",
        "    max_y_orig_for_range = np.max(sorted_y_orig)\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(sorted_X)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if np.any(np.isinf(predictions_orig)) or np.any(np.isnan(predictions_orig)):\n",
        "            logger.warning(\"é¢„æµ‹å€¼ä¸­å­˜åœ¨æ— ç©·å¤§æˆ–NaNå€¼\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "        true_values_orig = sorted_y_orig.flatten()\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(true_values_orig) < 2:\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "        \n",
        "        if np.var(predictions_orig) == 0 or np.var(true_values_orig) == 0:\n",
        "            logger.warning(\"é¢„æµ‹å€¼æˆ–çœŸå®å€¼æ²¡æœ‰å˜åŒ–ï¼Œæ— æ³•è®¡ç®—ç›¸å…³ç³»æ•°\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, true_values_orig)\n",
        "        if np.isnan(correlation) or np.isinf(correlation):\n",
        "            logger.warning(\"Pearsonç›¸å…³ç³»æ•°è®¡ç®—ç»“æœä¸ºNaNæˆ–æ— ç©·å¤§\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "        return (correlation if not np.isnan(correlation) else 0.0), min_y_orig_for_range, max_y_orig_for_range\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"è®¡ç®—Pearsonç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "\n",
        "def create_weighted_samples(num_total_samples, y_data_orig_for_weighting, \n",
        "                          nte_samples_X=None, nte_samples_y_orig=None, \n",
        "                          initial_lowest_indices=None, initial_highest_indices=None):\n",
        "    \"\"\"ä¸ºæ•°æ®é›†åˆ›å»ºæ ·æœ¬æƒé‡\"\"\"\n",
        "    sample_weights = np.ones(num_total_samples)  # é»˜è®¤æƒé‡ä¸º1\n",
        "\n",
        "    if initial_lowest_indices is not None:\n",
        "        sample_weights[initial_lowest_indices] = 2.0\n",
        "        logger.info(f\"ä¸ºåˆå§‹æ•°æ®é›†ä¸­ {len(initial_lowest_indices)} ä¸ªæœ€ä½yå€¼æ ·æœ¬è®¾ç½®æƒé‡ä¸º2.0\")\n",
        "\n",
        "    if initial_highest_indices is not None:\n",
        "        sample_weights[initial_highest_indices] = 3.0\n",
        "        logger.info(f\"ä¸ºåˆå§‹æ•°æ®é›†ä¸­ {len(initial_highest_indices)} ä¸ªæœ€é«˜yå€¼æ ·æœ¬è®¾ç½®æƒé‡ä¸º3.0\")\n",
        "\n",
        "    # ä¸ºNTEæ ·æœ¬è®¾ç½®æƒé‡\n",
        "    if nte_samples_X is not None and nte_samples_y_orig is not None:\n",
        "        num_nte_samples = nte_samples_X.shape[0]\n",
        "        start_index_for_nte = num_total_samples - num_nte_samples\n",
        "        \n",
        "        if num_nte_samples > 0:\n",
        "            nte_specific_weights = np.ones(num_nte_samples) * 4.0\n",
        "            sample_weights[start_index_for_nte:] = nte_specific_weights\n",
        "            logger.info(f\"ä¸ºæ–°åŠ å…¥çš„ {num_nte_samples} ä¸ªNTEæ ·æœ¬è®¾ç½®æƒé‡ä¸º4.0\")\n",
        "\n",
        "    final_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
        "    logger.info(f\"æœ€ç»ˆæ ·æœ¬æƒé‡ç»Ÿè®¡: æœ€å°å€¼={final_weights.min().item():.1f}, æœ€å¤§å€¼={final_weights.max().item():.1f}, å¹³å‡å€¼={final_weights.mean().item():.1f}\")\n",
        "    \n",
        "    return final_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: ä¸»è®­ç»ƒå‡½æ•°å’Œå¯åŠ¨è„šæœ¬\n",
        "def save_iteration_samples_to_csv(X_batch, y_batch, results_dir, iteration):\n",
        "    \"\"\"å°†æ¯æ¬¡è¿­ä»£çš„æ ·æœ¬ä¿å­˜åˆ°CSVæ–‡ä»¶\"\"\"\n",
        "    try:\n",
        "        csv_filename = os.path.join(results_dir, f\"iteration_{iteration}_samples.csv\")\n",
        "        \n",
        "        data_dict = {}\n",
        "        for i in range(X_batch.shape[1]):\n",
        "            data_dict[f'x{i+1}'] = X_batch[:, i]\n",
        "        data_dict['y_true'] = y_batch.flatten()\n",
        "        \n",
        "        df = pd.DataFrame(data_dict)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        \n",
        "        logger.info(f\"ç¬¬{iteration}è½®è¿­ä»£çš„{len(y_batch)}ä¸ªæ ·æœ¬å·²ä¿å­˜åˆ°: {csv_filename}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ä¿å­˜æ ·æœ¬åˆ°CSVå¤±è´¥: {e}\")\n",
        "\n",
        "def main_dante_training():\n",
        "    \"\"\"DANTEä¸»è®­ç»ƒæµç¨‹\"\"\"\n",
        "    logger.info(\"å¼€å§‹DANTE Rosenbrockå‡½æ•°ä¼˜åŒ–...\")\n",
        "    \n",
        "    # === å‚æ•°é…ç½® ===\n",
        "    ROSENBROCK_DIMENSION = 20\n",
        "    domain_min, domain_max = -2.048, 2.048 \n",
        "    ROSENBROCK_DOMAIN = [(domain_min, domain_max)] * ROSENBROCK_DIMENSION\n",
        "    \n",
        "    INITIAL_DATA_SIZE = 800\n",
        "    N_AL_ITERATIONS = 100  # åœ¨Kaggleä¸Šä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°è¿›è¡Œæµ‹è¯•\n",
        "    CNN_TRAINING_EPOCHS = 200\n",
        "    CNN_EARLY_STOPPING_PATIENCE = 30\n",
        "    CNN_VALIDATION_SPLIT = 0.2\n",
        "    CNN_BATCH_SIZE = 32\n",
        "    NTE_C0 = 1\n",
        "    NTE_ROLLOUT_ROUNDS = 100\n",
        "    VALIDATION_BATCH_SIZE = 20\n",
        "    \n",
        "    logger.info(f\"DANTEé…ç½®:\")\n",
        "    logger.info(f\"  ç»´åº¦: {ROSENBROCK_DIMENSION}\")\n",
        "    logger.info(f\"  åˆå§‹æ•°æ®å¤§å°: {INITIAL_DATA_SIZE}\")\n",
        "    logger.info(f\"  ä¸»åŠ¨å­¦ä¹ è¿­ä»£æ¬¡æ•°: {N_AL_ITERATIONS}\")\n",
        "    logger.info(f\"  CNNè®­ç»ƒè½®æ•°: {CNN_TRAINING_EPOCHS}\")\n",
        "    \n",
        "    # æ£€æµ‹GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "    \n",
        "    # === åˆ›å»ºç»“æœç›®å½• ===\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_dir = f\"/kaggle/working/dante_results_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    logger.info(f\"ç»“æœå°†ä¿å­˜åˆ°: {results_dir}\")\n",
        "    \n",
        "    # === åˆå§‹åŒ–å†å²è®°å½•å˜é‡ ===\n",
        "    cnn_performance_history = []\n",
        "    history_pearson_all = []\n",
        "    history_pearson_best_samples = []\n",
        "    history_iterations = []\n",
        "    history_best_y = []\n",
        "    history_nte_candidates_count = []\n",
        "    \n",
        "    no_improvement_streak = 0\n",
        "    previous_best_y = float('inf')\n",
        "    previous_iteration_samples_x = None\n",
        "    \n",
        "    viz_interval = 20  # æ¯20æ¬¡è¿­ä»£å¯è§†åŒ–ä¸€æ¬¡\n",
        "    logger.info(f\"å¯è§†åŒ–é—´éš”è®¾ç½®ä¸º: æ¯{viz_interval}æ¬¡è¿­ä»£è¾“å‡ºä¸€æ¬¡ç»“æœ\")\n",
        "    \n",
        "    # === 1. æ•°æ®åˆå§‹åŒ– ===\n",
        "    logger.info(\"ç”Ÿæˆåˆå§‹æ•°æ®...\")\n",
        "    initial_X = np.random.uniform(low=domain_min, high=domain_max, size=(INITIAL_DATA_SIZE, ROSENBROCK_DIMENSION))\n",
        "    initial_y = np.array([rosenbrock_function(x) for x in initial_X]).reshape(-1, 1)\n",
        "    logger.info(\"åˆå§‹æ•°æ®ç”Ÿæˆå®Œæˆ\")\n",
        "    \n",
        "    # === 2. DataManageråˆå§‹åŒ– ===\n",
        "    data_manager = DataManager(\n",
        "        initial_X=initial_X, \n",
        "        initial_y=initial_y, \n",
        "        dimension=ROSENBROCK_DIMENSION,\n",
        "        data_file_path=None\n",
        "    )\n",
        "    logger.info(\"DataManageråˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # === 3. åˆå§‹æ ·æœ¬æƒé‡è®¾å®š ===\n",
        "    if data_manager.num_samples > 0:\n",
        "        all_y_orig = data_manager.get_all_y_orig().flatten()\n",
        "        sorted_indices = np.argsort(all_y_orig)\n",
        "        \n",
        "        lowest_20_indices = sorted_indices[:20] if len(sorted_indices) >= 20 else sorted_indices\n",
        "        highest_20_indices = sorted_indices[-20:] if len(sorted_indices) >= 20 else np.array([])\n",
        "\n",
        "        initial_weights = create_weighted_samples(\n",
        "            num_total_samples=data_manager.num_samples,\n",
        "            y_data_orig_for_weighting=all_y_orig,\n",
        "            initial_lowest_indices=lowest_20_indices,\n",
        "            initial_highest_indices=highest_20_indices\n",
        "        )\n",
        "        data_manager.last_batch_weights = initial_weights\n",
        "    else:\n",
        "        data_manager.last_batch_weights = torch.empty(0, dtype=torch.float32)\n",
        "    \n",
        "    # === 4. CNNæ¨¡å‹åˆå§‹åŒ– ===\n",
        "    cnn_model = CNN1DSurrogate(input_dim=ROSENBROCK_DIMENSION)\n",
        "    logger.info(\"CNN1DSurrogateæ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # === 5. ä¸»åŠ¨å­¦ä¹ ä¸»å¾ªç¯ ===\n",
        "    for al_iteration in range(N_AL_ITERATIONS):\n",
        "        should_generate_plots = ((al_iteration + 1) % viz_interval == 0) or (al_iteration == N_AL_ITERATIONS - 1)\n",
        "        current_iteration_number = al_iteration + 1\n",
        "        history_iterations.append(current_iteration_number)\n",
        "        \n",
        "        logger.info(f\"\\n--- è®­ç»ƒCNN 1Dæ¨¡å‹ (è¿­ä»£ {current_iteration_number}/{N_AL_ITERATIONS}) ---\")\n",
        "        \n",
        "        # === 5a. è®­ç»ƒCNNæ¨¡å‹ ===\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=CNN_TRAINING_EPOCHS,\n",
        "            batch_size=CNN_BATCH_SIZE,\n",
        "            validation_split=CNN_VALIDATION_SPLIT,\n",
        "            patience=CNN_EARLY_STOPPING_PATIENCE,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # === 5b. è¯„ä¼°æ¨¡å‹æ€§èƒ½ ===\n",
        "        if should_generate_plots:\n",
        "            cnn_model.visualize_training_history(\n",
        "                training_history,\n",
        "                save_path=os.path.join(results_dir, f\"cnn_1d_training_history_iter_{current_iteration_number}.png\")\n",
        "            )\n",
        "        \n",
        "        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°\n",
        "        current_pearson = calculate_pearson_correlation(cnn_model, data_manager)\n",
        "        history_pearson_all.append(current_pearson)\n",
        "        logger.info(f\"æ¨¡å‹çš®å°”é€Šç›¸å…³ç³»æ•°: {current_pearson:.4f}\")\n",
        "        \n",
        "        current_pearson_best_samples, min_y_best, max_y_best = calculate_pearson_on_best_samples(cnn_model, data_manager)\n",
        "        history_pearson_best_samples.append(current_pearson_best_samples)\n",
        "        logger.info(f\"æœ€ä½³{min(60, data_manager.num_samples)}ä¸ªæ ·æœ¬çš®å°”é€Šç›¸å…³ç³»æ•°: {current_pearson_best_samples:.4f}\")\n",
        "        \n",
        "        # === 5c. NTEæœç´¢æ–°å€™é€‰ç‚¹ ===\n",
        "        logger.info(\"ä½¿ç”¨NTEæœç´¢æ–°å€™é€‰æ ·æœ¬...\")\n",
        "        \n",
        "        # NTE Surrogate Wrapper\n",
        "        class NTESurrogateWrapper:\n",
        "            def __init__(self, actual_surrogate_model, data_manager_instance):\n",
        "                self.surrogate_model = actual_surrogate_model\n",
        "                self.dm = data_manager_instance\n",
        "                self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "            def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:\n",
        "                if X_batch_orig.shape[0] == 0:\n",
        "                    return np.array([])\n",
        "                if X_batch_orig.ndim == 1:\n",
        "                     X_batch_orig = X_batch_orig.reshape(1, -1)\n",
        "\n",
        "                try:\n",
        "                    pred_log = self.surrogate_model.predict(X_batch_orig)\n",
        "                    \n",
        "                    if X_batch_orig.shape[0] == 1:\n",
        "                        return pred_log[0] if isinstance(pred_log, np.ndarray) else pred_log\n",
        "                    else:\n",
        "                        return pred_log\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"NTESurrogateWrapperé¢„æµ‹é”™è¯¯: {e}\")\n",
        "                    \n",
        "                    if X_batch_orig.shape[0] == 1:\n",
        "                        return np.log10(1e6 + 1.0)\n",
        "                    else:\n",
        "                        return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)\n",
        "        \n",
        "        nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)\n",
        "        \n",
        "        # åˆå§‹åŒ–NTEæœç´¢å™¨\n",
        "        nte_searcher = NTESearcher(\n",
        "            dimension=ROSENBROCK_DIMENSION, \n",
        "            domain=[(domain_min, domain_max) for _ in range(ROSENBROCK_DIMENSION)],\n",
        "            c0=NTE_C0,\n",
        "            rollout_rounds=NTE_ROLLOUT_ROUNDS,\n",
        "            validation_batch_size=VALIDATION_BATCH_SIZE\n",
        "        )\n",
        "        \n",
        "        current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "        \n",
        "        # ğŸ”§ å…³é”®è°ƒè¯•å‚æ•°ï¼šè®¡ç®—åŠ¨æ€æ¢ç´¢å› å­\n",
        "        dynamic_exploration_factor = 0.8 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "        logger.info(f\"ğŸ”§ ä½¿ç”¨åŠ¨æ€æ¢ç´¢å› å­: {dynamic_exploration_factor:.2f} (åŸºäºè¿ç»­æ— æ”¹è¿›æ¬¡æ•°: {no_improvement_streak})\")\n",
        "        \n",
        "        try:\n",
        "            logger.info(\"=\" * 50)\n",
        "            logger.info(f\"å¼€å§‹ç¬¬ {current_iteration_number} è½®NTEæœç´¢...\")\n",
        "            logger.info(\"=\" * 50)\n",
        "            \n",
        "            # æ‰§è¡ŒNTEæœç´¢\n",
        "            nte_search_result = nte_searcher.search(\n",
        "                surrogate_model=nte_model_wrapper,\n",
        "                current_best_state=current_best_x_orig if current_best_x_orig is not None else None,\n",
        "                min_y_observed=current_best_y_overall,\n",
        "                dynamic_exploration_factor=dynamic_exploration_factor,\n",
        "                all_states=data_manager.X_data if data_manager.num_samples > 0 else None,\n",
        "                all_values=data_manager.get_all_y_orig() if data_manager.num_samples > 0 else None,\n",
        "                previous_iteration_samples=previous_iteration_samples_x,\n",
        "                current_iteration=al_iteration,\n",
        "                return_ducb_trends=True\n",
        "            )\n",
        "            \n",
        "            nte_selected_samples_x, ducb_trends_data = nte_search_result\n",
        "            \n",
        "            if isinstance(nte_selected_samples_x, list):\n",
        "                nte_selected_samples_x = np.array(nte_selected_samples_x)\n",
        "            \n",
        "            logger.info(f\"NTEæœç´¢å®Œæˆï¼Œè¿”å› {len(nte_selected_samples_x)} ä¸ªå€™é€‰ç‚¹\")\n",
        "            \n",
        "            if nte_selected_samples_x.shape[0] > 0:\n",
        "                logger.info(f\"è¯„ä¼° {nte_selected_samples_x.shape[0]} ä¸ªæ–°å€™é€‰æ ·æœ¬...\")\n",
        "                nte_selected_samples_y_true = np.array([rosenbrock_function(x) for x in nte_selected_samples_x]).reshape(-1, 1)\n",
        "                \n",
        "                num_samples_before_adding = data_manager.num_samples\n",
        "                data_manager.add_samples(nte_selected_samples_x, nte_selected_samples_y_true, re_normalize=True, shuffle=True)\n",
        "                logger.info(f\"æ·»åŠ äº† {nte_selected_samples_x.shape[0]} ä¸ªæ–°æ ·æœ¬ã€‚DataManagerç°åœ¨æœ‰ {data_manager.num_samples} ä¸ªæ ·æœ¬ã€‚\")\n",
        "                \n",
        "                # æ›´æ–°æ ·æœ¬æƒé‡\n",
        "                if data_manager.num_samples > num_samples_before_adding:\n",
        "                    new_batch_weights = np.ones(data_manager.num_samples) \n",
        "                    if data_manager.last_batch_weights is not None and len(data_manager.last_batch_weights) == num_samples_before_adding:\n",
        "                        new_batch_weights[:num_samples_before_adding] = data_manager.last_batch_weights.numpy() \n",
        "                    \n",
        "                    new_nte_sample_weights = np.ones(nte_selected_samples_x.shape[0]) * 4.0\n",
        "                    new_batch_weights[num_samples_before_adding:] = new_nte_sample_weights\n",
        "                    data_manager.last_batch_weights = torch.tensor(new_batch_weights, dtype=torch.float32)\n",
        "                \n",
        "                # ä¿å­˜è¿­ä»£ç»“æœ\n",
        "                if should_generate_plots:\n",
        "                    save_iteration_samples_to_csv(nte_selected_samples_x, nte_selected_samples_y_true, results_dir, current_iteration_number)\n",
        "                \n",
        "                previous_iteration_samples_x = nte_selected_samples_x.copy()\n",
        "                \n",
        "                # æ›´æ–°æœ€ä½³å€¼\n",
        "                current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "                history_best_y.append(current_best_y_overall)\n",
        "                logger.info(f\"æ›´æ–°åå…¨å±€æœ€å°å‡½æ•°å€¼: {current_best_y_overall:.6e} @ è¿­ä»£ {current_iteration_number}\")\n",
        "                \n",
        "                history_nte_candidates_count.append(nte_selected_samples_x.shape[0])\n",
        "                \n",
        "                # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›\n",
        "                new_global_best_found = current_best_y_overall < previous_best_y\n",
        "                if new_global_best_found:\n",
        "                    logger.info(f\"ğŸ‰ å‘ç°æ–°çš„å…¨å±€æœ€ä¼˜è§£: {current_best_y_overall:.6e}\")\n",
        "                    \n",
        "            else:\n",
        "                logger.info(\"NTEæœªè¿”å›æœ‰æ•ˆå€™é€‰ç‚¹\")\n",
        "                history_nte_candidates_count.append(0)\n",
        "                if history_best_y:\n",
        "                    history_best_y.append(history_best_y[-1])\n",
        "                else:\n",
        "                    history_best_y.append(current_best_y_overall)\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"NTEæœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            nte_selected_samples_x = np.array([])\n",
        "        \n",
        "        # === 5d. æ›´æ–°æ”¹è¿›ç»Ÿè®¡ ===\n",
        "        iteration_improved = False\n",
        "        if 'new_global_best_found' in locals() and new_global_best_found:\n",
        "            iteration_improved = True\n",
        "        elif len(nte_selected_samples_x) == 0:\n",
        "            iteration_improved = False\n",
        "        else:\n",
        "            iteration_improved = current_best_y_overall < previous_best_y\n",
        "            \n",
        "        if iteration_improved:\n",
        "            no_improvement_streak = 0\n",
        "            logger.info(f\"æœ¬æ¬¡è¿­ä»£æœ‰æ”¹è¿›ï¼Œé‡ç½® no_improvement_streak = 0\")\n",
        "        else:\n",
        "            no_improvement_streak += 1\n",
        "            logger.info(f\"æœ¬æ¬¡è¿­ä»£æ— æ”¹è¿›ï¼Œno_improvement_streak å¢åŠ åˆ° {no_improvement_streak}\")\n",
        "            \n",
        "        previous_best_y = current_best_y_overall\n",
        "        \n",
        "        # === 5e. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===\n",
        "        if should_generate_plots:\n",
        "            logger.info(f\"åœ¨è¿­ä»£ {current_iteration_number} ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...\")\n",
        "            \n",
        "            # å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾\n",
        "            if history_best_y:\n",
        "                try:\n",
        "                    plot_global_min_value_trend(history_iterations[:len(history_best_y)], history_best_y, results_dir, \n",
        "                                              filename=f\"global_min_iter_{current_iteration_number}.png\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"ç»˜åˆ¶å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾å¤±è´¥: {e}\")\n",
        "            \n",
        "            # çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾\n",
        "            if history_pearson_all:\n",
        "                try:\n",
        "                    plot_pearson_correlation_trend(history_iterations, history_pearson_all, results_dir,\n",
        "                                                 filename=f\"pearson_corr_iter_{current_iteration_number}.png\",\n",
        "                                                 title=\"Pearson Correlation Trend\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"ç»˜åˆ¶çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾å¤±è´¥: {e}\")\n",
        "            \n",
        "            # æœ€ä½³æ ·æœ¬çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾\n",
        "            if history_pearson_best_samples:\n",
        "                try:\n",
        "                    plot_best_samples_pearson_trend(history_iterations, history_pearson_best_samples, results_dir,\n",
        "                                                  filename=f\"best_samples_pearson_iter_{current_iteration_number}.png\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"ç»˜åˆ¶æœ€ä½³æ ·æœ¬çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾å¤±è´¥: {e}\")\n",
        "            \n",
        "            # NTEæ€§èƒ½å›¾\n",
        "            if history_nte_candidates_count:\n",
        "                try:\n",
        "                    plot_nte_performance(history_iterations, history_nte_candidates_count, results_dir,\n",
        "                                       filename=f\"nte_perf_iter_{current_iteration_number}.png\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"ç»˜åˆ¶NTEæ€§èƒ½å›¾å¤±è´¥: {e}\")\n",
        "        \n",
        "        # === 5f. å†…å­˜æ¸…ç† ===\n",
        "        if current_iteration_number % 10 == 0:\n",
        "            data_manager.clear_cache()\n",
        "            gc.collect()\n",
        "            logger.info(f\"ç¬¬{current_iteration_number}è½®è¿­ä»£: å·²æ¸…ç†ç¼“å­˜\")\n",
        "    \n",
        "    # === 6. ç”Ÿæˆæœ€ç»ˆç»“æœ ===\n",
        "    logger.info(\"ç”Ÿæˆæœ€ç»ˆå›¾è¡¨...\")\n",
        "    \n",
        "    # æœ€ç»ˆå…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾\n",
        "    if history_best_y:\n",
        "        try:\n",
        "            plot_global_min_value_trend(history_iterations[:len(history_best_y)], history_best_y, results_dir, \n",
        "                                      filename=\"global_min_value_trend_final.png\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"ç»˜åˆ¶æœ€ç»ˆå…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾å¤±è´¥: {e}\")\n",
        "    \n",
        "    # ä¿å­˜æœ€ç»ˆDataManagerçŠ¶æ€\n",
        "    final_data_path = os.path.join(results_dir, \"final_data_manager_state.npz\")\n",
        "    data_manager.save_data(final_data_path)\n",
        "    logger.info(f\"æœ€ç»ˆDataManagerçŠ¶æ€å·²ä¿å­˜åˆ° {final_data_path}\")\n",
        "    \n",
        "    # ä¿å­˜æ€§èƒ½å†å²\n",
        "    try:\n",
        "        perf_data = {\n",
        "            'iterations': history_iterations,\n",
        "            'best_y_values': history_best_y,\n",
        "            'pearson_all': history_pearson_all,\n",
        "            'pearson_best_samples': history_pearson_best_samples,\n",
        "            'nte_candidates_count': history_nte_candidates_count\n",
        "        }\n",
        "        \n",
        "        # ç¡®ä¿æ‰€æœ‰åˆ—è¡¨é•¿åº¦ä¸€è‡´\n",
        "        min_length = min(len(v) if isinstance(v, list) else 0 for v in perf_data.values())\n",
        "        for k, v in perf_data.items():\n",
        "            if isinstance(v, list) and len(v) > min_length:\n",
        "                perf_data[k] = v[:min_length]\n",
        "        \n",
        "        perf_csv_path = os.path.join(results_dir, \"performance_history.csv\")\n",
        "        pd.DataFrame(perf_data).to_csv(perf_csv_path, index=False)\n",
        "        logger.info(f\"æ€§èƒ½å†å²å·²ä¿å­˜è‡³: {perf_csv_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ä¿å­˜æ€§èƒ½å†å²å¤±è´¥: {e}\")\n",
        "    \n",
        "    logger.info(f\"ğŸ‰ DANTEè®­ç»ƒå®Œæˆï¼\")\n",
        "    logger.info(f\"æœ€ç»ˆæœ€ä½³è§£: y = {current_best_y_overall:.4e}\")\n",
        "    logger.info(f\"æ‰€æœ‰ç»“æœå·²ä¿å­˜åœ¨: {results_dir}\")\n",
        "    \n",
        "    return results_dir, current_best_y_overall\n",
        "\n",
        "# æä¾›è®­ç»ƒå¯åŠ¨é€‰é¡¹\n",
        "def start_dante_training():\n",
        "    \"\"\"å¯åŠ¨DANTEè®­ç»ƒçš„ä¾¿æ·å‡½æ•°\"\"\"\n",
        "    print(\"ğŸš€ å‡†å¤‡å¯åŠ¨DANTEè®­ç»ƒ...\")\n",
        "    print(\"âš ï¸  æ³¨æ„ï¼šå®Œæ•´è®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶æ—¶é—´\")\n",
        "    print(\"ğŸ“Š å»ºè®®å…ˆè¿è¡Œç¯å¢ƒéªŒè¯ç¡®ä¿ä¸€åˆ‡æ­£å¸¸\")\n",
        "    \n",
        "    return main_dante_training()\n",
        "\n",
        "# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜\n",
        "print(\"ğŸ“‹ DANTEè®­ç»ƒä½¿ç”¨è¯´æ˜:\")\n",
        "print(\"1. é¦–å…ˆè¿è¡Œ Cell 7 è¿›è¡Œç¯å¢ƒéªŒè¯\")\n",
        "print(\"2. ç¯å¢ƒéªŒè¯é€šè¿‡åï¼Œè¿è¡Œä»¥ä¸‹ä»£ç å¼€å§‹è®­ç»ƒ:\")\n",
        "print(\"   results_dir, best_value = start_dante_training()\")\n",
        "print(\"3. è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ‰€æœ‰ç»“æœå°†ä¿å­˜åˆ° /kaggle/working/dante_results_[timestamp]/ ç›®å½•\")\n",
        "print(\"4. å¯ä»¥é€šè¿‡ä¿®æ”¹ main_dante_training() å‡½æ•°ä¸­çš„å‚æ•°æ¥è°ƒæ•´è®­ç»ƒé…ç½®\")\n",
        "print(\"\\nğŸ”§ å…³é”®è°ƒè¯•å‚æ•°:\")\n",
        "print(\"   - dynamic_exploration_factor: 0.8 + 1 * min(20, 2 ** no_improvement_streak)\")\n",
        "print(\"   - N_AL_ITERATIONS: å½“å‰è®¾ç½®ä¸º100æ¬¡è¿­ä»£ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰\")\n",
        "print(\"   - NTE_ROLLOUT_ROUNDS: NTEå†…éƒ¨æœç´¢è½®æ¬¡\")\n",
        "print(\"   - VALIDATION_BATCH_SIZE: æ¯æ¬¡è¿­ä»£é€‰æ‹©çš„æ ·æœ¬æ•°é‡\")\n",
        "print(\"\\nâœ… å‡†å¤‡å°±ç»ªï¼è¿è¡Œç¯å¢ƒéªŒè¯åå³å¯å¼€å§‹è®­ç»ƒã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: ç¯å¢ƒé…ç½®éªŒè¯ä¸åŠŸèƒ½æµ‹è¯•\n",
        "print(\"=\"*60)\n",
        "print(\"DANTE Kaggleç¯å¢ƒé…ç½®éªŒè¯ä¸åŠŸèƒ½æµ‹è¯•\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥\n",
        "print(\"\\n1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥:\")\n",
        "print(f\"   Pythonç‰ˆæœ¬: {sys.version}\")\n",
        "print(f\"   TensorFlowç‰ˆæœ¬: {tf.__version__}\")\n",
        "print(f\"   PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
        "print(f\"   Numpyç‰ˆæœ¬: {np.__version__}\")\n",
        "print(f\"   Pandasç‰ˆæœ¬: {pd.__version__}\")\n",
        "\n",
        "# 2. GPU/è®¡ç®—èµ„æºæ£€æŸ¥\n",
        "print(\"\\n2. è®¡ç®—èµ„æºæ£€æŸ¥:\")\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(f\"   TensorFlowæ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU:\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"     GPU {i}: {gpu}\")\n",
        "            # é…ç½®GPUå†…å­˜å¢é•¿\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    else:\n",
        "        print(\"   æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
        "    \n",
        "    print(f\"   PyTorch CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   PyTorch GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   GPUæ£€æŸ¥æ—¶å‡ºé”™: {e}\")\n",
        "\n",
        "# 3. æ•°æ®è·¯å¾„æ£€æŸ¥\n",
        "print(\"\\n3. æ•°æ®è·¯å¾„æ£€æŸ¥:\")\n",
        "input_paths = [\n",
        "    \"/kaggle/input/rosenbrock-dante-data\",\n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_train.npy\",\n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_train.npy\",\n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_test.npy\", \n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_test.npy\"\n",
        "]\n",
        "\n",
        "for path in input_paths:\n",
        "    exists = os.path.exists(path)\n",
        "    print(f\"   {path}: {'âœ“å­˜åœ¨' if exists else 'âœ—ç¼ºå¤±'}\")\n",
        "    \n",
        "# 4. æ ¸å¿ƒç»„ä»¶å®ä¾‹åŒ–æµ‹è¯•\n",
        "print(\"\\n4. æ ¸å¿ƒç»„ä»¶å®ä¾‹åŒ–æµ‹è¯•:\")\n",
        "\n",
        "try:\n",
        "    # æµ‹è¯•Rosenbrockå‡½æ•°\n",
        "    test_x = np.array([1.0] * 20)  # å…¨å±€æœ€ä¼˜è§£\n",
        "    test_result = rosenbrock_function(test_x)\n",
        "    print(f\"   Rosenbrockå‡½æ•°æµ‹è¯•: f([1]*20) = {test_result:.2e} {'âœ“æ­£ç¡®' if abs(test_result) < 1e-10 else 'âœ—é”™è¯¯'}\")\n",
        "    \n",
        "    # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
        "    print(\"   ç”Ÿæˆæµ‹è¯•æ•°æ®é›†...\")\n",
        "    test_X = np.random.uniform(-2.048, 2.048, size=(100, 20))\n",
        "    test_y = np.array([rosenbrock_function(x) for x in test_X]).reshape(-1, 1)\n",
        "    print(f\"   æµ‹è¯•æ•°æ®: X.shape={test_X.shape}, y.shape={test_y.shape}\")\n",
        "    print(f\"   yå€¼èŒƒå›´: [{test_y.min():.2e}, {test_y.max():.2e}]\")\n",
        "    \n",
        "    # æµ‹è¯•DataManager\n",
        "    print(\"   æµ‹è¯•DataManager...\")\n",
        "    test_dm = DataManager(test_X[:50], test_y[:50], dimension=20)\n",
        "    print(f\"   DataManageråˆå§‹åŒ–æˆåŠŸ: {test_dm.num_samples}ä¸ªæ ·æœ¬\")\n",
        "    \n",
        "    # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨\n",
        "    print(\"   æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨...\")\n",
        "    test_optimizer = PerformanceOptimizer()\n",
        "    test_optimizer.configure_tensorflow_performance()\n",
        "    print(\"   æ€§èƒ½ä¼˜åŒ–å™¨é…ç½®å®Œæˆ âœ“\")\n",
        "    \n",
        "    # æµ‹è¯•CNN1DSurrogate\n",
        "    print(\"   æµ‹è¯•CNN1DSurrogate...\")\n",
        "    test_cnn = CNN1DSurrogate(input_dim=20)\n",
        "    print(\"   CNNæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ âœ“\")\n",
        "    \n",
        "    # å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ1ä¸ªepochï¼‰\n",
        "    print(\"   å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ1ä¸ªepochï¼‰...\")\n",
        "    training_history = test_cnn.train(\n",
        "        data_manager=test_dm,\n",
        "        epochs=1,\n",
        "        batch_size=16,\n",
        "        validation_split=0.2,\n",
        "        patience=10,\n",
        "        verbose=0\n",
        "    )\n",
        "    print(\"   å¿«é€Ÿè®­ç»ƒå®Œæˆ âœ“\")\n",
        "    \n",
        "    # æµ‹è¯•é¢„æµ‹\n",
        "    test_pred = test_cnn.predict(test_X[:5])\n",
        "    print(f\"   é¢„æµ‹æµ‹è¯•: è¾“å…¥5ä¸ªæ ·æœ¬ï¼Œè¾“å‡ºshape={test_pred.shape}\")\n",
        "    print(f\"   é¢„æµ‹å€¼èŒƒå›´: [{test_pred.min():.2e}, {test_pred.max():.2e}]\")\n",
        "    \n",
        "    # æµ‹è¯•NTEæœç´¢å™¨åˆå§‹åŒ–\n",
        "    print(\"   æµ‹è¯•NTEæœç´¢å™¨...\")\n",
        "    test_nte = NTESearcher(\n",
        "        dimension=20,\n",
        "        domain=[(-2.048, 2.048)] * 20,\n",
        "        c0=1.0,\n",
        "        rollout_rounds=10,  # å‡å°‘è½®æ•°ç”¨äºæµ‹è¯•\n",
        "        validation_batch_size=5\n",
        "    )\n",
        "    print(\"   NTEæœç´¢å™¨åˆå§‹åŒ–æˆåŠŸ âœ“\")\n",
        "    \n",
        "    print(\"\\nâœ“ æ‰€æœ‰æ ¸å¿ƒç»„ä»¶æµ‹è¯•é€šè¿‡ï¼\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âœ— ç»„ä»¶æµ‹è¯•å¤±è´¥: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 5. å†…å­˜å’Œå­˜å‚¨æ£€æŸ¥\n",
        "print(\"\\n5. èµ„æºæ£€æŸ¥:\")\n",
        "try:\n",
        "    # æ£€æŸ¥å¯ç”¨å†…å­˜\n",
        "    import psutil\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"   å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB / {memory.total / (1024**3):.1f} GB\")\n",
        "    \n",
        "    # æ£€æŸ¥ç£ç›˜ç©ºé—´\n",
        "    disk = psutil.disk_usage('/kaggle/working')\n",
        "    print(f\"   å¯ç”¨ç£ç›˜: {disk.free / (1024**3):.1f} GB / {disk.total / (1024**3):.1f} GB\")\n",
        "    \n",
        "    # æ£€æŸ¥CPU\n",
        "    print(f\"   CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   èµ„æºæ£€æŸ¥å‡ºé”™: {e}\")\n",
        "\n",
        "# 6. ç”Ÿæˆé…ç½®æ‘˜è¦\n",
        "print(\"\\n6. è¿è¡Œé…ç½®æ‘˜è¦:\")\n",
        "print(\"   DANTEä¼˜åŒ–é…ç½®:\")\n",
        "print(\"   - ç›®æ ‡å‡½æ•°: 20ç»´Rosenbrockå‡½æ•°\")\n",
        "print(\"   - åˆå§‹æ ·æœ¬æ•°: 800\")\n",
        "print(\"   - ä¸»åŠ¨å­¦ä¹ è¿­ä»£æ•°: 400\")  \n",
        "print(\"   - æ¯æ¬¡è¿­ä»£æ‰¹é‡å¤§å°: 20\")\n",
        "print(\"   - CNNè®­ç»ƒepochs: 200\")\n",
        "print(\"   - NTE rolloutè½®æ•°: 100\")\n",
        "print(\"   - åŠ¨æ€æ¢ç´¢å› å­: 0.8 + 1 * min(20, 2^no_improvement_streak)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ç¯å¢ƒéªŒè¯å®Œæˆï¼å‡†å¤‡å¼€å§‹DANTEè®­ç»ƒ...\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: DANTEä¸»è®­ç»ƒå¾ªç¯\n",
        "def run_dante_optimization():\n",
        "    \"\"\"è¿è¡ŒDANTEä¼˜åŒ–ä¸»å¾ªç¯\"\"\"\n",
        "    print(\"å¼€å§‹DANTEä¼˜åŒ–...\")\n",
        "    \n",
        "    # ===== é…ç½®å‚æ•° =====\n",
        "    ROSENBROCK_DIMENSION = 20\n",
        "    DOMAIN_MIN, DOMAIN_MAX = -2.048, 2.048\n",
        "    ROSENBROCK_DOMAIN = [(DOMAIN_MIN, DOMAIN_MAX)] * ROSENBROCK_DIMENSION\n",
        "    \n",
        "    # è®­ç»ƒå‚æ•° - å¯è°ƒæ•´ç”¨äºè°ƒè¯•åŠ¨æ€æ¢ç´¢å› å­\n",
        "    INITIAL_DATA_SIZE = 800\n",
        "    N_AL_ITERATIONS = 400\n",
        "    CNN_TRAINING_EPOCHS = 200\n",
        "    CNN_EARLY_STOPPING_PATIENCE = 30\n",
        "    CNN_VALIDATION_SPLIT = 0.2\n",
        "    CNN_BATCH_SIZE = 32\n",
        "    NTE_C0 = 1\n",
        "    NTE_ROLLOUT_ROUNDS = 100\n",
        "    VALIDATION_BATCH_SIZE = 20\n",
        "    \n",
        "    # å¯è§†åŒ–é—´éš”ï¼ˆå‡å°‘è¾“å‡ºé‡ï¼‰\n",
        "    viz_interval = 10\n",
        "    \n",
        "    print(f\"é…ç½®å‚æ•°:\")\n",
        "    print(f\"- ç»´åº¦: {ROSENBROCK_DIMENSION}D\")\n",
        "    print(f\"- åˆå§‹æ ·æœ¬: {INITIAL_DATA_SIZE}\")\n",
        "    print(f\"- ALè¿­ä»£: {N_AL_ITERATIONS}\")\n",
        "    print(f\"- æ¯æ‰¹æ ·æœ¬: {VALIDATION_BATCH_SIZE}\")\n",
        "    print(f\"- CNNè®­ç»ƒepochs: {CNN_TRAINING_EPOCHS}\")\n",
        "    print(f\"- NTE rolloutè½®æ•°: {NTE_ROLLOUT_ROUNDS}\")\n",
        "    \n",
        "    # ===== åŠ è½½æ•°æ® =====\n",
        "    print(\"\\nåŠ è½½æ•°æ®...\")\n",
        "    try:\n",
        "        # å°è¯•ä»Kaggleæ•°æ®é›†åŠ è½½\n",
        "        X_train_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_train.npy\"\n",
        "        y_train_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_train.npy\"\n",
        "        X_test_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_test.npy\"\n",
        "        y_test_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_test.npy\"\n",
        "        \n",
        "        if os.path.exists(X_train_path) and os.path.exists(y_train_path):\n",
        "            initial_X = np.load(X_train_path)\n",
        "            initial_y = np.load(y_train_path)\n",
        "            if initial_y.ndim == 1:\n",
        "                initial_y = initial_y.reshape(-1, 1)\n",
        "            print(f\"ä»Kaggleæ•°æ®é›†åŠ è½½è®­ç»ƒæ•°æ®: {initial_X.shape}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨\")\n",
        "            \n",
        "        if os.path.exists(X_test_path) and os.path.exists(y_test_path):\n",
        "            X_test = np.load(X_test_path)\n",
        "            y_test = np.load(y_test_path)\n",
        "            if y_test.ndim == 1:\n",
        "                y_test = y_test.reshape(-1, 1)\n",
        "            print(f\"ä»Kaggleæ•°æ®é›†åŠ è½½æµ‹è¯•æ•°æ®: {X_test.shape}\")\n",
        "        else:\n",
        "            X_test, y_test = None, None\n",
        "            print(\"æµ‹è¯•æ•°æ®ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"æ•°æ®åŠ è½½å¤±è´¥: {e}\")\n",
        "        print(\"ç”Ÿæˆéšæœºæ•°æ®...\")\n",
        "        initial_X = np.random.uniform(DOMAIN_MIN, DOMAIN_MAX, size=(INITIAL_DATA_SIZE, ROSENBROCK_DIMENSION))\n",
        "        initial_y = np.array([rosenbrock_function(x) for x in initial_X]).reshape(-1, 1)\n",
        "        \n",
        "        # ç”Ÿæˆæµ‹è¯•é›†\n",
        "        test_size = 200\n",
        "        X_test = np.random.uniform(DOMAIN_MIN, DOMAIN_MAX, size=(test_size, ROSENBROCK_DIMENSION))\n",
        "        y_test = np.array([rosenbrock_function(x) for x in X_test]).reshape(-1, 1)\n",
        "        print(f\"ç”Ÿæˆéšæœºæ•°æ®: è®­ç»ƒ{initial_X.shape}, æµ‹è¯•{X_test.shape}\")\n",
        "    \n",
        "    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆå§‹æ ·æœ¬\n",
        "    if initial_X.shape[0] < INITIAL_DATA_SIZE:\n",
        "        additional_needed = INITIAL_DATA_SIZE - initial_X.shape[0]\n",
        "        additional_X = np.random.uniform(DOMAIN_MIN, DOMAIN_MAX, size=(additional_needed, ROSENBROCK_DIMENSION))\n",
        "        additional_y = np.array([rosenbrock_function(x) for x in additional_X]).reshape(-1, 1)\n",
        "        initial_X = np.vstack((initial_X, additional_X))\n",
        "        initial_y = np.vstack((initial_y, additional_y))\n",
        "        print(f\"è¡¥å……{additional_needed}ä¸ªéšæœºæ ·æœ¬ï¼Œæ€»è®¡{initial_X.shape[0]}ä¸ª\")\n",
        "    \n",
        "    # ===== åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ =====\n",
        "    print(\"\\nåˆå§‹åŒ–ç»„ä»¶...\")\n",
        "    \n",
        "    # æ•°æ®ç®¡ç†å™¨\n",
        "    data_manager = DataManager(initial_X, initial_y, dimension=ROSENBROCK_DIMENSION)\n",
        "    print(f\"DataManageråˆå§‹åŒ–: {data_manager.num_samples}ä¸ªæ ·æœ¬\")\n",
        "    \n",
        "    # æ€§èƒ½ä¼˜åŒ–å™¨\n",
        "    performance_optimizer = PerformanceOptimizer()\n",
        "    performance_optimizer.configure_tensorflow_performance()\n",
        "    \n",
        "    # CNNæ›¿ä»£æ¨¡å‹\n",
        "    cnn_model = CNN1DSurrogate(input_dim=ROSENBROCK_DIMENSION)\n",
        "    print(\"CNN1DSurrogateæ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # NTEæœç´¢å™¨\n",
        "    nte_searcher = NTESearcher(\n",
        "        dimension=ROSENBROCK_DIMENSION,\n",
        "        domain=ROSENBROCK_DOMAIN,\n",
        "        c0=NTE_C0,\n",
        "        rollout_rounds=NTE_ROLLOUT_ROUNDS,\n",
        "        validation_batch_size=VALIDATION_BATCH_SIZE\n",
        "    )\n",
        "    print(\"NTESearcheråˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # ===== å†å²è®°å½•å˜é‡ =====\n",
        "    history_iterations = []\n",
        "    history_best_y = []\n",
        "    history_pearson_all = []\n",
        "    history_pearson_best_samples = []\n",
        "    history_nte_candidates_count = []\n",
        "    cnn_performance_history = []\n",
        "    \n",
        "    # ä¼˜åŒ–ç›¸å…³å˜é‡\n",
        "    no_improvement_streak = 0\n",
        "    previous_best_y = float('inf')\n",
        "    previous_iteration_samples_x = None\n",
        "    \n",
        "    # è·å–åˆå§‹æœ€ä¼˜å€¼\n",
        "    current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "    print(f\"åˆå§‹æœ€ä¼˜å€¼: {current_best_y_overall:.6e}\")\n",
        "    \n",
        "    # ===== ä¸»è®­ç»ƒå¾ªç¯ =====\n",
        "    print(f\"\\nå¼€å§‹{N_AL_ITERATIONS}æ¬¡ä¸»åŠ¨å­¦ä¹ è¿­ä»£...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for al_iteration in range(N_AL_ITERATIONS):\n",
        "        current_iteration_number = al_iteration + 1\n",
        "        history_iterations.append(current_iteration_number)\n",
        "        \n",
        "        # å†³å®šæ˜¯å¦ç”Ÿæˆè¯¦ç»†å¯è§†åŒ–\n",
        "        should_generate_plots = ((current_iteration_number % viz_interval == 0) or \n",
        "                                (current_iteration_number == N_AL_ITERATIONS))\n",
        "        \n",
        "        if current_iteration_number % 50 == 0 or current_iteration_number <= 5:\n",
        "            print(f\"\\n--- è¿­ä»£ {current_iteration_number}/{N_AL_ITERATIONS} ---\")\n",
        "        \n",
        "        # === 1. è®­ç»ƒCNNæ›¿ä»£æ¨¡å‹ ===\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=CNN_TRAINING_EPOCHS,\n",
        "            batch_size=CNN_BATCH_SIZE,\n",
        "            validation_split=CNN_VALIDATION_SPLIT,\n",
        "            patience=CNN_EARLY_STOPPING_PATIENCE,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # === 2. è¯„ä¼°æ¨¡å‹æ€§èƒ½ ===\n",
        "        if X_test is not None:\n",
        "            current_pearson = calculate_pearson_on_test_set(cnn_model, X_test, y_test, data_manager)\n",
        "        else:\n",
        "            current_pearson = calculate_pearson_correlation(cnn_model, data_manager)\n",
        "        \n",
        "        history_pearson_all.append(current_pearson)\n",
        "        \n",
        "        # è®¡ç®—æœ€ä½³æ ·æœ¬ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\n",
        "        current_pearson_best_samples, _, _ = calculate_pearson_on_best_samples(cnn_model, data_manager)\n",
        "        history_pearson_best_samples.append(current_pearson_best_samples)\n",
        "        \n",
        "        # === 3. NTEæœç´¢æ–°å€™é€‰ç‚¹ ===\n",
        "        # è®¡ç®—åŠ¨æ€æ¢ç´¢å› å­ï¼ˆå…³é”®è°ƒè¯•å‚æ•°ï¼‰\n",
        "        dynamic_exploration_factor = 0.8 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "        \n",
        "        if current_iteration_number % 50 == 0 or current_iteration_number <= 5:\n",
        "            print(f\"åŠ¨æ€æ¢ç´¢å› å­: {dynamic_exploration_factor:.2f} (æ— æ”¹è¿›è¿ç»­æ¬¡æ•°: {no_improvement_streak})\")\n",
        "        \n",
        "        # NTEæœç´¢åŒ…è£…å™¨\n",
        "        class NTESurrogateWrapper:\n",
        "            def __init__(self, model, dm):\n",
        "                self.model = model\n",
        "                self.dm = dm\n",
        "            \n",
        "            def predict(self, X_batch):\n",
        "                if X_batch.shape[0] == 0:\n",
        "                    return np.array([])\n",
        "                if X_batch.ndim == 1:\n",
        "                    X_batch = X_batch.reshape(1, -1)\n",
        "                \n",
        "                try:\n",
        "                    pred_log = self.model.predict(X_batch)\n",
        "                    return pred_log[0] if X_batch.shape[0] == 1 else pred_log\n",
        "                except Exception as e:\n",
        "                    print(f\"é¢„æµ‹é”™è¯¯: {e}\")\n",
        "                    return np.log10(1e6 + 1.0) if X_batch.shape[0] == 1 else np.ones(X_batch.shape[0]) * np.log10(1e6 + 1.0)\n",
        "        \n",
        "        nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)\n",
        "        \n",
        "        try:\n",
        "            # æ‰§è¡ŒNTEæœç´¢\n",
        "            nte_search_result = nte_searcher.search(\n",
        "                surrogate_model=nte_model_wrapper,\n",
        "                current_best_state=current_best_x_orig,\n",
        "                min_y_observed=current_best_y_overall,\n",
        "                dynamic_exploration_factor=dynamic_exploration_factor,\n",
        "                all_states=data_manager.X_data,\n",
        "                all_values=data_manager.get_all_y_orig(),\n",
        "                previous_iteration_samples=previous_iteration_samples_x,\n",
        "                current_iteration=al_iteration,\n",
        "                return_ducb_trends=True\n",
        "            )\n",
        "            \n",
        "            nte_selected_samples_x, ducb_trends_data = nte_search_result\n",
        "            \n",
        "            if isinstance(nte_selected_samples_x, list):\n",
        "                nte_selected_samples_x = np.array(nte_selected_samples_x)\n",
        "            \n",
        "            if nte_selected_samples_x.shape[0] > 0:\n",
        "                # è¯„ä¼°æ–°æ ·æœ¬\n",
        "                nte_selected_samples_y_true = np.array([rosenbrock_function(x) for x in nte_selected_samples_x]).reshape(-1, 1)\n",
        "                \n",
        "                # æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨\n",
        "                data_manager.add_samples(nte_selected_samples_x, nte_selected_samples_y_true, re_normalize=True, shuffle=True)\n",
        "                \n",
        "                # æ›´æ–°æƒé‡ï¼ˆä¸ºæ–°æ ·æœ¬è®¾ç½®è¾ƒé«˜æƒé‡ï¼‰\n",
        "                num_new_samples = nte_selected_samples_x.shape[0]\n",
        "                new_weights = np.ones(data_manager.num_samples)\n",
        "                new_weights[-num_new_samples:] = 4.0  # æ–°æ ·æœ¬æƒé‡ä¸º4.0\n",
        "                data_manager.last_batch_weights = torch.tensor(new_weights, dtype=torch.float32)\n",
        "                \n",
        "                history_nte_candidates_count.append(num_new_samples)\n",
        "                previous_iteration_samples_x = nte_selected_samples_x.copy()\n",
        "                \n",
        "                # æ›´æ–°æœ€ä¼˜å€¼\n",
        "                current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "                history_best_y.append(current_best_y_overall)\n",
        "                \n",
        "                # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›\n",
        "                iteration_improved = current_best_y_overall < previous_best_y\n",
        "                if iteration_improved:\n",
        "                    no_improvement_streak = 0\n",
        "                    if current_iteration_number % 10 == 0 or current_iteration_number <= 10:\n",
        "                        print(f\"å‘ç°æ›´ä¼˜è§£: {current_best_y_overall:.6e}\")\n",
        "                else:\n",
        "                    no_improvement_streak += 1\n",
        "                \n",
        "                previous_best_y = current_best_y_overall\n",
        "                \n",
        "            else:\n",
        "                history_nte_candidates_count.append(0)\n",
        "                history_best_y.append(history_best_y[-1] if history_best_y else current_best_y_overall)\n",
        "                no_improvement_streak += 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"NTEæœç´¢å¤±è´¥: {e}\")\n",
        "            history_nte_candidates_count.append(0)\n",
        "            history_best_y.append(history_best_y[-1] if history_best_y else current_best_y_overall)\n",
        "            no_improvement_streak += 1\n",
        "        \n",
        "        # === 4. æ€§èƒ½è®°å½•å’Œå¯è§†åŒ– ===\n",
        "        perf_metrics = {\n",
        "            'iteration': current_iteration_number,\n",
        "            'best_y': current_best_y_overall,\n",
        "            'pearson_test': current_pearson,\n",
        "            'pearson_best_samples': current_pearson_best_samples,\n",
        "            'num_samples': data_manager.num_samples,\n",
        "            'no_improvement_streak': no_improvement_streak,\n",
        "            'dynamic_exploration_factor': dynamic_exploration_factor\n",
        "        }\n",
        "        cnn_performance_history.append(perf_metrics)\n",
        "        \n",
        "        # å®šæœŸå¯è§†åŒ–\n",
        "        if should_generate_plots:\n",
        "            print(f\"ç”Ÿæˆç¬¬{current_iteration_number}è½®å¯è§†åŒ–å›¾è¡¨...\")\n",
        "            \n",
        "            # å…¨å±€æœ€å°å€¼è¶‹åŠ¿\n",
        "            plot_global_min_value_trend(\n",
        "                history_iterations[:len(history_best_y)], \n",
        "                history_best_y, \n",
        "                \"/kaggle/working\", \n",
        "                filename=f\"global_min_iter_{current_iteration_number}.png\"\n",
        "            )\n",
        "            \n",
        "            # çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿\n",
        "            plot_pearson_correlation_trend(\n",
        "                history_iterations, \n",
        "                history_pearson_all, \n",
        "                \"/kaggle/working\",\n",
        "                filename=f\"pearson_trend_iter_{current_iteration_number}.png\",\n",
        "                title=\"Test Set Pearson Correlation\"\n",
        "            )\n",
        "            \n",
        "            # æœ€ä½³æ ·æœ¬çš®å°”é€Šè¶‹åŠ¿\n",
        "            plot_best_samples_pearson_trend(\n",
        "                history_iterations, \n",
        "                history_pearson_best_samples, \n",
        "                \"/kaggle/working\",\n",
        "                filename=f\"best_samples_pearson_iter_{current_iteration_number}.png\"\n",
        "            )\n",
        "            \n",
        "            # NTEæ€§èƒ½\n",
        "            plot_nte_performance(\n",
        "                history_iterations, \n",
        "                history_nte_candidates_count, \n",
        "                \"/kaggle/working\",\n",
        "                filename=f\"nte_performance_iter_{current_iteration_number}.png\"\n",
        "            )\n",
        "        \n",
        "        # å†…å­˜æ¸…ç†\n",
        "        if current_iteration_number % 10 == 0:\n",
        "            data_manager.clear_cache()\n",
        "            performance_optimizer.clear_memory_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "        \n",
        "        # è¿›åº¦æŠ¥å‘Š\n",
        "        if current_iteration_number % 50 == 0:\n",
        "            print(f\"è¿›åº¦: {current_iteration_number}/{N_AL_ITERATIONS} ({100*current_iteration_number/N_AL_ITERATIONS:.1f}%)\")\n",
        "            print(f\"å½“å‰æœ€ä¼˜: {current_best_y_overall:.6e}\")\n",
        "            print(f\"æ ·æœ¬æ€»æ•°: {data_manager.num_samples}\")\n",
        "            print(f\"æ— æ”¹è¿›è¿ç»­æ¬¡æ•°: {no_improvement_streak}\")\n",
        "    \n",
        "    # ===== è®­ç»ƒå®Œæˆ =====\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DANTEè®­ç»ƒå®Œæˆ!\")\n",
        "    print(f\"æœ€ç»ˆæœ€ä¼˜å€¼: {current_best_y_overall:.6e}\")\n",
        "    print(f\"æœ€ä¼˜è§£ä½ç½®: {current_best_x_orig}\")\n",
        "    print(f\"æ€»æ ·æœ¬æ•°: {data_manager.num_samples}\")\n",
        "    print(f\"è®­ç»ƒè¿­ä»£æ•°: {N_AL_ITERATIONS}\")\n",
        "    \n",
        "    # ä¿å­˜æœ€ç»ˆç»“æœ\n",
        "    results = {\n",
        "        'final_best_value': current_best_y_overall,\n",
        "        'final_best_x': current_best_x_orig,\n",
        "        'total_samples': data_manager.num_samples,\n",
        "        'iterations': N_AL_ITERATIONS,\n",
        "        'history_best_y': history_best_y,\n",
        "        'history_pearson': history_pearson_all,\n",
        "        'cnn_performance_history': cnn_performance_history\n",
        "    }\n",
        "    \n",
        "    # ä¿å­˜ä¸ºnumpyæ–‡ä»¶\n",
        "    np.savez(\"/kaggle/working/dante_results.npz\", **results)\n",
        "    print(\"ç»“æœå·²ä¿å­˜åˆ° /kaggle/working/dante_results.npz\")\n",
        "    \n",
        "    # ç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–\n",
        "    print(\"\\nç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–å›¾è¡¨...\")\n",
        "    plot_global_min_value_trend(\n",
        "        history_iterations[:len(history_best_y)], \n",
        "        history_best_y, \n",
        "        \"/kaggle/working\", \n",
        "        filename=\"final_global_min_trend.png\"\n",
        "    )\n",
        "    \n",
        "    plot_pearson_correlation_trend(\n",
        "        history_iterations, \n",
        "        history_pearson_all, \n",
        "        \"/kaggle/working\",\n",
        "        filename=\"final_pearson_trend.png\",\n",
        "        title=\"Final Pearson Correlation Trend\"\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "# å®šä¹‰è¾…åŠ©å‡½æ•°\n",
        "def calculate_pearson_on_test_set(model, X_test, y_test_orig, data_manager):\n",
        "    \"\"\"è®¡ç®—æµ‹è¯•é›†ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    if X_test.shape[0] < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(X_test)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2:\n",
        "            return 0.0\n",
        "            \n",
        "        from scipy.stats import pearsonr\n",
        "        correlation, _ = pearsonr(predictions_orig, y_test_orig.flatten())\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"è®¡ç®—æµ‹è¯•é›†çš®å°”é€Šç›¸å…³ç³»æ•°å¤±è´¥: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_best_samples(model, data_manager, n_best=60):\n",
        "    \"\"\"è®¡ç®—æœ€ä½³æ ·æœ¬ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    all_X = data_manager.get_all_x_orig()\n",
        "    all_y_orig = data_manager.get_all_y_orig()\n",
        "    \n",
        "    if all_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "    \n",
        "    # æŒ‰yå€¼æ’åºï¼Œå–æœ€å°çš„n_bestä¸ª\n",
        "    sorted_indices = np.argsort(all_y_orig.flatten())\n",
        "    best_indices = sorted_indices[:n_best]\n",
        "    \n",
        "    sorted_X = all_X[best_indices]\n",
        "    sorted_y_orig = all_y_orig[best_indices]\n",
        "    \n",
        "    if sorted_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "    \n",
        "    min_y = np.min(sorted_y_orig)\n",
        "    max_y = np.max(sorted_y_orig)\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(sorted_X)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if np.any(np.isinf(predictions_orig)) or np.any(np.isnan(predictions_orig)):\n",
        "            return 0.0, min_y, max_y\n",
        "        \n",
        "        from scipy.stats import pearsonr\n",
        "        correlation, _ = pearsonr(predictions_orig, sorted_y_orig.flatten())\n",
        "        return (correlation if not np.isnan(correlation) else 0.0), min_y, max_y\n",
        "        \n",
        "    except Exception as e:\n",
        "        return 0.0, min_y, max_y\n",
        "\n",
        "# è¿è¡Œè¯´æ˜\n",
        "print(\"DANTEè®­ç»ƒå¾ªç¯å·²å®šä¹‰å®Œæˆ!\")\n",
        "print(\"\\nè¦å¼€å§‹è®­ç»ƒï¼Œè¯·è¿è¡Œ:\")\n",
        "print(\"results = run_dante_optimization()\")\n",
        "print(\"\\næ³¨æ„: å®Œæ•´è®­ç»ƒå¯èƒ½éœ€è¦8-12å°æ—¶ï¼Œå»ºè®®åœ¨P100 GPUç¯å¢ƒä¸‹è¿è¡Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: ç¯å¢ƒéªŒè¯ä¸å®Œæ•´æ€§æ£€æŸ¥\n",
        "print(\"=\"*80)\n",
        "print(\"DANTE-Kaggle ç¯å¢ƒéªŒè¯ä¸å®Œæ•´æ€§æ£€æŸ¥\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. ç¡¬ä»¶ç¯å¢ƒéªŒè¯\n",
        "print(\"\\n1. ç¡¬ä»¶ç¯å¢ƒéªŒè¯\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# GPUæ£€æŸ¥\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"âœ… GPUå¯ç”¨: {gpu_name}\")\n",
        "    print(f\"   æ˜¾å­˜: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    # æµ‹è¯•GPUå†…å­˜åˆ†é…\n",
        "    try:\n",
        "        test_tensor = torch.ones(1000, 1000).cuda()\n",
        "        del test_tensor\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"âœ… GPUå†…å­˜åˆ†é…æµ‹è¯•é€šè¿‡\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GPUå†…å­˜åˆ†é…å¤±è´¥: {e}\")\n",
        "else:\n",
        "    print(\"âŒ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU\")\n",
        "\n",
        "# TensorFlow GPUæ£€æŸ¥\n",
        "tf_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if tf_gpus:\n",
        "    print(f\"âœ… TensorFlow GPUå¯ç”¨: {len(tf_gpus)}ä¸ªè®¾å¤‡\")\n",
        "else:\n",
        "    print(\"âŒ TensorFlowæœªæ£€æµ‹åˆ°GPU\")\n",
        "\n",
        "# 2. æ•°æ®éªŒè¯\n",
        "print(\"\\n2. æ•°æ®å®Œæ•´æ€§éªŒè¯\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "data_status = {\"train_x\": False, \"train_y\": False, \"test_x\": False, \"test_y\": False}\n",
        "data_shapes = {}\n",
        "\n",
        "for key, filename in [(\"train_x\", \"Rosenbrock_x_train.npy\"), \n",
        "                      (\"train_y\", \"Rosenbrock_y_train.npy\"),\n",
        "                      (\"test_x\", \"Rosenbrock_x_test.npy\"), \n",
        "                      (\"test_y\", \"Rosenbrock_y_test.npy\")]:\n",
        "    filepath = os.path.join(DATA_PATH, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        try:\n",
        "            data = np.load(filepath)\n",
        "            data_status[key] = True\n",
        "            data_shapes[key] = data.shape\n",
        "            print(f\"âœ… {filename}: {data.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {filename}: åŠ è½½å¤±è´¥ - {e}\")\n",
        "    else:\n",
        "        print(f\"âŒ {filename}: æ–‡ä»¶ä¸å­˜åœ¨\")\n",
        "\n",
        "# éªŒè¯æ•°æ®ç»´åº¦ä¸€è‡´æ€§\n",
        "if data_status[\"train_x\"] and data_status[\"train_y\"]:\n",
        "    if data_shapes[\"train_x\"][0] == data_shapes[\"train_y\"][0]:\n",
        "        print(\"âœ… è®­ç»ƒæ•°æ®X,Yæ ·æœ¬æ•°ä¸€è‡´\")\n",
        "    else:\n",
        "        print(f\"âŒ è®­ç»ƒæ•°æ®ç»´åº¦ä¸åŒ¹é…: X={data_shapes['train_x']}, Y={data_shapes['train_y']}\")\n",
        "\n",
        "# 3. æ ¸å¿ƒåŠŸèƒ½éªŒè¯\n",
        "print(\"\\n3. æ ¸å¿ƒåŠŸèƒ½æ¨¡å—éªŒè¯\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# æµ‹è¯•Rosenbrockå‡½æ•°\n",
        "try:\n",
        "    test_x = np.random.uniform(-2, 2, 20)\n",
        "    result = rosenbrock_function(test_x)\n",
        "    print(f\"âœ… Rosenbrockå‡½æ•°: è¾“å…¥ç»´åº¦{test_x.shape} â†’ è¾“å‡º{result:.4e}\")\n",
        "    \n",
        "    # æ‰¹é‡æµ‹è¯•\n",
        "    batch_x = np.random.uniform(-2, 2, (5, 20))\n",
        "    batch_result = rosenbrock_function(batch_x)\n",
        "    print(f\"âœ… Rosenbrockæ‰¹é‡å‡½æ•°: è¾“å…¥{batch_x.shape} â†’ è¾“å‡º{batch_result.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Rosenbrockå‡½æ•°æµ‹è¯•å¤±è´¥: {e}\")\n",
        "\n",
        "# æµ‹è¯•safe_power10å‡½æ•°\n",
        "try:\n",
        "    test_log = np.array([1.0, 2.0, 3.0])\n",
        "    result = safe_power10(test_log)\n",
        "    print(f\"âœ… safe_power10å‡½æ•°: è¾“å…¥{test_log} â†’ è¾“å‡º{result}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ safe_power10å‡½æ•°æµ‹è¯•å¤±è´¥: {e}\")\n",
        "\n",
        "# 4. å†…å­˜çŠ¶æ€æ£€æŸ¥\n",
        "print(\"\\n4. ç³»ç»Ÿèµ„æºæ£€æŸ¥\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# å†…å­˜æ£€æŸ¥\n",
        "try:\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    available_gb = memory_info.available / 1e9\n",
        "    total_gb = memory_info.total / 1e9\n",
        "    print(f\"âœ… ç³»ç»Ÿå†…å­˜: {available_gb:.1f}GB å¯ç”¨ / {total_gb:.1f}GB æ€»è®¡\")\n",
        "    \n",
        "    if available_gb < 8:\n",
        "        print(\"âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³8GBï¼Œå¯èƒ½å½±å“è®­ç»ƒæ€§èƒ½\")\n",
        "    else:\n",
        "        print(\"âœ… å†…å­˜å……è¶³ï¼Œå¯æ”¯æŒå®Œæ•´è®­ç»ƒ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ å†…å­˜æ£€æŸ¥å¤±è´¥: {e}\")\n",
        "\n",
        "# 5. æ¨¡å—å¯¼å…¥éªŒè¯\n",
        "print(\"\\n5. å…³é”®æ¨¡å—éªŒè¯\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "modules_to_test = [\n",
        "    (\"numpy\", np),\n",
        "    (\"pandas\", pd), \n",
        "    (\"torch\", torch),\n",
        "    (\"tensorflow\", tf),\n",
        "    (\"matplotlib\", plt),\n",
        "    (\"scipy.stats\", pearsonr),\n",
        "]\n",
        "\n",
        "for name, module in modules_to_test:\n",
        "    try:\n",
        "        # ç®€å•åŠŸèƒ½æµ‹è¯•\n",
        "        if name == \"numpy\":\n",
        "            test_array = module.array([1, 2, 3])\n",
        "        elif name == \"torch\":\n",
        "            test_tensor = module.tensor([1, 2, 3])\n",
        "        elif name == \"tensorflow\":\n",
        "            test_tf = module.constant([1, 2, 3])\n",
        "        print(f\"âœ… {name}: å¯¼å…¥æˆåŠŸ\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {name}: æµ‹è¯•å¤±è´¥ - {e}\")\n",
        "\n",
        "# 6. é¢„æœŸæ€§èƒ½ä¼°ç®—\n",
        "print(\"\\n6. è®­ç»ƒæ€§èƒ½é¢„ä¼°\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "ROSENBROCK_DIMENSION = 20\n",
        "INITIAL_DATA_SIZE = 800\n",
        "N_AL_ITERATIONS = 400\n",
        "CNN_TRAINING_EPOCHS = 200\n",
        "\n",
        "estimated_time_per_cnn = 30  # ç§’\n",
        "estimated_time_per_nte = 10  # ç§’\n",
        "total_estimated_time = N_AL_ITERATIONS * (estimated_time_per_cnn + estimated_time_per_nte)\n",
        "\n",
        "print(f\"é…ç½®å‚æ•°:\")\n",
        "print(f\"  - ç»´åº¦: {ROSENBROCK_DIMENSION}D\")\n",
        "print(f\"  - åˆå§‹æ ·æœ¬: {INITIAL_DATA_SIZE}\")\n",
        "print(f\"  - ALè¿­ä»£: {N_AL_ITERATIONS}\")\n",
        "print(f\"  - CNNè½®æ•°: {CNN_TRAINING_EPOCHS}\")\n",
        "print(f\"é¢„ä¼°è®­ç»ƒæ—¶é—´: {total_estimated_time/3600:.1f} å°æ—¶\")\n",
        "\n",
        "if total_estimated_time > 43200:  # 12å°æ—¶\n",
        "    print(\"âš ï¸  é¢„ä¼°æ—¶é—´è¶…è¿‡12å°æ—¶ï¼Œå»ºè®®åœ¨P100ç¯å¢ƒä¸‹è¿è¡Œ\")\n",
        "else:\n",
        "    print(\"âœ… é¢„ä¼°æ—¶é—´åˆç†ï¼Œå¯åœ¨Kaggleç¯å¢ƒå®Œæˆ\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ç¯å¢ƒéªŒè¯å®Œæˆï¼å¦‚æœæ‰€æœ‰é¡¹ç›®éƒ½æ˜¾ç¤ºâœ…ï¼Œåˆ™å¯ä»¥å¼€å§‹è®­ç»ƒã€‚\")\n",
        "print(\"å¦‚æœæœ‰âŒé¡¹ç›®ï¼Œè¯·æ£€æŸ¥å¯¹åº”çš„ç¯å¢ƒé…ç½®ã€‚\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: NTE Searcher æ ¸å¿ƒç®—æ³•æ–¹æ³•\n",
        "\n",
        "def _calculate_ducb(self, node, parent_total_visits, c_rho, current_c0):\n",
        "    \"\"\"è®¡ç®—DUCBå€¼\"\"\"\n",
        "    if node.visit_count < 0:\n",
        "        raise ValueError(\"Node visit count cannot be negative.\")\n",
        "    if parent_total_visits <= 0:\n",
        "        parent_total_visits = 1\n",
        "\n",
        "    exploitation_term = -node.value_pred\n",
        "    n = node.visit_count\n",
        "    N = parent_total_visits\n",
        "    \n",
        "    log_term = math.log(max(N, 1))\n",
        "    exploration_term = current_c0 * c_rho * math.sqrt((2 * log_term) / (n + 1))\n",
        "\n",
        "    return exploitation_term + exploration_term\n",
        "\n",
        "def _calculate_ducb_with_global_awareness(self, node, parent_total_visits, \n",
        "                                        c_rho, current_c0,\n",
        "                                        global_access_record, nte_id, \n",
        "                                        state_tuple):\n",
        "    \"\"\"è®¡ç®—è€ƒè™‘å…¨å±€è®¿é—®æƒ…å†µçš„DUCBå€¼\"\"\"\n",
        "    if node.visit_count < 0:\n",
        "        raise ValueError(\"Node visit count cannot be negative.\")\n",
        "    if parent_total_visits <= 0:\n",
        "        parent_total_visits = 1\n",
        "\n",
        "    exploitation_term = -node.value_pred\n",
        "    n = node.visit_count\n",
        "    \n",
        "    # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤è®¿é—®çš„èŠ‚ç‚¹\n",
        "    if (state_tuple in global_access_record and \n",
        "        len(global_access_record[state_tuple]['nte_visits']) > 1):\n",
        "        # è¿™æ˜¯ä¸€ä¸ªè¢«å¤šä¸ªNTEè®¿é—®è¿‡çš„èŠ‚ç‚¹\n",
        "        # ä½¿ç”¨å¢å¼ºçš„Nå€¼æ¥æé«˜æ¢ç´¢æ€§\n",
        "        other_nte_visits = sum(\n",
        "            visits for other_nte_id, visits in global_access_record[state_tuple]['nte_visits'].items()\n",
        "            if other_nte_id != nte_id\n",
        "        )\n",
        "        enhanced_N = parent_total_visits + other_nte_visits\n",
        "        \n",
        "        logger.debug(f\"é‡å¤è®¿é—®èŠ‚ç‚¹æ£€æµ‹: åŸN={parent_total_visits}, å…¶ä»–NTEè®¿é—®={other_nte_visits}, å¢å¼ºN={enhanced_N}\")\n",
        "    else:\n",
        "        # æ­£å¸¸æƒ…å†µï¼Œä½¿ç”¨å½“å‰NTEçš„æ€»è®¿é—®æ¬¡æ•°\n",
        "        enhanced_N = parent_total_visits\n",
        "    \n",
        "    log_term = math.log(max(enhanced_N, 1))\n",
        "    exploration_term = current_c0 * c_rho * math.sqrt((2 * log_term) / (n + 1))\n",
        "\n",
        "    return exploitation_term + exploration_term\n",
        "\n",
        "def _conditional_selection(self, nodes, parent_node):\n",
        "    \"\"\"æ¡ä»¶é€‰æ‹©æœºåˆ¶ï¼Œé¿å…é™·å…¥ä½ä»·å€¼åˆ†æ”¯\"\"\"\n",
        "    if not nodes:\n",
        "        return None\n",
        "        \n",
        "    # è®¾ç½®é˜ˆå€¼ï¼šçˆ¶èŠ‚ç‚¹é¢„æµ‹å€¼çš„ä¸€å®šæ¯”ä¾‹\n",
        "    value_threshold = parent_node.value_pred * 1.5  # å…è®¸æœ€å¤šæ¯”çˆ¶èŠ‚ç‚¹çš„é¢„æµ‹å€¼å¤§50%\n",
        "    \n",
        "    # ç­›é€‰æ‰é¢„æµ‹å€¼è¿œå·®äºçˆ¶èŠ‚ç‚¹çš„èŠ‚ç‚¹\n",
        "    filtered_nodes = [node for node in nodes if node.value_pred <= value_threshold]\n",
        "    \n",
        "    # å¦‚æœè¿‡æ»¤åæ²¡æœ‰èŠ‚ç‚¹ï¼Œåˆ™ä½¿ç”¨åŸå§‹èŠ‚ç‚¹åˆ—è¡¨\n",
        "    if not filtered_nodes:\n",
        "        filtered_nodes = nodes\n",
        "        \n",
        "    # åœ¨è¿‡æ»¤åçš„èŠ‚ç‚¹ä¸­é€‰æ‹©DUCBæœ€é«˜çš„\n",
        "    best_node = max(filtered_nodes, key=lambda n: n.visit_count)\n",
        "    return best_node\n",
        "\n",
        "def _select_candidates_mixed_strategy(self, visited_nodes):\n",
        "    \"\"\"ä½¿ç”¨æ··åˆç­–ç•¥ä»å•ä¸ªèµ·å§‹ç‚¹çš„NTEæœç´¢ç»“æœä¸­é€‰æ‹©å€™é€‰ç‚¹\"\"\"\n",
        "    if not visited_nodes:\n",
        "        return []\n",
        "\n",
        "    nodes_list = list(visited_nodes.values())\n",
        "    \n",
        "    # æŒ‰é¢„æµ‹å€¼æ’åºï¼Œé€‰æ‹©æœ€ä¼˜çš„\n",
        "    nodes_by_pred = sorted(nodes_list, key=lambda n: n.value_pred)\n",
        "    best_pred_candidates = [np.array(node.state) for node in nodes_by_pred[:15]]\n",
        "    \n",
        "    # æŒ‰è®¿é—®æ¬¡æ•°æ’åºï¼Œé€‰æ‹©æœ€å¸¸è®¿é—®çš„\n",
        "    nodes_by_visits = sorted(nodes_list, key=lambda n: n.visit_count, reverse=True)\n",
        "    most_visited_candidates = [np.array(node.state) for node in nodes_by_visits[:3]]\n",
        "    \n",
        "    # éšæœºé€‰æ‹©2ä¸ª\n",
        "    remaining_nodes = [node for node in nodes_list \n",
        "                      if node not in nodes_by_pred[:15] and node not in nodes_by_visits[:3]]\n",
        "    random_candidates = []\n",
        "    if len(remaining_nodes) >= 2:\n",
        "        random_selection = random.sample(remaining_nodes, 2)\n",
        "        random_candidates = [np.array(node.state) for node in random_selection]\n",
        "    elif remaining_nodes:\n",
        "        random_candidates = [np.array(node.state) for node in remaining_nodes]\n",
        "    \n",
        "    # åˆå¹¶æ‰€æœ‰å€™é€‰ç‚¹\n",
        "    all_candidates = best_pred_candidates + most_visited_candidates + random_candidates\n",
        "    \n",
        "    # å»é‡ï¼ˆåŸºäºçŠ¶æ€ç›¸ä¼¼æ€§ï¼‰\n",
        "    unique_candidates = []\n",
        "    tolerance = 1e-8\n",
        "    \n",
        "    for candidate in all_candidates:\n",
        "        is_duplicate = False\n",
        "        for existing in unique_candidates:\n",
        "            if np.allclose(candidate, existing, atol=tolerance, rtol=tolerance):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        if not is_duplicate:\n",
        "            unique_candidates.append(candidate)\n",
        "    \n",
        "    return unique_candidates[:self.validation_batch_size]\n",
        "\n",
        "def _deduplicate_candidates_with_predictions(self, candidates_with_pred, tolerance=1e-8):\n",
        "    \"\"\"å¯¹å¸¦é¢„æµ‹å€¼çš„å€™é€‰ç‚¹åˆ—è¡¨è¿›è¡Œå»é‡ï¼Œä¿ç•™é¢„æµ‹å€¼æ›´å¥½çš„ç‰ˆæœ¬\"\"\"\n",
        "    if not candidates_with_pred:\n",
        "        return []\n",
        "    \n",
        "    unique_candidates = []\n",
        "    \n",
        "    for candidate, pred_value in candidates_with_pred:\n",
        "        is_duplicate = False\n",
        "        candidate_array = np.array(candidate)\n",
        "        \n",
        "        for i, (existing_candidate, existing_pred) in enumerate(unique_candidates):\n",
        "            existing_array = np.array(existing_candidate)\n",
        "            \n",
        "            # æ£€æŸ¥ä¸¤ä¸ªç‚¹æ˜¯å¦åœ¨å®¹å·®èŒƒå›´å†…ç›¸ç­‰\n",
        "            if np.allclose(candidate_array, existing_array, atol=tolerance, rtol=tolerance):\n",
        "                # å‘ç°é‡å¤ï¼Œä¿ç•™é¢„æµ‹å€¼æ›´å¥½çš„é‚£ä¸ª\n",
        "                if pred_value < existing_pred:  # å¯¹äºæœ€å°åŒ–é—®é¢˜ï¼Œæ›´å°çš„å€¼æ›´å¥½\n",
        "                    unique_candidates[i] = (candidate, pred_value)\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        \n",
        "        if not is_duplicate:\n",
        "            unique_candidates.append((candidate, pred_value))\n",
        "    \n",
        "    return unique_candidates\n",
        "\n",
        "# å°†æ‰€æœ‰æ–¹æ³•æ·»åŠ åˆ°NTESearcherç±»ä¸­\n",
        "NTESearcher._calculate_ducb = _calculate_ducb\n",
        "NTESearcher._calculate_ducb_with_global_awareness = _calculate_ducb_with_global_awareness\n",
        "NTESearcher._conditional_selection = _conditional_selection\n",
        "NTESearcher._select_candidates_mixed_strategy = _select_candidates_mixed_strategy\n",
        "NTESearcher._deduplicate_candidates_with_predictions = _deduplicate_candidates_with_predictions\n",
        "\n",
        "print(\"âœ“ NTE Searcher æ ¸å¿ƒç®—æ³•æ–¹æ³•å·²æ·»åŠ \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: NTE Searcher ä¸»æœç´¢æ–¹æ³• - å®Œæ•´å®ç°\n",
        "\n",
        "def search(self, surrogate_model, current_best_state, min_y_observed, \n",
        "           dynamic_exploration_factor=1.0, all_states=None, all_values=None,\n",
        "           previous_iteration_samples=None, current_iteration=1, \n",
        "           return_ducb_trends=False):\n",
        "    \"\"\"\n",
        "    NTEæœç´¢ä¸»æ–¹æ³• - å®Œæ•´å®ç°ï¼ŒåŒ…å«æ‰€æœ‰åŸå§‹åŠŸèƒ½\n",
        "    \n",
        "    è¿™æ˜¯è°ƒè¯•åŠ¨æ€æ¢ç´¢å› å­çš„æ ¸å¿ƒæ–¹æ³•ï¼š\n",
        "    dynamic_exploration_factor = 0.8 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(f\"å¼€å§‹NTEæœç´¢ï¼ŒåŠ¨æ€æ¢ç´¢å› å­: {dynamic_exploration_factor:.2f}\")\n",
        "    \n",
        "    # åˆå§‹åŒ–æ•°æ®ç»“æ„\n",
        "    visited_nodes = {}  # çŠ¶æ€tuple -> Nodeå¯¹è±¡çš„æ˜ å°„\n",
        "    all_candidates_with_predictions = []  # æ‰€æœ‰å€™é€‰ç‚¹åŠå…¶é¢„æµ‹å€¼\n",
        "    ducb_trends_data = []  # DUCBè¶‹åŠ¿æ•°æ®æ”¶é›†\n",
        "    global_access_record = {}  # å…¨å±€è®¿é—®è®°å½•\n",
        "    \n",
        "    # è®¡ç®—è‡ªé€‚åº”æ¢ç´¢å› å­c(rho)\n",
        "    c_rho = self.rho_scaling_factor_heuristic(min_y_observed)\n",
        "    logger.info(f\"è‡ªé€‚åº”æ¢ç´¢å› å­ c(rho): {c_rho:.4f}\")\n",
        "    \n",
        "    # é¢„è®¡ç®—åŠ¨æ€æœç´¢è¾¹ç•Œä»¥æé«˜æ•ˆç‡\n",
        "    pre_computed_bounds = self._calculate_dynamic_search_space(all_states, all_values, current_iteration)\n",
        "    dynamic_mins, dynamic_maxs = pre_computed_bounds\n",
        "    logger.info(f\"åŠ¨æ€æœç´¢åŒºé—´: [{dynamic_mins[0]:.3f}, {dynamic_maxs[0]:.3f}] (ç¤ºä¾‹ç¬¬1ç»´)\")\n",
        "    \n",
        "    # ç¡®å®šNTEæœç´¢çš„èµ·å§‹ç‚¹åˆ—è¡¨\n",
        "    best_states_for_nte = []\n",
        "    \n",
        "    if current_best_state is not None:\n",
        "        # æ€»æ˜¯åŒ…å«å½“å‰å…¨å±€æœ€ä¼˜ç‚¹\n",
        "        best_states_for_nte.append(current_best_state.copy())\n",
        "        \n",
        "    # å¦‚æœæœ‰å†å²æ•°æ®ï¼Œé€‰æ‹©é¢å¤–çš„èµ·å§‹ç‚¹\n",
        "    if all_states is not None and all_values is not None and len(all_states) > 0:\n",
        "        # ä»æœ€ä¼˜çš„10%æ ·æœ¬ä¸­é€‰æ‹©é¢å¤–èµ·å§‹ç‚¹\n",
        "        sorted_indices = np.argsort(all_values.flatten())\n",
        "        top_candidates_count = max(1, min(3, len(sorted_indices) // 10))\n",
        "        \n",
        "        for i in range(min(top_candidates_count, len(sorted_indices))):\n",
        "            candidate_state = all_states[sorted_indices[i]].copy()\n",
        "            \n",
        "            # é¿å…é‡å¤ï¼ˆå¦‚æœå€™é€‰çŠ¶æ€ä¸å·²æœ‰çŠ¶æ€å¤ªæ¥è¿‘ï¼‰\n",
        "            is_duplicate = False\n",
        "            for existing_state in best_states_for_nte:\n",
        "                if np.allclose(candidate_state, existing_state, atol=1e-6):\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "            \n",
        "            if not is_duplicate:\n",
        "                best_states_for_nte.append(candidate_state)\n",
        "    \n",
        "    # å¦‚æœä»ç„¶æ²¡æœ‰è¶³å¤Ÿçš„èµ·å§‹ç‚¹ï¼Œæ·»åŠ éšæœºèµ·å§‹ç‚¹\n",
        "    while len(best_states_for_nte) < 2:\n",
        "        random_start = np.array([\n",
        "            random.uniform(dynamic_mins[i], dynamic_maxs[i]) \n",
        "            for i in range(self.dimension)\n",
        "        ])\n",
        "        best_states_for_nte.append(random_start)\n",
        "    \n",
        "    logger.info(f\"é€‰æ‹©äº† {len(best_states_for_nte)} ä¸ªNTEèµ·å§‹ç‚¹è¿›è¡Œæœç´¢\")\n",
        "    \n",
        "    # === ä¸»è¦NTEæœç´¢å¾ªç¯ï¼šå¯¹æ¯ä¸ªèµ·å§‹ç‚¹æ‰§è¡Œæœç´¢ ===\n",
        "    for nte_id, start_state in enumerate(best_states_for_nte):\n",
        "        # åˆå§‹åŒ–å½“å‰NTEçš„DUCBè¶‹åŠ¿æ”¶é›†å™¨\n",
        "        current_nte_ducb_trend = {\n",
        "            'nte_id': nte_id,\n",
        "            'ducb_values': [],\n",
        "            'rollout_indices': [],\n",
        "            'selected_states': [],\n",
        "            'model_pred_values': [],\n",
        "            'node_changes': []\n",
        "        }\n",
        "        \n",
        "        # åˆå§‹åŒ–æ ¹èŠ‚ç‚¹\n",
        "        root_state_tuple = tuple(start_state)\n",
        "        try:\n",
        "            root_pred_value = float(surrogate_model.predict(np.array([start_state]))[0])\n",
        "        except Exception as e:\n",
        "            logger.error(f\"é¢„æµ‹æ ¹èŠ‚ç‚¹å¤±è´¥: {e}\")\n",
        "            root_pred_value = float('inf')\n",
        "        \n",
        "        if root_state_tuple not in visited_nodes:\n",
        "            visited_nodes[root_state_tuple] = Node(\n",
        "                state=root_state_tuple, \n",
        "                value_pred=root_pred_value, \n",
        "                visit_count=0\n",
        "            )\n",
        "        \n",
        "        current_root_state_tuple = root_state_tuple\n",
        "        logger.debug(f\"å¼€å§‹ä»èµ·å§‹ç‚¹ {nte_id+1}/{len(best_states_for_nte)} è¿›è¡ŒNTEæœç´¢ï¼Œé¢„æµ‹å€¼: {root_pred_value:.4e}\")\n",
        "        \n",
        "        # è°ƒæ•´å½“å‰NTEçš„c0å€¼\n",
        "        current_c0 = self.base_c0 * dynamic_exploration_factor\n",
        "        nte_total_visits = 0  # å½“å‰NTEçš„æ€»è®¿é—®æ¬¡æ•°\n",
        "        \n",
        "        # === Rollout Phase ===\n",
        "        for rollout_round in range(self.rollout_rounds):\n",
        "            # è·å–å½“å‰æ ¹èŠ‚ç‚¹å¯¹è±¡\n",
        "            current_root_node_obj = visited_nodes[current_root_state_tuple]\n",
        "            current_root_state_vector = np.array(current_root_node_obj.state)\n",
        "            \n",
        "            # é€’å¢å½“å‰æ ¹èŠ‚ç‚¹çš„è®¿é—®è®¡æ•°\n",
        "            current_root_node_obj.visit_count += 1\n",
        "            nte_total_visits += 1\n",
        "            \n",
        "            # æ›´æ–°å…¨å±€è®¿é—®è®°å½•\n",
        "            if current_root_state_tuple not in global_access_record:\n",
        "                global_access_record[current_root_state_tuple] = {\n",
        "                    'total_visits': 0,\n",
        "                    'nte_visits': {}\n",
        "                }\n",
        "            global_access_record[current_root_state_tuple]['total_visits'] += 1\n",
        "            if nte_id not in global_access_record[current_root_state_tuple]['nte_visits']:\n",
        "                global_access_record[current_root_state_tuple]['nte_visits'][nte_id] = 0\n",
        "            global_access_record[current_root_state_tuple]['nte_visits'][nte_id] += 1\n",
        "            \n",
        "            # === ç”Ÿæˆå¶èŠ‚ç‚¹(è†¨èƒ€é˜¶æ®µ) ===\n",
        "            leaf_states_vectors = self._mixed_expansion(\n",
        "                current_root_state_vector, \n",
        "                dynamic_exploration_factor,\n",
        "                all_states, \n",
        "                all_values, \n",
        "                current_iteration,\n",
        "                pre_computed_bounds=pre_computed_bounds\n",
        "            )\n",
        "            \n",
        "            if not leaf_states_vectors:\n",
        "                # å¦‚æœæ··åˆæ‰©å±•å¤±è´¥ï¼Œç”ŸæˆéšæœºçŠ¶æ€\n",
        "                rand_state = np.array([random.uniform(dynamic_mins[i], dynamic_maxs[i]) for i in range(self.dimension)])\n",
        "                leaf_states_vectors = [rand_state]\n",
        "            \n",
        "            # ç”¨æ›¿ä»£æ¨¡å‹é¢„æµ‹å¶èŠ‚ç‚¹å€¼\n",
        "            try:\n",
        "                leaf_states_batch = np.array(leaf_states_vectors)\n",
        "                leaf_pred_values = surrogate_model.predict(leaf_states_batch)\n",
        "                if not isinstance(leaf_pred_values, np.ndarray):\n",
        "                    leaf_pred_values = np.array(leaf_pred_values)\n",
        "                \n",
        "                leaf_pred_values = leaf_pred_values.flatten()\n",
        "\n",
        "                # å¤„ç† NaN/Inf å€¼\n",
        "                if np.any(np.isnan(leaf_pred_values)) or np.any(np.isinf(leaf_pred_values)):\n",
        "                    logger.warning(f\"å¶èŠ‚ç‚¹é¢„æµ‹ä¸­å­˜åœ¨NaN/Infå€¼ï¼Œå°†æ›¿æ¢å®ƒä»¬ã€‚\")\n",
        "                    leaf_pred_values = np.nan_to_num(leaf_pred_values, nan=float('inf'), posinf=float('inf'), neginf=float('inf'))\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"é¢„æµ‹å¶èŠ‚ç‚¹é”™è¯¯: {e}ã€‚ä½¿ç”¨æ— ç©·å¤§ã€‚\")\n",
        "                leaf_pred_values = np.array([float('inf')] * len(leaf_states_vectors))\n",
        "            \n",
        "            # åˆ›å»ºæˆ–æ›´æ–°å¶èŠ‚ç‚¹\n",
        "            leaf_nodes = []\n",
        "            \n",
        "            for state_vec, pred_val in zip(leaf_states_vectors, leaf_pred_values):\n",
        "                state_tuple = tuple(state_vec)\n",
        "                node_value = float(pred_val)\n",
        "                \n",
        "                if state_tuple not in visited_nodes:\n",
        "                    visited_nodes[state_tuple] = Node(state=state_tuple, value_pred=node_value, visit_count=0)\n",
        "                    \n",
        "                    # åˆå§‹åŒ–å…¨å±€è®¿é—®è®°å½•\n",
        "                    if state_tuple not in global_access_record:\n",
        "                        global_access_record[state_tuple] = {\n",
        "                            'total_visits': 0,\n",
        "                            'nte_visits': {}\n",
        "                        }\n",
        "                else:\n",
        "                    # æ›´æ–°é¢„æµ‹å€¼ï¼Œä¿æŒè®¿é—®è®¡æ•°\n",
        "                    visited_nodes[state_tuple].value_pred = node_value\n",
        "                \n",
        "                leaf_nodes.append(visited_nodes[state_tuple])\n",
        "            \n",
        "            # è®¡ç®—æ ¹èŠ‚ç‚¹å’Œå¶èŠ‚ç‚¹çš„DUCB\n",
        "            root_ducb = self._calculate_ducb_with_global_awareness(\n",
        "                current_root_node_obj, \n",
        "                current_root_node_obj.visit_count,\n",
        "                c_rho, \n",
        "                current_c0,\n",
        "                global_access_record,\n",
        "                nte_id,\n",
        "                current_root_state_tuple\n",
        "            )\n",
        "            \n",
        "            # ä½¿ç”¨æ¡ä»¶é€‰æ‹©\n",
        "            best_leaf_node = self._conditional_selection(leaf_nodes, current_root_node_obj)\n",
        "            if best_leaf_node is None:\n",
        "                # å¦‚æœæ¡ä»¶é€‰æ‹©å¤±è´¥ï¼Œå›é€€åˆ°æ ¹èŠ‚ç‚¹\n",
        "                next_root_state_tuple = current_root_state_tuple\n",
        "                self.last_move_successful = False\n",
        "                \n",
        "                # æ”¶é›†DUCBè¶‹åŠ¿æ•°æ®\n",
        "                if return_ducb_trends:\n",
        "                    current_nte_ducb_trend['ducb_values'].append(root_ducb)\n",
        "                    current_nte_ducb_trend['rollout_indices'].append(rollout_round)\n",
        "                    current_nte_ducb_trend['selected_states'].append(np.array(current_root_state_tuple))\n",
        "                    current_nte_ducb_trend['model_pred_values'].append(-current_root_node_obj.value_pred)\n",
        "            else:\n",
        "                # å¯¹é€‰æ‹©çš„æœ€ä½³å¶èŠ‚ç‚¹è®¡ç®—DUCB\n",
        "                best_leaf_ducb = self._calculate_ducb_with_global_awareness(\n",
        "                    best_leaf_node, \n",
        "                    nte_total_visits,\n",
        "                    c_rho, \n",
        "                    current_c0,\n",
        "                    global_access_record,\n",
        "                    nte_id,\n",
        "                    best_leaf_node.state\n",
        "                )\n",
        "                \n",
        "                # å±€éƒ¨åå‘ä¼ æ’­ï¼šæ ¹æ®DUCBæ¯”è¾ƒå†³å®šä¸‹ä¸€æ­¥\n",
        "                if root_ducb >= best_leaf_ducb:\n",
        "                    next_root_state_tuple = current_root_state_tuple\n",
        "                    self.last_move_successful = False\n",
        "                    \n",
        "                    # æ”¶é›†DUCBè¶‹åŠ¿æ•°æ®ï¼ˆæ ¹èŠ‚ç‚¹ï¼‰\n",
        "                    if return_ducb_trends:\n",
        "                        current_nte_ducb_trend['ducb_values'].append(root_ducb)\n",
        "                        current_nte_ducb_trend['rollout_indices'].append(rollout_round)\n",
        "                        current_nte_ducb_trend['selected_states'].append(np.array(current_root_state_tuple))\n",
        "                        current_nte_ducb_trend['model_pred_values'].append(-current_root_node_obj.value_pred)\n",
        "                else:\n",
        "                    next_root_state_tuple = best_leaf_node.state\n",
        "                    # åˆ¤æ–­æ˜¯å¦æ‰¾åˆ°äº†æ›´å¥½çš„ç‚¹\n",
        "                    self.last_move_successful = best_leaf_node.value_pred < current_root_node_obj.value_pred\n",
        "                    if self.last_move_successful and self.last_move_deterministic:\n",
        "                        logger.debug(f\"ç¡®å®šæ€§ç§»åŠ¨æˆåŠŸï¼Œæ‰¾åˆ°äº†æ›´ä¼˜å€¼: {current_root_node_obj.value_pred:.4e} -> {best_leaf_node.value_pred:.4e}\")\n",
        "                    \n",
        "                    # æ”¶é›†DUCBè¶‹åŠ¿æ•°æ®ï¼ˆå¶èŠ‚ç‚¹ï¼‰\n",
        "                    if return_ducb_trends:\n",
        "                        current_nte_ducb_trend['ducb_values'].append(best_leaf_ducb)\n",
        "                        current_nte_ducb_trend['rollout_indices'].append(rollout_round)\n",
        "                        current_nte_ducb_trend['selected_states'].append(np.array(best_leaf_node.state))\n",
        "                        current_nte_ducb_trend['model_pred_values'].append(-best_leaf_node.value_pred)\n",
        "                        # è®°å½•èŠ‚ç‚¹å˜æ›´\n",
        "                        if current_root_state_tuple != best_leaf_node.state:\n",
        "                            current_nte_ducb_trend['node_changes'].append(rollout_round)\n",
        "                    \n",
        "                    # æ ¹æ®NTEï¼Œå¢åŠ é€‰ä¸­å¶èŠ‚ç‚¹çš„è®¿é—®æ¬¡æ•°\n",
        "                    best_leaf_node.visit_count += 1\n",
        "                    nte_total_visits += 1\n",
        "                    \n",
        "                    # æ›´æ–°å…¨å±€è®¿é—®è®°å½•\n",
        "                    if next_root_state_tuple not in global_access_record:\n",
        "                        global_access_record[next_root_state_tuple] = {\n",
        "                            'total_visits': 0,\n",
        "                            'nte_visits': {}\n",
        "                        }\n",
        "                    global_access_record[next_root_state_tuple]['total_visits'] += 1\n",
        "                    if nte_id not in global_access_record[next_root_state_tuple]['nte_visits']:\n",
        "                        global_access_record[next_root_state_tuple]['nte_visits'][nte_id] = 0\n",
        "                    global_access_record[next_root_state_tuple]['nte_visits'][nte_id] += 1\n",
        "            \n",
        "            # æ›´æ–°ä¸‹ä¸€è½®è¿­ä»£çš„æ ¹èŠ‚ç‚¹\n",
        "            current_root_state_tuple = next_root_state_tuple\n",
        "        \n",
        "        # ä»è¿™ä¸ªèµ·å§‹ç‚¹æœç´¢å¾—åˆ°çš„å€™é€‰ç‚¹æ·»åŠ åˆ°æ€»åˆ—è¡¨ä¸­\n",
        "        candidates_from_this_root = self._select_candidates_mixed_strategy(visited_nodes)\n",
        "        for candidate in candidates_from_this_root:\n",
        "            candidate_tuple = tuple(candidate)\n",
        "            # è·å–é¢„æµ‹å€¼\n",
        "            if candidate_tuple in visited_nodes:\n",
        "                pred_value = visited_nodes[candidate_tuple].value_pred\n",
        "            else:\n",
        "                try:\n",
        "                    pred_value = float(surrogate_model.predict(np.array([candidate]))[0])\n",
        "                except:\n",
        "                    pred_value = float('inf')\n",
        "            \n",
        "            all_candidates_with_predictions.append((candidate, pred_value))\n",
        "        \n",
        "        # å°†å½“å‰èµ·å§‹ç‚¹çš„DUCBè¶‹åŠ¿æ•°æ®æ·»åŠ åˆ°æ€»çš„è¶‹åŠ¿æ•°æ®ä¸­\n",
        "        if return_ducb_trends:\n",
        "            ducb_trends_data.append(current_nte_ducb_trend)\n",
        "    \n",
        "    # === æœ€ç»ˆå¤„ç†é˜¶æ®µï¼šæ™ºèƒ½å»é‡å’Œæœ€ä¼˜é€‰æ‹© ===\n",
        "    logger.info(f\"NTEæœç´¢å®Œæˆï¼Œåˆå¹¶å‰å…±æœ‰ {len(all_candidates_with_predictions)} ä¸ªå€™é€‰ç‚¹\")\n",
        "    \n",
        "    # å»é‡å€™é€‰ç‚¹ï¼ˆä¿ç•™é¢„æµ‹å€¼æ›´å¥½çš„ç‰ˆæœ¬ï¼‰\n",
        "    unique_candidates_with_pred = self._deduplicate_candidates_with_predictions(\n",
        "        all_candidates_with_predictions, \n",
        "        tolerance=1e-8\n",
        "    )\n",
        "    logger.info(f\"å»é‡åå…±æœ‰ {len(unique_candidates_with_pred)} ä¸ªå”¯ä¸€å€™é€‰ç‚¹ï¼Œ\" +\n",
        "               f\"å»é‡æ¯”ä¾‹: {(len(all_candidates_with_predictions) - len(unique_candidates_with_pred))/len(all_candidates_with_predictions)*100:.1f}%\")\n",
        "    \n",
        "    # å¦‚æœå€™é€‰ç‚¹ä¸è¶³ï¼Œæ·»åŠ éšæœºç‚¹\n",
        "    if len(unique_candidates_with_pred) < self.validation_batch_size:\n",
        "        num_random_to_add = self.validation_batch_size - len(unique_candidates_with_pred)\n",
        "        logger.info(f\"å€™é€‰ç‚¹ä¸è¶³ï¼Œæ·»åŠ {num_random_to_add}ä¸ªéšæœºç‚¹\")\n",
        "        \n",
        "        # åˆ›å»ºç°æœ‰å€™é€‰ç‚¹çš„é›†åˆç”¨äºé¿å…é‡å¤\n",
        "        existing_candidates_set = set()\n",
        "        for candidate, _ in unique_candidates_with_pred:\n",
        "            rounded_candidate = tuple(np.round(candidate, 6))\n",
        "            existing_candidates_set.add(rounded_candidate)\n",
        "        \n",
        "        added_random_count = 0\n",
        "        max_attempts = num_random_to_add * 10\n",
        "        attempts = 0\n",
        "        \n",
        "        while added_random_count < num_random_to_add and attempts < max_attempts:\n",
        "            random_point = np.array([random.uniform(self.domain_mins[i], self.domain_maxs[i]) \n",
        "                                     for i in range(self.dimension)])\n",
        "            rounded_random = tuple(np.round(random_point, 6))\n",
        "            \n",
        "            if rounded_random not in existing_candidates_set:\n",
        "                try:\n",
        "                    random_pred = float(surrogate_model.predict(np.array([random_point]))[0])\n",
        "                except:\n",
        "                    random_pred = float('inf')\n",
        "                \n",
        "                unique_candidates_with_pred.append((random_point, random_pred))\n",
        "                existing_candidates_set.add(rounded_random)\n",
        "                added_random_count += 1\n",
        "            \n",
        "            attempts += 1\n",
        "        \n",
        "        if added_random_count < num_random_to_add:\n",
        "            logger.warning(f\"åªèƒ½æ·»åŠ  {added_random_count}/{num_random_to_add} ä¸ªä¸é‡å¤çš„éšæœºç‚¹\")\n",
        "    \n",
        "    # æŒ‰é¢„æµ‹å€¼æ’åºï¼ˆå‡åºï¼Œå› ä¸ºRosenbrockæ˜¯æœ€å°åŒ–é—®é¢˜ï¼‰\n",
        "    unique_candidates_with_pred.sort(key=lambda x: x[1])\n",
        "    \n",
        "    # é€‰æ‹©æœ€ç»ˆçš„20ä¸ªæ ·æœ¬ï¼šå‰15ä¸ªæœ€ä¼˜ + å5ä¸ªéšæœºæ¢ç´¢\n",
        "    final_candidates = []\n",
        "    \n",
        "    # å‰15ä¸ªé¢„æµ‹å€¼æœ€ä¼˜çš„æ ·æœ¬\n",
        "    best_15_count = min(15, len(unique_candidates_with_pred))\n",
        "    for i in range(best_15_count):\n",
        "        final_candidates.append(unique_candidates_with_pred[i][0])\n",
        "    \n",
        "    # å‰©ä½™5ä¸ªä»å…¶ä»–å€™é€‰ç‚¹ä¸­éšæœºé€‰æ‹©\n",
        "    remaining_candidates = unique_candidates_with_pred[best_15_count:]\n",
        "    explore_count = min(self.validation_batch_size - best_15_count, len(remaining_candidates))\n",
        "    \n",
        "    if explore_count > 0:\n",
        "        if len(remaining_candidates) > explore_count:\n",
        "            # éšæœºé€‰æ‹©å‰©ä½™çš„æ¢ç´¢æ ·æœ¬\n",
        "            import random as rand\n",
        "            selected_indices = rand.sample(range(len(remaining_candidates)), explore_count)\n",
        "            for idx in selected_indices:\n",
        "                final_candidates.append(remaining_candidates[idx][0])\n",
        "        else:\n",
        "            # å…¨éƒ¨æ·»åŠ \n",
        "            for candidate, _ in remaining_candidates:\n",
        "                final_candidates.append(candidate)\n",
        "    \n",
        "    logger.info(f\"æœ€ç»ˆé€‰æ‹© {len(final_candidates)} ä¸ªæ ·æœ¬ï¼š{best_15_count} ä¸ªæœ€ä¼˜æ ·æœ¬ + {len(final_candidates) - best_15_count} ä¸ªæ¢ç´¢æ ·æœ¬\")\n",
        "    \n",
        "    # ç¡®ä¿è¿”å›åˆ—è¡¨ä¸è¶…è¿‡æ‰€è¯·æ±‚çš„å€™é€‰ç‚¹æ•°é‡\n",
        "    final_result = final_candidates[:self.validation_batch_size]\n",
        "    \n",
        "    if return_ducb_trends:\n",
        "        return final_result, ducb_trends_data\n",
        "    else:\n",
        "        return final_result\n",
        "\n",
        "# å°†searchæ–¹æ³•æ·»åŠ åˆ°NTESearcherç±»ä¸­\n",
        "NTESearcher.search = search\n",
        "\n",
        "print(\"âœ“ NTE Searcher å®Œæ•´å®ç°å·²å®Œæˆ - è¿™æ˜¯è°ƒè¯•åŠ¨æ€æ¢ç´¢å› å­çš„æ ¸å¿ƒæ–¹æ³•ï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: å®Œæ•´å¯è§†åŒ–å·¥å…· (plot_utils.py) - æ¢å¤æ‰€æœ‰åŸå§‹åŠŸèƒ½\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import matplotlib\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# é…ç½®å…¨å±€å­—ä½“è®¾ç½®ä¸ºé»˜è®¤è‹±æ–‡å­—ä½“\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# ä½¿ç”¨Aggåç«¯ï¼Œä»¥ä¾¿åœ¨æ²¡æœ‰GUIçš„ç¯å¢ƒä¸­ä¹Ÿèƒ½ä¿å­˜å›¾åƒ\n",
        "plt.switch_backend('Agg') \n",
        "\n",
        "def create_results_directory(base_results_path):\n",
        "    \"\"\"åœ¨æŒ‡å®šçš„åŸºç¡€è·¯å¾„ä¸‹åˆ›å»ºä¸€ä¸ªä»¥å½“å‰æ—¶é—´æˆ³å‘½åçš„å­ç›®å½•\"\"\"\n",
        "    try:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_dir = os.path.join(base_results_path, timestamp)\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        logger.info(f\"Results directory created: {results_dir}\")\n",
        "        return results_dir\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating results directory: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_global_min_value_trend(iterations, min_values, output_dir, filename=\"global_min_value_trend.png\"):\n",
        "    \"\"\"ç»˜åˆ¶å¹¶ä¿å­˜å…¨å±€æœ€å°çœŸå€¼çš„å˜åŒ–è¶‹åŠ¿å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # ç¡®ä¿iterationså’Œmin_valuesé•¿åº¦ç›¸åŒ\n",
        "        if len(iterations) != len(min_values):\n",
        "            logger.warning(f\"Iterationså’Œmin_valuesæ•°ç»„é•¿åº¦ä¸åŒ: {len(iterations)} vs {len(min_values)}.\")\n",
        "            min_length = min(len(iterations), len(min_values))\n",
        "            iterations = iterations[:min_length]\n",
        "            min_values = min_values[:min_length]\n",
        "            logger.warning(f\"æ•°ç»„æˆªæ–­è‡³ç›¸åŒé•¿åº¦: {min_length}\")\n",
        "            \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, min_values, marker='o', linestyle='-', color='b')\n",
        "        plt.title('Global Minimum Value Trend')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Current Minimum True Value Found')\n",
        "        plt.grid(True)\n",
        "        plt.xticks(iterations)\n",
        "        \n",
        "        # åªåœ¨æ‰¾åˆ°æ›´ä½å…¨å±€æœ€å°å€¼æ—¶æ·»åŠ æ ‡æ³¨\n",
        "        min_values_array = np.array(min_values)\n",
        "        global_min = float('inf')\n",
        "        \n",
        "        for i, (iter_num, current_min) in enumerate(zip(iterations, min_values_array)):\n",
        "            if current_min < global_min:\n",
        "                global_min = current_min\n",
        "                if current_min < 1000:\n",
        "                    label_txt = f\"{current_min:.2f}\"\n",
        "                else:\n",
        "                    label_txt = f\"{current_min:.2e}\"\n",
        "                plt.annotate(label_txt, \n",
        "                            (iter_num, current_min),\n",
        "                            textcoords=\"offset points\", \n",
        "                            xytext=(0,10), \n",
        "                            ha='center',\n",
        "                            fontsize=9,\n",
        "                            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "def plot_loss_trend(data_points, output_dir, x_label=\"Epoch/Iteration\", title=\"Model Loss Trend\", \n",
        "                   filename=\"loss_trend.png\", custom_ticks=None, pearson_coeffs=None):\n",
        "    \"\"\"ç»˜åˆ¶å¹¶ä¿å­˜æŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
        "        if custom_ticks and len(custom_ticks) == len(data_points):\n",
        "            plt.plot(custom_ticks, data_points, marker='.', linestyle='-', color='r', label='Loss')\n",
        "            x_values = custom_ticks\n",
        "        else:\n",
        "            plt.plot(data_points, marker='.', linestyle='-', color='r', label='Loss')\n",
        "            x_values = range(len(data_points))\n",
        "        \n",
        "        plt.title(title if title else 'Model Loss Trend')\n",
        "        plt.xlabel(x_label if x_label else 'Training Epochs')\n",
        "        plt.ylabel('Loss Value')\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # å¦‚æœæä¾›äº†çš®å°”é€Šç›¸å…³ç³»æ•°ï¼Œæ·»åŠ ç¬¬äºŒä¸ªyè½´å¹¶ç»˜åˆ¶\n",
        "        if pearson_coeffs is not None and len(pearson_coeffs) == len(data_points):\n",
        "            ax2 = plt.gca().twinx()\n",
        "            ax2.plot(x_values, pearson_coeffs, marker='s', linestyle='--', color='g', label='Pearson Correlation')\n",
        "            ax2.set_ylabel('Pearson Correlation Coefficient', color='g')\n",
        "            ax2.tick_params(axis='y', labelcolor='g')\n",
        "            ax2.set_ylim(-0.1, 1.1)\n",
        "            \n",
        "            # åœ¨æ¯ä¸ªæ•°æ®ç‚¹ä¸Šæ ‡æ³¨çš®å°”é€Šç³»æ•°å€¼\n",
        "            for i, coef in enumerate(pearson_coeffs):\n",
        "                ax2.annotate(f\"{coef:.3f}\",\n",
        "                           (x_values[i], pearson_coeffs[i]),\n",
        "                           textcoords=\"offset points\", \n",
        "                           xytext=(0,10), \n",
        "                           ha='center',\n",
        "                           fontsize=8,\n",
        "                           color='g')\n",
        "            \n",
        "            # ä¿®æ”¹å›¾ä¾‹ä»¥åŒ…å«ä¸¤ä¸ªæ›²çº¿\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "        else:\n",
        "            plt.legend(loc='upper right')\n",
        "        \n",
        "        # ä½¿ç”¨è‡ªå®šä¹‰xè½´åˆ»åº¦ï¼ˆå¦‚æœæä¾›ï¼‰\n",
        "        if custom_ticks:\n",
        "            plt.xticks(custom_ticks)\n",
        "\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Lossè¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æŸå¤±è¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "\n",
        "def plot_pearson_correlation_trend(iterations, pearson_coeffs, output_dir, \n",
        "                                  filename=\"pearson_correlation_trend.png\", title=None):\n",
        "    \"\"\"ç»˜åˆ¶å¹¶ä¿å­˜çš®å°”é€Šç›¸å…³ç³»æ•°çš„å˜åŒ–è¶‹åŠ¿å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, pearson_coeffs, marker='o', linestyle='-', color='g')\n",
        "        plt.title(title if title else 'Pearson Correlation Coefficient Trend (Model vs. Truth)')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Pearson Correlation Coefficient')\n",
        "        plt.ylim(-1.1, 1.1)\n",
        "        plt.grid(True)\n",
        "        plt.xticks(iterations)\n",
        "        \n",
        "        # æ·»åŠ æ°´å¹³å‚è€ƒçº¿\n",
        "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate Correlation')\n",
        "        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Strong Correlation')\n",
        "        \n",
        "        plt.legend()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Pearsonç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶Pearsonç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "\n",
        "def plot_nte_performance(iterations, num_candidates, output_dir, filename=\"nte_candidates_trend.png\"):\n",
        "    \"\"\"ç»˜åˆ¶å¹¶ä¿å­˜NTEæœç´¢æ€§èƒ½å›¾ï¼Œæ˜¾ç¤ºæ¯æ¬¡è¿­ä»£æ‰¾åˆ°çš„å€™é€‰ç‚¹æ•°é‡\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, num_candidates, marker='s', linestyle='-', color='purple', linewidth=2)\n",
        "        plt.title('NTE Search Performance: Number of Candidates Found')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Number of Candidate Points')\n",
        "        plt.grid(True)\n",
        "        plt.xticks(iterations)\n",
        "        \n",
        "        # æ·»åŠ å¹³å‡çº¿\n",
        "        avg_candidates = np.mean(num_candidates)\n",
        "        plt.axhline(y=avg_candidates, color='red', linestyle='--', alpha=0.7, \n",
        "                   label=f'Average: {avg_candidates:.1f}')\n",
        "        \n",
        "        plt.legend()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"NTEæ€§èƒ½å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶NTEæ€§èƒ½å›¾å‡ºé”™: {e}\")\n",
        "\n",
        "print(\"âœ“ å®Œæ•´å¯è§†åŒ–å·¥å…· (ç¬¬1éƒ¨åˆ†) å·²åŠ è½½\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: å®Œæ•´å¯è§†åŒ–å·¥å…· (ç¬¬2éƒ¨åˆ†) - é«˜çº§å¯è§†åŒ–åŠŸèƒ½\n",
        "\n",
        "def plot_prediction_vs_truth(predictions, ground_truth, output_dir, iteration=None, \n",
        "                            filename=None, title_suffix=None):\n",
        "    \"\"\"ç»˜åˆ¶é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾ï¼Œå¹¶è®¡ç®—è¯„ä¼°æŒ‡æ ‡\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        predictions = np.array(predictions).flatten()\n",
        "        ground_truth = np.array(ground_truth).flatten()\n",
        "        \n",
        "        if len(predictions) != len(ground_truth):\n",
        "            logger.error(f\"é¢„æµ‹å€¼å’ŒçœŸå®å€¼é•¿åº¦ä¸åŒ¹é…: {len(predictions)} vs {len(ground_truth)}\")\n",
        "            return None, None, None\n",
        "        \n",
        "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
        "        mse = np.mean((predictions - ground_truth) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = np.mean(np.abs(predictions - ground_truth))\n",
        "        \n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        # ç»˜åˆ¶æ•£ç‚¹å›¾\n",
        "        plt.scatter(ground_truth, predictions, alpha=0.6, s=20)\n",
        "        \n",
        "        # ç»˜åˆ¶ç†æƒ³çº¿ (y=x)\n",
        "        min_val = min(np.min(ground_truth), np.min(predictions))\n",
        "        max_val = max(np.max(ground_truth), np.max(predictions))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "        \n",
        "        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾\n",
        "        title = f'Prediction vs Ground Truth'\n",
        "        if title_suffix:\n",
        "            title += f' ({title_suffix})'\n",
        "        if iteration is not None:\n",
        "            title += f' - Iteration {iteration}'\n",
        "        \n",
        "        plt.title(title)\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Prediction')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        \n",
        "        # æ·»åŠ è¯„ä¼°æŒ‡æ ‡æ–‡æœ¬\n",
        "        textstr = f'RMSE: {rmse:.4e}\\\\nMAE: {mae:.4e}\\\\nSamples: {len(predictions)}'\n",
        "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
        "        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=props)\n",
        "        \n",
        "        # ä¿å­˜å›¾å½¢\n",
        "        if filename is None:\n",
        "            if iteration is not None:\n",
        "                filename = f\"prediction_vs_truth_iter_{iteration}.png\"\n",
        "            else:\n",
        "                filename = \"prediction_vs_truth.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"é¢„æµ‹vsçœŸå®å€¼å›¾å·²ä¿å­˜: {save_path} (RMSE: {rmse:.4e})\")\n",
        "        return mse, rmse, mae\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶é¢„æµ‹vsçœŸå®å€¼å›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None, None, None\n",
        "\n",
        "def plot_residuals(predictions, ground_truth, output_dir, iteration=None, filename=None):\n",
        "    \"\"\"ç»˜åˆ¶æ®‹å·®åˆ†æå›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        predictions = np.array(predictions).flatten()\n",
        "        ground_truth = np.array(ground_truth).flatten()\n",
        "        \n",
        "        if len(predictions) != len(ground_truth):\n",
        "            logger.error(f\"é¢„æµ‹å€¼å’ŒçœŸå®å€¼é•¿åº¦ä¸åŒ¹é…: {len(predictions)} vs {len(ground_truth)}\")\n",
        "            return\n",
        "        \n",
        "        residuals = predictions - ground_truth\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # å­å›¾1: æ®‹å·®vsé¢„æµ‹å€¼\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.scatter(predictions, residuals, alpha=0.6, s=20)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predictions')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Residuals vs Predictions')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # å­å›¾2: æ®‹å·®vsçœŸå®å€¼\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.scatter(ground_truth, residuals, alpha=0.6, s=20)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Residuals vs Ground Truth')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # å­å›¾3: æ®‹å·®ç›´æ–¹å›¾\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Residuals Distribution')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # å­å›¾4: Q-Qå›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
        "        plt.subplot(2, 2, 4)\n",
        "        sorted_residuals = np.sort(residuals)\n",
        "        theoretical_quantiles = np.linspace(-3, 3, len(sorted_residuals))\n",
        "        plt.scatter(theoretical_quantiles, sorted_residuals, alpha=0.6, s=20)\n",
        "        plt.plot(theoretical_quantiles, theoretical_quantiles * np.std(residuals), 'r--')\n",
        "        plt.xlabel('Theoretical Quantiles')\n",
        "        plt.ylabel('Sample Quantiles')\n",
        "        plt.title('Q-Q Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # ä¿å­˜å›¾å½¢\n",
        "        if filename is None:\n",
        "            if iteration is not None:\n",
        "                filename = f\"residual_analysis_iter_{iteration}.png\"\n",
        "            else:\n",
        "                filename = \"residual_analysis.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"æ®‹å·®åˆ†æå›¾å·²ä¿å­˜: {save_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æ®‹å·®åˆ†æå›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "def plot_best_samples_pearson_trend(iterations_history, pearson_history, results_dir, \n",
        "                                   filename=\"best_samples_pearson_trend.png\", \n",
        "                                   title=\"Pearson Correlation on Best Samples\"):\n",
        "    \"\"\"ç»˜åˆ¶æœ€ä½³æ ·æœ¬çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾\"\"\"\n",
        "    if not os.path.exists(results_dir):\n",
        "        logger.warning(f\"Results directory {results_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations_history, pearson_history, marker='o', linestyle='-', \n",
        "                color='blue', linewidth=2, markersize=6)\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Pearson Correlation Coefficient')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.ylim(-1.1, 1.1)\n",
        "        \n",
        "        # æ·»åŠ æ°´å¹³å‚è€ƒçº¿\n",
        "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate (0.5)')\n",
        "        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Strong (0.8)')\n",
        "        \n",
        "        # å¦‚æœæœ‰è¶³å¤Ÿçš„æ•°æ®ç‚¹ï¼Œæ·»åŠ è¶‹åŠ¿çº¿\n",
        "        if len(iterations_history) > 2:\n",
        "            z = np.polyfit(iterations_history, pearson_history, 1)\n",
        "            p = np.poly1d(z)\n",
        "            plt.plot(iterations_history, p(iterations_history), \"g--\", alpha=0.8, label='Trend')\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.xticks(iterations_history)\n",
        "        \n",
        "        save_path = os.path.join(results_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"æœ€ä½³æ ·æœ¬Pearsonè¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æœ€ä½³æ ·æœ¬Pearsonè¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "def plot_surrogate_vs_selected_samples(surrogate_model, selected_X, selected_y_true, \n",
        "                                      data_manager, output_dir, iteration, filename=None):\n",
        "    \"\"\"ç»˜åˆ¶æ›¿ä»£æ¨¡å‹é¢„æµ‹vsæ–°é€‰æ‹©æ ·æœ¬çš„å¯¹æ¯”å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if len(selected_X) == 0:\n",
        "            logger.warning(\"æ²¡æœ‰é€‰æ‹©çš„æ ·æœ¬ï¼Œè·³è¿‡å¯¹æ¯”å›¾ç»˜åˆ¶\")\n",
        "            return None\n",
        "        \n",
        "        # ä½¿ç”¨æ›¿ä»£æ¨¡å‹é¢„æµ‹æ–°é€‰æ‹©çš„æ ·æœ¬\n",
        "        if hasattr(surrogate_model, 'predict'):\n",
        "            predictions_log = surrogate_model.predict(selected_X)\n",
        "            # è½¬æ¢ä¸ºåŸå§‹å°ºåº¦\n",
        "            predictions_orig = safe_power10(predictions_log)\n",
        "        else:\n",
        "            logger.error(\"æ›¿ä»£æ¨¡å‹æ²¡æœ‰predictæ–¹æ³•\")\n",
        "            return None\n",
        "        \n",
        "        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
        "        if len(predictions_orig) >= 2:\n",
        "            pearson_corr, _ = pearsonr(predictions_orig, selected_y_true.flatten())\n",
        "            if np.isnan(pearson_corr):\n",
        "                pearson_corr = 0.0\n",
        "        else:\n",
        "            pearson_corr = 0.0\n",
        "        \n",
        "        mae = np.mean(np.abs(predictions_orig - selected_y_true.flatten()))\n",
        "        rmse = np.sqrt(np.mean((predictions_orig - selected_y_true.flatten()) ** 2))\n",
        "        \n",
        "        # åˆ›å»ºå¯¹æ¯”å›¾\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        plt.scatter(selected_y_true.flatten(), predictions_orig, alpha=0.7, s=50, \n",
        "                   c='blue', label=f'Selected Samples (n={len(selected_X)})')\n",
        "        \n",
        "        # ç»˜åˆ¶ç†æƒ³çº¿\n",
        "        min_val = min(np.min(selected_y_true), np.min(predictions_orig))\n",
        "        max_val = max(np.max(selected_y_true), np.max(predictions_orig))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "        \n",
        "        plt.xlabel('True Values (Selected Samples)')\n",
        "        plt.ylabel('Surrogate Model Predictions')\n",
        "        plt.title(f'Surrogate Model vs Selected Samples (Iteration {iteration})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        \n",
        "        # æ·»åŠ è¯„ä¼°æŒ‡æ ‡\n",
        "        textstr = f'Pearson: {pearson_corr:.4f}\\\\nMAE: {mae:.4e}\\\\nRMSE: {rmse:.4e}'\n",
        "        props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
        "        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=props)\n",
        "        \n",
        "        # ä¿å­˜å›¾å½¢\n",
        "        if filename is None:\n",
        "            filename = f\"surrogate_vs_selected_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"æ›¿ä»£æ¨¡å‹vsé€‰æ‹©æ ·æœ¬å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}\")\n",
        "        \n",
        "        return {\n",
        "            'pearson': pearson_corr,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'n_samples': len(selected_X)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æ›¿ä»£æ¨¡å‹vsé€‰æ‹©æ ·æœ¬å¯¹æ¯”å›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "print(\"âœ“ å®Œæ•´å¯è§†åŒ–å·¥å…· (ç¬¬2éƒ¨åˆ†) å·²åŠ è½½\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: å®Œæ•´å¯è§†åŒ–å·¥å…· (ç¬¬3éƒ¨åˆ†) - ç‰¹æ®Šå¯è§†åŒ–åŠŸèƒ½\n",
        "\n",
        "def plot_top_samples_dimensions(data_manager, output_dir, iteration, n_top_samples=15, filename=None):\n",
        "    \"\"\"\n",
        "    ç»˜åˆ¶æœ€ä¼˜æ ·æœ¬åœ¨å„ä¸ªç»´åº¦ä¸Šçš„æ•°å€¼åˆ†å¸ƒå›¾\n",
        "    \n",
        "    Args:\n",
        "        data_manager: æ•°æ®ç®¡ç†å™¨\n",
        "        output_dir: è¾“å‡ºç›®å½•\n",
        "        iteration: å½“å‰è¿­ä»£æ¬¡æ•°\n",
        "        n_top_samples: è¦æ˜¾ç¤ºçš„æœ€ä¼˜æ ·æœ¬æ•°é‡\n",
        "        filename: å¯é€‰çš„è‡ªå®šä¹‰æ–‡ä»¶å\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # è·å–æ‰€æœ‰æ ·æœ¬æ•°æ®\n",
        "        all_X = data_manager.get_all_x_orig()\n",
        "        all_y = data_manager.get_all_y_orig()\n",
        "        \n",
        "        if len(all_X) == 0:\n",
        "            logger.warning(\"æ•°æ®ç®¡ç†å™¨ä¸­æ²¡æœ‰æ ·æœ¬æ•°æ®\")\n",
        "            return None\n",
        "        \n",
        "        # æŒ‰yå€¼æ’åºï¼Œè·å–æœ€ä¼˜çš„n_top_samplesä¸ªæ ·æœ¬\n",
        "        sorted_indices = np.argsort(all_y.flatten())\n",
        "        top_indices = sorted_indices[:min(n_top_samples, len(sorted_indices))]\n",
        "        top_samples_X = all_X[top_indices]\n",
        "        top_samples_y = all_y[top_indices]\n",
        "        \n",
        "        # è·å–ç»´åº¦æ•°é‡\n",
        "        n_dimensions = top_samples_X.shape[1]\n",
        "        dimensions = np.arange(1, n_dimensions + 1)\n",
        "        \n",
        "        # åˆ›å»ºå›¾å½¢\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        \n",
        "        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…é¢œè‰²å’Œé€æ˜åº¦\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_samples_X)))\n",
        "        \n",
        "        # ç»˜åˆ¶æ¯ä¸ªæœ€ä¼˜æ ·æœ¬åœ¨å„ç»´åº¦ä¸Šçš„å€¼\n",
        "        for i, (sample_x, sample_y) in enumerate(zip(top_samples_X, top_samples_y)):\n",
        "            alpha = 1.0 if i == 0 else 0.6  # æœ€ä¼˜æ ·æœ¬ä½¿ç”¨ä¸é€æ˜ï¼Œå…¶ä»–ä½¿ç”¨åŠé€æ˜\n",
        "            linewidth = 3 if i == 0 else 1.5\n",
        "            marker_size = 8 if i == 0 else 5\n",
        "            \n",
        "            if i == 0:\n",
        "                label = f\"Best (y={sample_y[0]:.4e})\"\n",
        "            elif i < 3:\n",
        "                label = f\"#{i+1} (y={sample_y[0]:.4e})\"\n",
        "            else:\n",
        "                label = None\n",
        "            \n",
        "            plt.plot(dimensions, sample_x, marker='o', linestyle='-', \n",
        "                    color=colors[i], alpha=alpha, linewidth=linewidth, \n",
        "                    markersize=marker_size, label=label)\n",
        "        \n",
        "        # æ·»åŠ å‚è€ƒçº¿ï¼ˆå¹³å‡å€¼ï¼‰\n",
        "        if len(top_samples_X) > 1:\n",
        "            mean_values = np.mean(top_samples_X, axis=0)\n",
        "            plt.plot(dimensions, mean_values, marker='s', linestyle='--', \n",
        "                    color='red', alpha=0.8, linewidth=2, markersize=6,\n",
        "                    label=f'Mean of Top {len(top_samples_X)}')\n",
        "        \n",
        "        # è®¾ç½®å›¾è¡¨å±æ€§\n",
        "        plt.title(f'Top {n_top_samples} Samples Dimension Values (Iteration {iteration})', fontsize=14)\n",
        "        plt.xlabel('Dimension', fontsize=12)\n",
        "        plt.ylabel('Dimension Value', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # è®¾ç½®xè½´åˆ»åº¦\n",
        "        plt.xticks(dimensions)\n",
        "        \n",
        "        # è®¾ç½®å›¾ä¾‹ï¼Œä½†åªæ˜¾ç¤ºå‰å‡ ä¸ªæœ€é‡è¦çš„æ ·æœ¬ä»¥é¿å…å›¾ä¾‹è¿‡äºæ‹¥æŒ¤\n",
        "        handles, labels = plt.gca().get_legend_handles_labels()\n",
        "        # æ˜¾ç¤ºæœ€ä¼˜æ ·æœ¬ã€å‚è€ƒçº¿ä»¥åŠå…¶ä»–å‡ ä¸ªæ ·æœ¬\n",
        "        important_indices = [0, -1] + list(range(1, min(4, len(handles)-1)))  # æœ€ä¼˜ã€å‚è€ƒçº¿ã€å‰å‡ ä¸ªå…¶ä»–æ ·æœ¬\n",
        "        important_handles = [handles[i] for i in important_indices if i < len(handles)]\n",
        "        important_labels = [labels[i] for i in important_indices if i < len(labels)]\n",
        "        plt.legend(important_handles, important_labels, \n",
        "                  bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "        \n",
        "        # è°ƒæ•´å¸ƒå±€ä»¥é€‚åº”å›¾ä¾‹\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # ä¿å­˜å›¾å½¢\n",
        "        if filename is None:\n",
        "            filename = f\"top_samples_dimensions_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"æœ€ä¼˜æ ·æœ¬ç»´åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}\")\n",
        "        return save_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æœ€ä¼˜æ ·æœ¬ç»´åº¦åˆ†å¸ƒå›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def plot_nte_ducb_trends(ducb_trends_data, output_dir, iteration, dynamic_exploration_factor, \n",
        "                        new_global_best_value=None, filename=None):\n",
        "    \"\"\"\n",
        "    å¯è§†åŒ–NTEæœç´¢è¿‡ç¨‹ä¸­é€‰æ‹©çš„çˆ¶èŠ‚ç‚¹çš„DUCBå€¼å˜åŒ–è¶‹åŠ¿\n",
        "    \n",
        "    Args:\n",
        "        ducb_trends_data (list): åŒ…å«å¤šä¸ªåˆå§‹ç‚¹çš„DUCBå˜åŒ–æ•°æ®\n",
        "        output_dir (str): ä¿å­˜å›¾è¡¨çš„ç›®å½•è·¯å¾„\n",
        "        iteration (int): å½“å‰è¿­ä»£æ¬¡æ•°\n",
        "        dynamic_exploration_factor (float): å½“å‰è¿­ä»£ä½¿ç”¨çš„åŠ¨æ€æ¢ç´¢å› å­\n",
        "        new_global_best_value (float, optional): å¦‚æœæ‰¾åˆ°æ–°çš„å…¨å±€æœ€å°å€¼ï¼Œä¼ å…¥è¯¥å€¼\n",
        "        filename (str, optional): è‡ªå®šä¹‰æ–‡ä»¶å\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # åˆ›å»ºå›¾å½¢\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        \n",
        "        # å®šä¹‰é¢œè‰²å’ŒèŠ‚ç‚¹å˜åŒ–æ ‡è®°\n",
        "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "        node_change_markers = ['s', '^', 'D', 'v', 'p']  # æ–¹å—ã€ä¸‰è§’ã€é’»çŸ³ã€å€’ä¸‰è§’ã€äº”è§’å½¢\n",
        "        node_change_sizes = [80, 80, 80, 80, 80]  # èŠ‚ç‚¹å˜åŒ–æ ‡è®°å¤§å°\n",
        "        \n",
        "        # ç»˜åˆ¶æ¯ä¸ªåˆå§‹ç‚¹çš„DUCBå˜åŒ–è¶‹åŠ¿\n",
        "        for i, trend_data in enumerate(ducb_trends_data):\n",
        "            if not trend_data or 'ducb_values' not in trend_data:\n",
        "                continue\n",
        "                \n",
        "            ducb_values = trend_data['ducb_values']\n",
        "            rollout_indices = trend_data.get('rollout_indices', list(range(len(ducb_values))))\n",
        "            node_changes = trend_data.get('node_changes', [])\n",
        "            model_pred_values = trend_data.get('model_pred_values', [])\n",
        "            \n",
        "            if len(ducb_values) == 0:\n",
        "                continue\n",
        "            \n",
        "            color_idx = i % len(colors)\n",
        "            \n",
        "            # ç»˜åˆ¶DUCBå€¼å˜åŒ–æ›²çº¿\n",
        "            plt.plot(rollout_indices, ducb_values, \n",
        "                    color=colors[color_idx], \n",
        "                    linewidth=2, alpha=0.8,\n",
        "                    label=f'Initial Point {i+1} DUCB')\n",
        "            \n",
        "            # ç»˜åˆ¶æ¨¡å‹é¢„æµ‹å€¼çš„è´Ÿå€¼æ›²çº¿ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰\n",
        "            if len(model_pred_values) == len(ducb_values):\n",
        "                plt.plot(rollout_indices, model_pred_values, \n",
        "                        color=colors[color_idx], \n",
        "                        linewidth=1.5, alpha=0.6, linestyle='--',\n",
        "                        label=f'Initial Point {i+1} -v_ML')\n",
        "            \n",
        "            # åœ¨èŠ‚ç‚¹å˜æ›´ä½ç½®æ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
        "            for change_idx in node_changes:\n",
        "                if change_idx < len(rollout_indices) and change_idx < len(ducb_values):\n",
        "                    plt.scatter(rollout_indices[change_idx], ducb_values[change_idx], \n",
        "                              marker=node_change_markers[color_idx],\n",
        "                              s=node_change_sizes[color_idx],\n",
        "                              color=colors[color_idx], \n",
        "                              edgecolors='white', linewidth=1.5,\n",
        "                              alpha=0.9, zorder=5)\n",
        "        \n",
        "        # è®¾ç½®å›¾è¡¨å±æ€§\n",
        "        title = f'NTE DUCB Values Trend (Iteration {iteration})'\n",
        "        if dynamic_exploration_factor is not None:\n",
        "            title += f'\\\\nExploration Factor: {dynamic_exploration_factor:.2f}'\n",
        "        if new_global_best_value is not None:\n",
        "            title += f' | New Global Best: {new_global_best_value:.6e}'\n",
        "        \n",
        "        plt.title(title, fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('NTE Rollout Round', fontsize=12)\n",
        "        plt.ylabel('DUCB Value', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # åˆ›å»ºå›¾ä¾‹ï¼ŒåŒ…æ‹¬DUCBæ›²çº¿ã€æ¨¡å‹é¢„æµ‹å€¼æ›²çº¿å’ŒèŠ‚ç‚¹å˜åŒ–æ ‡è®°çš„è¯´æ˜\n",
        "        legend_elements = []\n",
        "        \n",
        "        # æ·»åŠ DUCBçº¿æ¡å›¾ä¾‹\n",
        "        for i in range(min(len(ducb_trends_data), len(colors))):\n",
        "            if (ducb_trends_data[i] and 'ducb_values' in ducb_trends_data[i] and \n",
        "                len(ducb_trends_data[i]['ducb_values']) > 0):\n",
        "                legend_elements.append(plt.Line2D([0], [0], color=colors[i], \n",
        "                                                linewidth=2, label=f'Point {i+1} DUCB'))\n",
        "        \n",
        "        # æ·»åŠ æ¨¡å‹é¢„æµ‹å€¼çº¿æ¡å›¾ä¾‹\n",
        "        for i in range(min(len(ducb_trends_data), len(colors))):\n",
        "            if (ducb_trends_data[i] and 'model_pred_values' in ducb_trends_data[i] and \n",
        "                len(ducb_trends_data[i]['model_pred_values']) > 0):\n",
        "                legend_elements.append(plt.Line2D([0], [0], color=colors[i], \n",
        "                                                linewidth=1.5, linestyle='--', alpha=0.6,\n",
        "                                                label=f'Point {i+1} -v_ML'))\n",
        "        \n",
        "        # æ·»åŠ èŠ‚ç‚¹å˜åŒ–æ ‡è®°çš„è¯´æ˜\n",
        "        for i in range(min(3, len(ducb_trends_data))):  # æœ€å¤šæ˜¾ç¤º3ä¸ªæ ‡è®°è¯´æ˜\n",
        "            if i < len(colors) and i < len(node_change_markers):\n",
        "                legend_elements.append(plt.Line2D([0], [0], marker=node_change_markers[i], \n",
        "                                                color=colors[i], linestyle='None',\n",
        "                                                markersize=8, markeredgecolor='white', markeredgewidth=1.5,\n",
        "                                                label=f'Point {i+1} Node Change'))\n",
        "        \n",
        "        plt.legend(handles=legend_elements, fontsize=9, loc='best')\n",
        "        \n",
        "        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
        "        total_rollouts = max([len(trend['ducb_values']) for trend in ducb_trends_data \n",
        "                             if trend and 'ducb_values' in trend] + [0])\n",
        "        total_node_changes = sum([len(trend.get('node_changes', [])) for trend in ducb_trends_data \n",
        "                                 if trend])\n",
        "        \n",
        "        stats_text = f'Total Rollouts: {total_rollouts}\\\\nNode Changes: {total_node_changes}'\n",
        "        plt.text(0.02, 0.98, stats_text, \n",
        "                transform=plt.gca().transAxes, fontsize=10,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        # è°ƒæ•´å¸ƒå±€\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # ä¿å­˜å›¾å½¢\n",
        "        if filename is None:\n",
        "            filename = f\"nte_ducb_trends_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"NTE DUCBè¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "        return save_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶NTE DUCBè¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def plot_search_boundaries(iterations, boundaries_data, output_dir, filename_prefix=\"search_boundaries\"):\n",
        "    \"\"\"ç»˜åˆ¶æœç´¢è¾¹ç•Œå˜åŒ–å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        if not boundaries_data:\n",
        "            logger.warning(\"æ²¡æœ‰è¾¹ç•Œæ•°æ®å¯ä»¥ç»˜åˆ¶\")\n",
        "            return\n",
        "        \n",
        "        # å‡è®¾æˆ‘ä»¬åªæ˜¾ç¤ºå‰å‡ ä¸ªç»´åº¦çš„è¾¹ç•Œå˜åŒ–\n",
        "        n_dims_to_show = min(5, len(boundaries_data[0][1]) if boundaries_data else 0)\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        for dim in range(n_dims_to_show):\n",
        "            mins = [boundary[1][dim] for boundary in boundaries_data]\n",
        "            maxs = [boundary[2][dim] for boundary in boundaries_data]\n",
        "            \n",
        "            plt.subplot(2, 3, dim + 1)\n",
        "            plt.plot(iterations, mins, 'b-', label=f'Min (Dim {dim+1})')\n",
        "            plt.plot(iterations, maxs, 'r-', label=f'Max (Dim {dim+1})')\n",
        "            plt.fill_between(iterations, mins, maxs, alpha=0.3)\n",
        "            plt.title(f'Dimension {dim+1} Search Bounds')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Boundary Value')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, f\"{filename_prefix}.png\")\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"æœç´¢è¾¹ç•Œå›¾å·²ä¿å­˜: {save_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æœç´¢è¾¹ç•Œå›¾å‡ºé”™: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "print(\"âœ“ å®Œæ•´å¯è§†åŒ–å·¥å…· (ç¬¬3éƒ¨åˆ†) å·²åŠ è½½\")\n",
        "print(\"âœ“ æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½å·²å®Œå…¨æ¢å¤ï¼Œä¸åŸä»£ç ä¸€è‡´ï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: æ€§èƒ½ä¼˜åŒ–å’Œè¿›ç¨‹ç®¡ç†æ¨¡å—\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "import subprocess\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PerformanceOptimizer:\n",
        "    \"\"\"æ€§èƒ½ä¼˜åŒ–å™¨ï¼Œè´Ÿè´£ä¼˜åŒ–TensorFlowè®­ç»ƒæ€§èƒ½\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cpu_count = psutil.cpu_count(logical=True)\n",
        "        self.physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "        self.memory_cache = {}\n",
        "        self.is_configured = False\n",
        "        logger.info(f\"æ£€æµ‹åˆ°CPUæ ¸å¿ƒæ•°: ç‰©ç†={self.physical_cpu_count}, é€»è¾‘={self.cpu_count}\")\n",
        "    \n",
        "    def configure_tensorflow_performance(self):\n",
        "        \"\"\"é…ç½®TensorFlowæ€§èƒ½è®¾ç½® - ä¸“é—¨ä¼˜åŒ–Kaggle P100æ˜¾å¡\"\"\"\n",
        "        if self.is_configured:\n",
        "            return\n",
        "            \n",
        "        logger.info(\"å¼€å§‹é…ç½®TensorFlowæ€§èƒ½ä¼˜åŒ– (é’ˆå¯¹Kaggle P100)...\")\n",
        "        \n",
        "        # P100ä¸“é—¨çš„GPUå†…å­˜é…ç½®\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    # P100æ˜¾å¡å†…å­˜ç®¡ç†ä¼˜åŒ–\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                    # è®¾ç½®è™šæ‹ŸGPUå†…å­˜é™åˆ¶ï¼Œé¿å…OOM\n",
        "                    tf.config.experimental.set_virtual_device_configuration(\n",
        "                        gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=15000)]  # P100çº¦16GBï¼Œç•™1GBä½™é‡\n",
        "                    )\n",
        "                logger.info(f\"å·²é…ç½®P100 GPUå†…å­˜ç®¡ç†ï¼Œæ£€æµ‹åˆ°{len(gpus)}ä¸ªGPU\")\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPUå†…å­˜é…ç½®å¤±è´¥: {e}\")\n",
        "        \n",
        "        # é’ˆå¯¹P100çš„å¹¶è¡Œåº¦ä¼˜åŒ–\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(4)  # P100æœ€ä¼˜é…ç½®\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(2)  # P100æœ€ä¼˜é…ç½®\n",
        "        \n",
        "        # P100æ··åˆç²¾åº¦ä¼˜åŒ–\n",
        "        if gpus:\n",
        "            try:\n",
        "                # P100æ”¯æŒæ··åˆç²¾åº¦ä½†éœ€è¦è°¨æ…è®¾ç½®\n",
        "                policy = mixed_precision.Policy('mixed_float16')\n",
        "                mixed_precision.set_global_policy(policy)\n",
        "                logger.info(\"å·²å¯ç”¨P100æ··åˆç²¾åº¦è®­ç»ƒ (mixed_float16)\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"P100æ··åˆç²¾åº¦é…ç½®å¤±è´¥: {e}\")\n",
        "        \n",
        "        # P100ä¸“é—¨çš„æ€§èƒ½é€‰é¡¹\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "        os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # P100æœ€ä¼˜çº¿ç¨‹æ¨¡å¼\n",
        "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # P100æœ€ä¼˜çº¿ç¨‹æ•°\n",
        "        os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'  # P100æ‰¹å½’ä¸€åŒ–ä¼˜åŒ–\n",
        "        \n",
        "        # XLAç¼–è¯‘ä¼˜åŒ– (P100æ”¯æŒ)\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "        \n",
        "        self.is_configured = True\n",
        "        logger.info(\"P100 TensorFlowæ€§èƒ½ä¼˜åŒ–é…ç½®å®Œæˆ\")\n",
        "    \n",
        "    def clear_memory_cache(self):\n",
        "        \"\"\"æ¸…ç†å†…å­˜ç¼“å­˜\"\"\"\n",
        "        self.memory_cache.clear()\n",
        "        gc.collect()\n",
        "        logger.info(\"å·²æ¸…ç†å†…å­˜ç¼“å­˜\")\n",
        "    \n",
        "    def monitor_performance(self, stage_name=\"\"):\n",
        "        \"\"\"ç›‘æ§æ€§èƒ½æŒ‡æ ‡\"\"\"\n",
        "        try:\n",
        "            process = psutil.Process()\n",
        "            memory_info = process.memory_info()\n",
        "            memory_mb = memory_info.rss / 1024 / 1024\n",
        "            cpu_percent = process.cpu_percent()\n",
        "            logger.info(f\"[{stage_name}] å†…å­˜: {memory_mb:.1f}MB, CPU: {cpu_percent:.1f}%\")\n",
        "            return memory_mb, cpu_percent\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"æ€§èƒ½ç›‘æ§å¤±è´¥: {e}\")\n",
        "            return 0, 0\n",
        "\n",
        "class ProcessOptimizer:\n",
        "    \"\"\"è¿›ç¨‹ä¼˜åŒ–å™¨ï¼Œç”¨äºæš‚åœéå¿…è¦çš„è¿›ç¨‹ä»¥é‡Šæ”¾CPUèµ„æº\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.suspended_processes = []\n",
        "        self.process_whitelist = [\n",
        "            'python', 'python3', 'nvidia-smi', 'ssh', 'systemd', 'kernel', \n",
        "            'kthread', 'migration', 'rcu_', 'watchdog', 'bash', 'sh'\n",
        "        ]\n",
        "    \n",
        "    def find_resource_heavy_processes(self, cpu_threshold=20.0, exclude_current=True):\n",
        "        \"\"\"æ‰¾åˆ°å ç”¨CPUèµ„æºè¾ƒå¤šçš„éå¿…è¦è¿›ç¨‹\"\"\"\n",
        "        heavy_processes = []\n",
        "        current_pid = os.getpid()\n",
        "        \n",
        "        try:\n",
        "            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'username']):\n",
        "                try:\n",
        "                    proc_info = proc.info\n",
        "                    \n",
        "                    if exclude_current and proc_info['pid'] == current_pid:\n",
        "                        continue\n",
        "                    \n",
        "                    if any(keyword in proc_info['name'].lower() for keyword in self.process_whitelist):\n",
        "                        continue\n",
        "                    \n",
        "                    cpu_percent = proc_info['cpu_percent']\n",
        "                    if cpu_percent is not None and cpu_percent > cpu_threshold:\n",
        "                        heavy_processes.append({\n",
        "                            'pid': proc_info['pid'],\n",
        "                            'name': proc_info['name'],\n",
        "                            'cpu_percent': cpu_percent,\n",
        "                            'username': proc_info['username']\n",
        "                        })\n",
        "                        \n",
        "                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "                    continue\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"æŸ¥æ‰¾èµ„æºå¯†é›†è¿›ç¨‹æ—¶å‡ºé”™: {e}\")\n",
        "        \n",
        "        return heavy_processes\n",
        "    \n",
        "    def optimize_for_training(self, auto_suspend=False, cpu_threshold=25.0):\n",
        "        \"\"\"ä¸ºè®­ç»ƒä¼˜åŒ–ç³»ç»Ÿè¿›ç¨‹ (Kaggleç¯å¢ƒä¸­ç¦ç”¨è‡ªåŠ¨æš‚åœ)\"\"\"\n",
        "        logger.info(\"æ£€æŸ¥ç³»ç»Ÿè¿›ç¨‹çŠ¶æ€...\")\n",
        "        \n",
        "        if not auto_suspend:\n",
        "            logger.info(\"åœ¨Kaggleç¯å¢ƒä¸­ç¦ç”¨è¿›ç¨‹ä¼˜åŒ–åŠŸèƒ½\")\n",
        "            return []\n",
        "        \n",
        "        # Kaggleç¯å¢ƒä¸æ‰§è¡Œå®é™…çš„è¿›ç¨‹ä¼˜åŒ–\n",
        "        return []\n",
        "    \n",
        "    def restore_all_processes(self):\n",
        "        \"\"\"æ¢å¤æ‰€æœ‰è¢«æš‚åœçš„è¿›ç¨‹ (Kaggleç¯å¢ƒä¸­ä¸ºç©ºæ“ä½œ)\"\"\"\n",
        "        if not self.suspended_processes:\n",
        "            return\n",
        "        \n",
        "        logger.info(\"åœ¨Kaggleç¯å¢ƒä¸­æ— éœ€æ¢å¤è¿›ç¨‹\")\n",
        "\n",
        "# åˆ›å»ºå…¨å±€å®ä¾‹\n",
        "performance_optimizer = PerformanceOptimizer()\n",
        "process_optimizer = ProcessOptimizer()\n",
        "\n",
        "logger.info(\"æ€§èƒ½ä¼˜åŒ–å’Œè¿›ç¨‹ç®¡ç†æ¨¡å—åŠ è½½å®Œæˆ\")\n",
        "print(\"æ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–ï¼Œé€‚é…Kaggleç¯å¢ƒ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: å®Œæ•´çš„ä¸»æ‰§è¡Œé€»è¾‘ (main.pyæ ¸å¿ƒåŠŸèƒ½)\n",
        "import atexit\n",
        "import traceback\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# ä»ä¸»å‡½æ•°main.pyç§»æ¤çš„æ ¸å¿ƒå¸¸é‡å’Œå·¥å…·å‡½æ•°\n",
        "LOG_EPSILON = 1.0  # å¯¹æ•°å˜æ¢å¸¸é‡\n",
        "\n",
        "def rosenbrock_function(x):\n",
        "    \"\"\"è®¡ç®—ç»™å®šå‘é‡xçš„Rosenbrockå‡½æ•°å€¼\"\"\"\n",
        "    return np.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0, axis=0)\n",
        "\n",
        "def rosenbrock_evaluator(x_vector):\n",
        "    \"\"\"å¤–éƒ¨è¯„ä¼°å™¨\"\"\"\n",
        "    return rosenbrock_function(x_vector)\n",
        "\n",
        "def calculate_pearson_correlation(model, data_manager):\n",
        "    \"\"\"è®¡ç®—æ¨¡å‹é¢„æµ‹ä¸çœŸå€¼çš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    if data_manager.num_samples < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        X_data = data_manager.get_all_x_orig()\n",
        "        y_data_orig = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        predictions_log = model.predict(X_data)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_data_orig) < 2:\n",
        "            logger.warning(\"æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°\")\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_data_orig)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_test_set(model, X_test, y_test):\n",
        "    \"\"\"è®¡ç®—æµ‹è¯•é›†ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    if X_test is None or y_test is None or X_test.shape[0] < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(X_test)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        y_test_flat = y_test.flatten()\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_test_flat) < 2:\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_test_flat)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    except Exception as e:\n",
        "        logger.error(f\"è®¡ç®—æµ‹è¯•é›†çš®å°”é€Šç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_cnn_performance(model, data_manager, results_dir, iteration, X_test=None, y_test_orig=None, should_generate_plots=True):\n",
        "    \"\"\"è¯„ä¼°CNNæ€§èƒ½\"\"\"\n",
        "    logger.info(f\"è¯„ä¼°CNNæ€§èƒ½ (è¿­ä»£ {iteration})...\")\n",
        "    metrics = {}\n",
        "    \n",
        "    # è®­ç»ƒé›†è¯„ä¼°\n",
        "    X_train = data_manager.get_all_x_orig()\n",
        "    y_train_orig = data_manager.get_all_y_orig().flatten()\n",
        "    \n",
        "    train_predictions_log = model.predict(X_train)\n",
        "    train_predictions_orig = safe_power10(train_predictions_log)\n",
        "    \n",
        "    if should_generate_plots:\n",
        "        train_mse, train_rmse, train_mae = plot_prediction_vs_truth(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration, \n",
        "            filename=f\"train_prediction_vs_truth_iter_{iteration}.png\"\n",
        "        )\n",
        "        plot_residuals(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration,\n",
        "            filename=f\"train_residual_analysis_iter_{iteration}.png\"\n",
        "        )\n",
        "    else:\n",
        "        train_mse = np.mean((train_predictions_orig - y_train_orig) ** 2)\n",
        "        train_rmse = np.sqrt(train_mse)\n",
        "        train_mae = np.mean(np.abs(train_predictions_orig - y_train_orig))\n",
        "        \n",
        "    metrics.update({\n",
        "        'train_mse': train_mse, 'train_rmse': train_rmse, 'train_mae': train_mae,\n",
        "        'train_sample_count': len(y_train_orig)\n",
        "    })\n",
        "    \n",
        "    # æµ‹è¯•é›†è¯„ä¼°\n",
        "    if X_test is not None and y_test_orig is not None and X_test.shape[0] > 0:\n",
        "        test_predictions_log = model.predict(X_test)\n",
        "        test_predictions_orig = safe_power10(test_predictions_log)\n",
        "        test_ground_truth_orig = y_test_orig.flatten()\n",
        "        \n",
        "        if should_generate_plots:\n",
        "            test_mse, test_rmse, test_mae = plot_prediction_vs_truth(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_prediction_vs_truth_iter_{iteration}.png\"\n",
        "            )\n",
        "            plot_residuals(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_residual_analysis_iter_{iteration}.png\"\n",
        "            )\n",
        "        else:\n",
        "            test_mse = np.mean((test_predictions_orig - test_ground_truth_orig) ** 2)\n",
        "            test_rmse = np.sqrt(test_mse)\n",
        "            test_mae = np.mean(np.abs(test_predictions_orig - test_ground_truth_orig))\n",
        "            \n",
        "        if len(test_predictions_orig) >= 2 and len(test_ground_truth_orig) >= 2:\n",
        "            test_pearson, _ = pearsonr(test_predictions_orig, test_ground_truth_orig)\n",
        "            test_pearson = 0.0 if np.isnan(test_pearson) else test_pearson\n",
        "        else:\n",
        "            test_pearson = 0.0\n",
        "        metrics.update({\n",
        "            'test_mse': test_mse, 'test_rmse': test_rmse, 'test_mae': test_mae,\n",
        "            'test_pearson': test_pearson, 'test_sample_count': len(test_ground_truth_orig)\n",
        "        })\n",
        "    \n",
        "    logger.info(f\"CNNæ€§èƒ½ (è¿­ä»£{iteration}): è®­ç»ƒæ ·æœ¬: {metrics.get('train_sample_count')}, è®­ç»ƒRMSE: {metrics.get('train_rmse'):.4e}\")\n",
        "    if 'test_rmse' in metrics:\n",
        "        logger.info(f\"  æµ‹è¯•æ ·æœ¬: {metrics.get('test_sample_count')}, æµ‹è¯•RMSE: {metrics.get('test_rmse'):.4e}, æµ‹è¯•Pearson: {metrics.get('test_pearson'):.4f}\")\n",
        "    return metrics\n",
        "\n",
        "def calculate_pearson_on_best_samples(model, data_manager, n_best=60):\n",
        "    \"\"\"è®¡ç®—æœ€ä¼˜æ ·æœ¬ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    all_X = data_manager.get_all_x_orig()\n",
        "    all_y_orig = data_manager.get_all_y_orig()\n",
        "    \n",
        "    sorted_indices = np.argsort(all_y_orig.flatten())\n",
        "    sorted_X = all_X[sorted_indices[:n_best]]\n",
        "    sorted_y_orig = all_y_orig[sorted_indices[:n_best]]\n",
        "    \n",
        "    if sorted_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "\n",
        "    min_y_orig_for_range = np.min(sorted_y_orig)\n",
        "    max_y_orig_for_range = np.max(sorted_y_orig)\n",
        "\n",
        "    try:\n",
        "        predictions_log = model.predict(sorted_X)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if np.any(np.isinf(predictions_orig)) or np.any(np.isnan(predictions_orig)):\n",
        "            logger.warning(\"é¢„æµ‹å€¼ä¸­å­˜åœ¨æ— ç©·å¤§æˆ–NaNå€¼\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"è®¡ç®—é¢„æµ‹å€¼æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    \n",
        "    true_values_orig = sorted_y_orig.flatten()\n",
        "    \n",
        "    if len(predictions_orig) < 2 or len(true_values_orig) < 2:\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    \n",
        "    if np.var(predictions_orig) == 0 or np.var(true_values_orig) == 0:\n",
        "        logger.warning(\"é¢„æµ‹å€¼æˆ–çœŸå®å€¼æ²¡æœ‰å˜åŒ–ï¼Œæ— æ³•è®¡ç®—ç›¸å…³ç³»æ•°\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "        \n",
        "    try:\n",
        "        correlation, _ = pearsonr(predictions_orig, true_values_orig)\n",
        "        if np.isnan(correlation) or np.isinf(correlation):\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"è®¡ç®—Pearsonç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    \n",
        "    return (correlation if not np.isnan(correlation) else 0.0), min_y_orig_for_range, max_y_orig_for_range\n",
        "\n",
        "class NTESurrogateWrapper:\n",
        "    \"\"\"NTEæœç´¢å™¨çš„ä»£ç†æ¨¡å‹åŒ…è£…å™¨\"\"\"\n",
        "    def __init__(self, actual_surrogate_model, data_manager_instance):\n",
        "        self.surrogate_model = actual_surrogate_model\n",
        "        self.dm = data_manager_instance\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:\n",
        "        if X_batch_orig.shape[0] == 0:\n",
        "            return np.array([])\n",
        "        if X_batch_orig.ndim == 1:\n",
        "             X_batch_orig = X_batch_orig.reshape(1, -1)\n",
        "\n",
        "        try:\n",
        "            pred_log = self.surrogate_model.predict(X_batch_orig)\n",
        "            \n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return pred_log[0] if isinstance(pred_log, np.ndarray) else pred_log\n",
        "            else:\n",
        "                return pred_log\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"ä»£ç†æ¨¡å‹é¢„æµ‹é”™è¯¯: {e}\")\n",
        "            \n",
        "            # å‡ºé”™æ—¶è¿”å›ä¿å®ˆçš„é«˜å€¼\n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return np.log10(1e6 + 1.0)\n",
        "            else:\n",
        "                return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)\n",
        "\n",
        "def save_iteration_samples_to_csv(X_batch, y_batch, results_dir, iteration):\n",
        "    \"\"\"ä¿å­˜è¿­ä»£æ ·æœ¬åˆ°CSVæ–‡ä»¶\"\"\"\n",
        "    try:\n",
        "        csv_filename = os.path.join(results_dir, f\"iteration_{iteration}_samples.csv\")\n",
        "        \n",
        "        data_dict = {}\n",
        "        for i in range(X_batch.shape[1]):\n",
        "            data_dict[f'x{i+1}'] = X_batch[:, i]\n",
        "        data_dict['y_true'] = y_batch.flatten()\n",
        "        \n",
        "        df = pd.DataFrame(data_dict)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        \n",
        "        logger.info(f\"è¿­ä»£{iteration}çš„{len(y_batch)}ä¸ªæ ·æœ¬å·²ä¿å­˜åˆ°: {csv_filename}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ä¿å­˜æ ·æœ¬åˆ°CSVå¤±è´¥: {e}\")\n",
        "\n",
        "logger.info(\"ä¸»æ‰§è¡Œé€»è¾‘æ ¸å¿ƒåŠŸèƒ½åŠ è½½å®Œæˆ\")\n",
        "print(\"å®Œæ•´çš„main.pyæ ¸å¿ƒåŠŸèƒ½å·²è¿ç§»å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: CNN 1D ä»£ç†æ¨¡å‹ (cnn_1d_surrogate.py)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class CNN1DSurrogate:\n",
        "    def __init__(self, input_dim, log_transform=True):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–CNN 1Dæ›¿ä»£æ¨¡å‹\n",
        "        å‚æ•°:\n",
        "            input_dim: è¾“å…¥ç»´åº¦ (Rosenbrockå‡½æ•°ä¸º20ç»´)\n",
        "            log_transform: æ˜¯å¦ä½¿ç”¨å¯¹æ•°å˜æ¢é¢„å¤„ç†\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        # é…ç½®TensorFlowæ€§èƒ½ä¼˜åŒ–\n",
        "        performance_optimizer.configure_tensorflow_performance()\n",
        "        \n",
        "        self.model = self._build_cnn_model()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.log_transform = log_transform\n",
        "        self.trained = False\n",
        "        \n",
        "        # æ·»åŠ æ•°æ®ç¼“å­˜\n",
        "        self._data_cache_key = None\n",
        "        self._cached_dataset = None\n",
        "    \n",
        "    def _build_cnn_model(self):\n",
        "        \"\"\"æ„å»ºCNN 1Dæ¨¡å‹ç»“æ„\"\"\"\n",
        "        model = Sequential([\n",
        "            # è¾“å…¥å±‚å½¢çŠ¶: (dims, 1)\n",
        "            Conv1D(filters=128, kernel_size=3, strides=1, padding='same', \n",
        "                   activation='elu', input_shape=(self.input_dim, 1)),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Dropout(0.2),\n",
        "            \n",
        "            Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Dropout(0.2),\n",
        "            \n",
        "            Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            Conv1D(filters=16, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            \n",
        "            Conv1D(filters=8, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            Conv1D(filters=4, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            \n",
        "            Flatten(),\n",
        "            Dense(64, activation='elu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "    \n",
        "    def train(self, data_manager, epochs=200, batch_size=32, validation_split=0.2, patience=30, verbose=0, save_path=None):\n",
        "        \"\"\"\n",
        "        ä½¿ç”¨DataManagerä¸­çš„æ•°æ®è®­ç»ƒCNNæ¨¡å‹ï¼Œé›†æˆæ€§èƒ½ä¼˜åŒ–ã€‚\n",
        "        Args:\n",
        "            data_manager (DataManager): æ•°æ®ç®¡ç†å™¨å¯¹è±¡ï¼ŒåŒ…å«è®­ç»ƒæ•°æ®\n",
        "            epochs (int): è®­ç»ƒè½®æ•°\n",
        "            batch_size (int): æ‰¹æ¬¡å¤§å°\n",
        "            validation_split (float): éªŒè¯é›†æ¯”ä¾‹\n",
        "            patience (int): æ—©åœè€å¿ƒå€¼\n",
        "            verbose (int): è®­ç»ƒè¿›åº¦æ˜¾ç¤ºçº§åˆ«\n",
        "            save_path (str, optional): æ¨¡å‹ä¿å­˜è·¯å¾„\n",
        "        Returns:\n",
        "            dict: åŒ…å«è®­ç»ƒå†å²ä¿¡æ¯çš„å­—å…¸\n",
        "        \"\"\"\n",
        "        # ç›‘æ§è®­ç»ƒå¼€å§‹æ—¶çš„æ€§èƒ½\n",
        "        performance_optimizer.monitor_performance(\"è®­ç»ƒå¼€å§‹\")\n",
        "        \n",
        "        # è·å–è®­ç»ƒæ•°æ® - ä½¿ç”¨logå°ºåº¦çš„æ•°æ®ï¼Œé¿å…é‡å¤å˜æ¢\n",
        "        X_train = data_manager.get_all_x_orig()\n",
        "        y_train = data_manager.get_all_y_log()  # ä½¿ç”¨å·²ç»logå˜æ¢çš„æ•°æ®\n",
        "        \n",
        "        # å¦‚æœæ•°æ®æ˜¯äºŒç»´çš„ï¼Œç¡®ä¿y_trainæ˜¯ä¸€ç»´æ•°ç»„\n",
        "        if y_train.ndim > 1:\n",
        "            y_train = y_train.flatten()\n",
        "        \n",
        "        logger.info(f\"è®­ç»ƒæ•°æ®å½¢çŠ¶: X={X_train.shape}, y={y_train.shape}\")\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®\n",
        "        data_cache_key = f\"preprocessed_{hash(X_train.tobytes())}_{self.input_dim}\"\n",
        "        cached_data = performance_optimizer.get_cached_data(data_cache_key)\n",
        "        \n",
        "        if cached_data is not None:\n",
        "            logger.info(\"ä½¿ç”¨ç¼“å­˜çš„é¢„å¤„ç†æ•°æ®\")\n",
        "            X_train_scaled = cached_data\n",
        "        else:\n",
        "            # æ ‡å‡†åŒ–è¾“å…¥æ•°æ®\n",
        "            logger.info(\"è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–...\")\n",
        "            self.scaler.fit(X_train)\n",
        "            X_train_scaled = self.scaler.transform(X_train)\n",
        "            \n",
        "            # ç¼“å­˜é¢„å¤„ç†åçš„æ•°æ®\n",
        "            performance_optimizer.cache_data(data_cache_key, X_train_scaled)\n",
        "            logger.info(\"å·²ç¼“å­˜é¢„å¤„ç†æ•°æ®\")\n",
        "        \n",
        "        # è°ƒæ•´è¾“å…¥å½¢çŠ¶ä¸ºCNN 1Dæ‰€éœ€çš„ (samples, dims, 1)\n",
        "        X_train_scaled = X_train_scaled.reshape(-1, self.input_dim, 1)\n",
        "        \n",
        "        # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®é›†\n",
        "        dataset_cache_key = f\"dataset_{data_cache_key}_{batch_size}_{validation_split}\"\n",
        "        \n",
        "        if self._cached_dataset is not None and self._data_cache_key == dataset_cache_key:\n",
        "            logger.info(\"ä½¿ç”¨ç¼“å­˜çš„æ•°æ®é›†\")\n",
        "            train_dataset = self._cached_dataset\n",
        "        else:\n",
        "            logger.info(\"åˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒæ•°æ®é›†...\")\n",
        "            \n",
        "            # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®\n",
        "            n_samples = len(X_train_scaled)\n",
        "            n_val = int(n_samples * validation_split)\n",
        "            n_train = n_samples - n_val\n",
        "            \n",
        "            # ä½¿ç”¨tf.dataåˆ›å»ºä¼˜åŒ–çš„æ•°æ®é›†\n",
        "            train_dataset = performance_optimizer.create_optimized_dataset(\n",
        "                X_train_scaled[:n_train], \n",
        "                y_train[:n_train], \n",
        "                batch_size=batch_size, \n",
        "                shuffle=True, \n",
        "                cache=True\n",
        "            )\n",
        "            \n",
        "            val_dataset = performance_optimizer.create_optimized_dataset(\n",
        "                X_train_scaled[n_train:], \n",
        "                y_train[n_train:], \n",
        "                batch_size=batch_size, \n",
        "                shuffle=False, \n",
        "                cache=True\n",
        "            )\n",
        "            \n",
        "            # ç¼“å­˜æ•°æ®é›†\n",
        "            self._cached_dataset = (train_dataset, val_dataset)\n",
        "            self._data_cache_key = dataset_cache_key\n",
        "            logger.info(\"å·²ç¼“å­˜ä¼˜åŒ–çš„æ•°æ®é›†\")\n",
        "        \n",
        "        if isinstance(self._cached_dataset, tuple):\n",
        "            train_dataset, val_dataset = self._cached_dataset\n",
        "        else:\n",
        "            train_dataset = self._cached_dataset\n",
        "            val_dataset = None\n",
        "        \n",
        "        # è®¾ç½®æ—©åœå›è°ƒ\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, \n",
        "                                       restore_best_weights=True, verbose=1)\n",
        "        \n",
        "        # ç›‘æ§è®­ç»ƒå‰çš„æ€§èƒ½\n",
        "        performance_optimizer.monitor_performance(\"æ¨¡å‹è®­ç»ƒå‰\")\n",
        "        \n",
        "        # è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®é›†\n",
        "        if val_dataset is not None:\n",
        "            history = self.model.fit(\n",
        "                train_dataset,\n",
        "                validation_data=val_dataset,\n",
        "                epochs=epochs, \n",
        "                callbacks=[early_stopping], \n",
        "                verbose=verbose\n",
        "            )\n",
        "        else:\n",
        "            # å›é€€åˆ°åŸå§‹æ–¹æ³•\n",
        "            history = self.model.fit(\n",
        "                X_train_scaled, y_train, \n",
        "                epochs=epochs, \n",
        "                batch_size=batch_size, \n",
        "                validation_split=validation_split, \n",
        "                callbacks=[early_stopping], \n",
        "                verbose=verbose\n",
        "            )\n",
        "        \n",
        "        self.trained = True\n",
        "        \n",
        "        # ç›‘æ§è®­ç»ƒåçš„æ€§èƒ½\n",
        "        performance_optimizer.monitor_performance(\"æ¨¡å‹è®­ç»ƒå\")\n",
        "        \n",
        "        # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰\n",
        "        if save_path:\n",
        "            self.model.save(save_path)\n",
        "            \n",
        "        # æ¸…ç†ä¸å¿…è¦çš„å†…å­˜\n",
        "        performance_optimizer.optimize_memory_usage()\n",
        "            \n",
        "        return history\n",
        "    \n",
        "    def predict(self, states):\n",
        "        \"\"\"\n",
        "        ä½¿ç”¨è®­ç»ƒå¥½çš„CNN 1Dæ¨¡å‹é¢„æµ‹çŠ¶æ€å€¼\n",
        "        å‚æ•°:\n",
        "            states: éœ€è¦é¢„æµ‹çš„çŠ¶æ€å‘é‡æˆ–çŠ¶æ€å‘é‡åˆ—è¡¨\n",
        "        è¿”å›:\n",
        "            é¢„æµ‹çš„å‡½æ•°å€¼ (logå°ºåº¦)\n",
        "        \"\"\"\n",
        "        if not self.trained:\n",
        "            raise RuntimeError(\"æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹\")\n",
        "            \n",
        "        # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„\n",
        "        states = np.array(states)\n",
        "        \n",
        "        # è¾“å…¥å¯èƒ½æ˜¯å•ä¸ªçŠ¶æ€æˆ–å¤šä¸ªçŠ¶æ€\n",
        "        single_input = False\n",
        "        if states.ndim == 1:\n",
        "            states = states.reshape(1, -1)\n",
        "            single_input = True\n",
        "            \n",
        "        # æ ‡å‡†åŒ–è¾“å…¥\n",
        "        states_scaled = self.scaler.transform(states)\n",
        "        \n",
        "        # è°ƒæ•´å½¢çŠ¶ä¸ºCNN 1Dæ‰€éœ€çš„ (samples, dims, 1)\n",
        "        states_scaled = states_scaled.reshape(-1, self.input_dim, 1)\n",
        "        \n",
        "        # æ¨¡å‹é¢„æµ‹ - è¿”å›logå°ºåº¦çš„é¢„æµ‹å€¼\n",
        "        predictions = self.model.predict(states_scaled, verbose=0)\n",
        "        \n",
        "        # ä¸è¿›è¡Œé€†å˜æ¢ï¼Œç›´æ¥è¿”å›logå°ºåº¦çš„é¢„æµ‹å€¼\n",
        "        return predictions[0][0] if single_input else predictions.flatten()\n",
        "    \n",
        "    def visualize_training_history(self, history, save_path=None):\n",
        "        \"\"\"\n",
        "        å¯è§†åŒ–è®­ç»ƒå†å²\n",
        "        å‚æ•°:\n",
        "            history: è®­ç»ƒè¿”å›çš„å†å²å¯¹è±¡\n",
        "            save_path: å›¾è¡¨ä¿å­˜è·¯å¾„\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        \n",
        "        # æŸå¤±æ›²çº¿\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Loss Curves')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        \n",
        "        # MAEæ›²çº¿\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['mae'], label='Training MAE')\n",
        "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "        plt.title('MAE Curves')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            plt.savefig(save_path)\n",
        "            \n",
        "        plt.show()\n",
        "\n",
        "logger.info(\"CNN1DSurrogateæ¨¡å‹ç±»åŠ è½½å®Œæˆ\")\n",
        "print(\"CNN 1Dä»£ç†æ¨¡å‹å·²å‡†å¤‡å°±ç»ª\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: NTEæœç´¢å™¨ (nte_searcher.py) - é‡ç‚¹è°ƒè¯•æ¨¡å—\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Any, Callable, Optional, Union\n",
        "\n",
        "# Nodeæ•°æ®ç±»\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: Tuple[float, ...]  # çŠ¶æ€å‘é‡ (tuple for hashability)\n",
        "    value_pred: float = float('inf')  # æ›¿ä»£æ¨¡å‹é¢„æµ‹ (v_ML)\n",
        "    visit_count: int = 0  # è®¿é—®æ¬¡æ•°\n",
        "\n",
        "class NTESearcher:\n",
        "    \"\"\"\n",
        "    å®ç°NTE (Neural-Surrogate-Guided Tree Exploration) æœç´¢ç­–ç•¥\n",
        "    é‡ç‚¹ç”¨äºè°ƒè¯•åŠ¨æ€æ¢ç´¢å› å­å‚æ•°\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dimension: int,\n",
        "                 domain: List[Tuple[float, float]],\n",
        "                 c0: float = NTE_C0,\n",
        "                 rollout_rounds: int = NTE_ROLLOUT_ROUNDS,\n",
        "                 validation_batch_size: int = 20):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–NTEæœç´¢å™¨\n",
        "        Args:\n",
        "            dimension (int): çŠ¶æ€å‘é‡ç»´åº¦\n",
        "            domain (List[Tuple[float, float]]): æ¯ä¸ªç»´åº¦çš„èŒƒå›´ [(min1, max1), ...]\n",
        "            c0 (float): DUCBå…¬å¼ä¸­çš„æ¢ç´¢è¶…å‚æ•°\n",
        "            rollout_rounds (int): æ¯æ¬¡æœç´¢è°ƒç”¨çš„å†…éƒ¨æ¢ç´¢è½®æ•°\n",
        "            validation_batch_size (int): æ¯æ¬¡æœç´¢åé€‰æ‹©çš„å€™é€‰æ•°é‡\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.domain = domain\n",
        "        self.domain_mins = np.array([d[0] for d in domain])\n",
        "        self.domain_maxs = np.array([d[1] for d in domain])\n",
        "        self.base_c0 = c0  # å­˜å‚¨åŸºç¡€ c0 å€¼\n",
        "        self.rollout_rounds = rollout_rounds\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "        \n",
        "        # åˆå§‹æµ‹è¯•èŒƒå›´\n",
        "        self.initial_range_length = self.domain_maxs - self.domain_mins\n",
        "        \n",
        "        logger.info(f\"NTESearcher initialized: dim={dimension}, c0={c0}, rollout_rounds={rollout_rounds}\")\n",
        "\n",
        "    def _calculate_dynamic_search_space(self, all_states: np.ndarray = None, all_values: np.ndarray = None, iteration: int = 1):\n",
        "        \"\"\"\n",
        "        æ ¹æ®å½“å‰æ•°æ®é›†ä¸­yå€¼æœ€å°çš„20ä¸ªæ ·æœ¬è®¡ç®—åŠ¨æ€æœç´¢åŒºé—´\n",
        "        Args:\n",
        "            all_states: æ‰€æœ‰å·²çŸ¥çŠ¶æ€\n",
        "            all_values: æ‰€æœ‰å·²çŸ¥çŠ¶æ€å¯¹åº”çš„å‡½æ•°å€¼\n",
        "            iteration: å½“å‰è¿­ä»£æ¬¡æ•°ï¼Œç”¨äºè®¡ç®—æ‹“å±•åŒºé—´\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]: è¿”å›åŠ¨æ€æœç´¢åŒºé—´çš„ä¸Šä¸‹ç•Œ\n",
        "        \"\"\"\n",
        "        # é»˜è®¤ä½¿ç”¨åˆå§‹æœç´¢åŒºé—´\n",
        "        dynamic_mins = self.domain_mins.copy()\n",
        "        dynamic_maxs = self.domain_maxs.copy()\n",
        "        \n",
        "        # å¦‚æœæ²¡æœ‰æä¾›çŠ¶æ€å’Œå€¼ï¼Œè¿”å›åŸå§‹åŒºé—´\n",
        "        if all_states is None or all_values is None or all_states.shape[0] == 0:\n",
        "            return dynamic_mins, dynamic_maxs\n",
        "            \n",
        "        try:\n",
        "            # é€‰æ‹©yå€¼æœ€å°çš„20ä¸ªæ ·æœ¬\n",
        "            num_samples_to_use = min(20, all_values.shape[0])\n",
        "            sorted_indices = np.argsort(all_values.flatten())[:num_samples_to_use]\n",
        "            best_samples = all_states[sorted_indices]\n",
        "            \n",
        "            # è®¡ç®—è¿™äº›æ ·æœ¬åœ¨å„ä¸ªç»´åº¦ä¸Šçš„æœ€å°å€¼å’Œæœ€å¤§å€¼\n",
        "            sample_mins = np.min(best_samples, axis=0)\n",
        "            sample_maxs = np.max(best_samples, axis=0)\n",
        "            \n",
        "            # è®¡ç®—åŸºæœ¬åŒºé—´é•¿åº¦l_iå’ŒåŒºé—´ä¸­å¿ƒa_i\n",
        "            interval_lengths = sample_maxs - sample_mins\n",
        "            interval_centers = (sample_maxs + sample_mins) / 2.0\n",
        "            \n",
        "            # è®¡ç®—æ‹“å±•åŒºé—´å¤§å°dï¼Œéšè¿­ä»£æ¬¡æ•°é€’å‡\n",
        "            extension = self.initial_range_length / (2 * max(1, iteration))\n",
        "            \n",
        "            # ç¡®ä¿æœ€å°åŒºé—´é•¿åº¦\n",
        "            min_interval_length = 3.0\n",
        "            final_intervals = np.maximum(interval_lengths + 2 * extension, min_interval_length)\n",
        "            \n",
        "            # è®¡ç®—åŠ¨æ€æœç´¢åŒºé—´\n",
        "            dynamic_mins = np.maximum(interval_centers - final_intervals / 2.0, self.domain_mins)\n",
        "            dynamic_maxs = np.minimum(interval_centers + final_intervals / 2.0, self.domain_maxs)\n",
        "            \n",
        "            return dynamic_mins, dynamic_maxs\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"è®¡ç®—åŠ¨æ€æœç´¢åŒºé—´æ—¶å‡ºé”™: {str(e)}\")\n",
        "            return self.domain_mins.copy(), self.domain_maxs.copy()\n",
        "\n",
        "    def _mixed_expansion(self, state_vector: np.ndarray, dynamic_factor: float = 1.0, \n",
        "                        all_states: np.ndarray = None, all_values: np.ndarray = None, \n",
        "                        iteration: int = 1, pre_computed_bounds: tuple = None) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        æ··åˆè†¨èƒ€ç­–ç•¥ï¼šç»“åˆç¡®å®šæ€§å’Œéšæœºæ–¹æ³•ç”Ÿæˆå¶èŠ‚ç‚¹\n",
        "        \"\"\"\n",
        "        if pre_computed_bounds is not None:\n",
        "            dynamic_mins, dynamic_maxs = pre_computed_bounds\n",
        "        else:\n",
        "            dynamic_mins, dynamic_maxs = self._calculate_dynamic_search_space(all_states, all_values, iteration)\n",
        "        \n",
        "        # åŸºç¡€æ­¥é•¿ï¼šæ ¹æ®åŠ¨æ€å› å­å’Œæœç´¢åŒºé—´è°ƒæ•´\n",
        "        base_step_size = 0.1 * (dynamic_maxs - dynamic_mins) * dynamic_factor\n",
        "        \n",
        "        leaf_states = []\n",
        "        \n",
        "        # 1. ç¡®å®šæ€§ç§»åŠ¨ (4ä¸ªæ–¹å‘ï¼Œæ¯ä¸ªç»´åº¦éšæœºé€‰æ‹©)\n",
        "        num_deterministic = 4\n",
        "        for _ in range(num_deterministic):\n",
        "            # éšæœºé€‰æ‹©ä¸€ä¸ªç»´åº¦\n",
        "            dim_idx = random.randint(0, self.dimension - 1)\n",
        "            direction = random.choice([-1, 1])\n",
        "            \n",
        "            new_state = state_vector.copy()\n",
        "            step_size = base_step_size[dim_idx]\n",
        "            new_state[dim_idx] += direction * step_size\n",
        "            \n",
        "            # ç¡®ä¿åœ¨åŸŸèŒƒå›´å†…\n",
        "            new_state = np.clip(new_state, dynamic_mins, dynamic_maxs)\n",
        "            leaf_states.append(new_state)\n",
        "        \n",
        "        # 2. éšæœºè†¨èƒ€ (4ä¸ªéšæœºç‚¹)\n",
        "        num_random = 4\n",
        "        for _ in range(num_random):\n",
        "            # åœ¨å½“å‰çŠ¶æ€é™„è¿‘ç”Ÿæˆéšæœºç‚¹\n",
        "            noise = np.random.normal(0, base_step_size * 0.5, self.dimension)\n",
        "            new_state = state_vector + noise\n",
        "            \n",
        "            # ç¡®ä¿åœ¨åŸŸèŒƒå›´å†…\n",
        "            new_state = np.clip(new_state, dynamic_mins, dynamic_maxs)\n",
        "            leaf_states.append(new_state)\n",
        "        \n",
        "        return leaf_states\n",
        "\n",
        "    def _calculate_ducb(self, node: Node, parent_total_visits: int, c_rho: float, current_c0: float) -> float:\n",
        "        \"\"\"\n",
        "        è®¡ç®—DUCB (Discriminative Upper Confidence Bound) å€¼\n",
        "        æ ¸å¿ƒå…¬å¼ï¼šDUCB = -pred_value + c0 * c(rho) * sqrt(ln(N)/n)\n",
        "        \"\"\"\n",
        "        if node.visit_count == 0:\n",
        "            return float('inf')  # æœªè®¿é—®çš„èŠ‚ç‚¹ä¼˜å…ˆçº§æœ€é«˜\n",
        "        \n",
        "        # æ¢ç´¢é¡¹\n",
        "        exploration_term = current_c0 * c_rho * math.sqrt(math.log(max(1, parent_total_visits)) / node.visit_count)\n",
        "        \n",
        "        # DUCB = è´Ÿé¢„æµ‹å€¼ + æ¢ç´¢é¡¹ (å› ä¸ºæˆ‘ä»¬è¦æœ€å°åŒ–)\n",
        "        ducb_value = -node.value_pred + exploration_term\n",
        "        \n",
        "        return ducb_value\n",
        "\n",
        "    def _select_candidates_mixed_strategy(self, visited_nodes: Dict[Tuple[float, ...], Node]) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        æ··åˆç­–ç•¥é€‰æ‹©å€™é€‰ç‚¹ï¼šç»“åˆDUCBå€¼å’Œé¢„æµ‹å€¼\n",
        "        \"\"\"\n",
        "        if not visited_nodes:\n",
        "            return []\n",
        "        \n",
        "        nodes_list = list(visited_nodes.values())\n",
        "        \n",
        "        # æŒ‰é¢„æµ‹å€¼æ’åºï¼Œé€‰æ‹©å‰15ä¸ªæœ€ä¼˜ç‚¹\n",
        "        nodes_by_pred = sorted(nodes_list, key=lambda n: n.value_pred)\n",
        "        best_by_pred = nodes_by_pred[:min(15, len(nodes_by_pred))]\n",
        "        \n",
        "        # è®¡ç®—æ€»è®¿é—®æ¬¡æ•°\n",
        "        total_visits = sum(node.visit_count for node in nodes_list)\n",
        "        \n",
        "        # æŒ‰DUCBå€¼æ’åºï¼Œé€‰æ‹©å‰5ä¸ªé«˜DUCBç‚¹\n",
        "        c_rho = 1.0  # ç®€åŒ–çš„c_rhoå€¼\n",
        "        current_c0 = self.base_c0\n",
        "        \n",
        "        nodes_with_ducb = []\n",
        "        for node in nodes_list:\n",
        "            ducb_val = self._calculate_ducb(node, total_visits, c_rho, current_c0)\n",
        "            nodes_with_ducb.append((node, ducb_val))\n",
        "        \n",
        "        nodes_with_ducb.sort(key=lambda x: x[1], reverse=True)  # æŒ‰DUCBé™åº\n",
        "        best_by_ducb = [item[0] for item in nodes_with_ducb[:5]]\n",
        "        \n",
        "        # åˆå¹¶å¹¶å»é‡\n",
        "        selected_nodes = best_by_pred + best_by_ducb\n",
        "        unique_states = []\n",
        "        seen_states = set()\n",
        "        \n",
        "        for node in selected_nodes:\n",
        "            if node.state not in seen_states:\n",
        "                unique_states.append(np.array(node.state))\n",
        "                seen_states.add(node.state)\n",
        "        \n",
        "        return unique_states[:self.validation_batch_size]\n",
        "\n",
        "    def search(self,\n",
        "               surrogate_model: Any, \n",
        "               current_best_state: np.ndarray,\n",
        "               min_y_observed: float,\n",
        "               dynamic_exploration_factor: float = 1.0,\n",
        "               all_states: np.ndarray = None, \n",
        "               all_values: np.ndarray = None,\n",
        "               previous_iteration_samples: np.ndarray = None,\n",
        "               current_iteration: int = 1,\n",
        "               return_ducb_trends: bool = False) -> Union[List[np.ndarray], Tuple[List[np.ndarray], List[Dict]]]:\n",
        "        \"\"\"\n",
        "        æ”¹è¿›ç‰ˆçš„NTEæœç´¢ç®—æ³•\n",
        "        \n",
        "        é‡ç‚¹è°ƒè¯•å‚æ•°ï¼š\n",
        "        - dynamic_exploration_factor: åŠ¨æ€æ¢ç´¢å› å­ï¼Œå½±å“æœç´¢çš„æ¢ç´¢å¼ºåº¦\n",
        "        \n",
        "        Args:\n",
        "            surrogate_model: æ›¿ä»£æ¨¡å‹ï¼Œå¿…é¡»æœ‰predictæ–¹æ³•\n",
        "            current_best_state: å½“å‰å·²çŸ¥æœ€ä¼˜çŠ¶æ€\n",
        "            min_y_observed: å½“å‰å·²çŸ¥æœ€å°å€¼\n",
        "            dynamic_exploration_factor: åŠ¨æ€æ¢ç´¢å› å­ (é‡ç‚¹è°ƒè¯•å‚æ•°)\n",
        "            all_states: æ‰€æœ‰å·²çŸ¥çŠ¶æ€ç‚¹\n",
        "            all_values: æ‰€æœ‰å·²çŸ¥çŠ¶æ€å€¼\n",
        "            previous_iteration_samples: ä¸Šä¸€æ¬¡è¿­ä»£é€‰æ‹©çš„æ ·æœ¬\n",
        "            current_iteration: å½“å‰è¿­ä»£æ¬¡æ•°\n",
        "            return_ducb_trends: æ˜¯å¦è¿”å›DUCBè¶‹åŠ¿æ•°æ®\n",
        "        \"\"\"\n",
        "        logger.info(f\"NTEæœç´¢å¼€å§‹: è¿­ä»£{current_iteration}, åŠ¨æ€æ¢ç´¢å› å­={dynamic_exploration_factor:.2f}\")\n",
        "        \n",
        "        # DUCBè¶‹åŠ¿æ•°æ®æ”¶é›†\n",
        "        ducb_trends_data = [] if return_ducb_trends else None\n",
        "        \n",
        "        # åŠ¨æ€è°ƒæ•´c0\n",
        "        current_c0 = self.base_c0 * dynamic_exploration_factor\n",
        "        c_rho = 1.0  # ç®€åŒ–çš„rhoè°ƒæ•´å› å­\n",
        "        \n",
        "        logger.info(f\"è°ƒæ•´åçš„c0å€¼: {current_c0:.2f} (åŸºç¡€c0={self.base_c0}, åŠ¨æ€å› å­={dynamic_exploration_factor:.2f})\")\n",
        "        \n",
        "        # é€‰æ‹©èµ·å§‹ç‚¹\n",
        "        best_states_for_nte = []\n",
        "        if all_states is not None and all_values is not None and all_states.shape[0] > 0:\n",
        "            # é€‰æ‹©2ä¸ªæœ€ä¼˜ç‚¹\n",
        "            num_best = min(2, all_states.shape[0])\n",
        "            sorted_indices = np.argsort(all_values.flatten())\n",
        "            for idx in sorted_indices[:num_best]:\n",
        "                best_states_for_nte.append(all_states[idx])\n",
        "            \n",
        "            # é€‰æ‹©1ä¸ªéšæœºç‚¹\n",
        "            if all_states.shape[0] > num_best:\n",
        "                remaining_indices = sorted_indices[num_best:]\n",
        "                random_idx = np.random.choice(remaining_indices, size=1)[0]\n",
        "                best_states_for_nte.append(all_states[random_idx])\n",
        "        else:\n",
        "            # ä½¿ç”¨å½“å‰æœ€ä¼˜çŠ¶æ€æˆ–ç”ŸæˆéšæœºçŠ¶æ€\n",
        "            if current_best_state is not None:\n",
        "                best_states_for_nte.append(current_best_state)\n",
        "            else:\n",
        "                random_state = np.array([random.uniform(self.domain_mins[d], self.domain_maxs[d]) \n",
        "                                       for d in range(self.dimension)])\n",
        "                best_states_for_nte.append(random_state)\n",
        "        \n",
        "        logger.info(f\"ä½¿ç”¨ {len(best_states_for_nte)} ä¸ªèµ·å§‹ç‚¹è¿›è¡ŒNTEæœç´¢\")\n",
        "        \n",
        "        # è®¡ç®—åŠ¨æ€æœç´¢åŒºé—´\n",
        "        try:\n",
        "            dynamic_mins, dynamic_maxs = self._calculate_dynamic_search_space(all_states, all_values, current_iteration)\n",
        "            pre_computed_bounds = (dynamic_mins, dynamic_maxs)\n",
        "            \n",
        "            original_range = np.mean(self.domain_maxs - self.domain_mins)\n",
        "            dynamic_range = np.mean(dynamic_maxs - dynamic_mins)\n",
        "            logger.info(f\"æœç´¢åŒºé—´: åŸå§‹èŒƒå›´={original_range:.2f}, åŠ¨æ€èŒƒå›´={dynamic_range:.2f}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"è®¡ç®—åŠ¨æ€æœç´¢åŒºé—´å¤±è´¥: {e}\")\n",
        "            pre_computed_bounds = None\n",
        "        \n",
        "        # å¯¹æ¯ä¸ªèµ·å§‹ç‚¹è¿›è¡ŒNTEæœç´¢\n",
        "        all_candidates = []\n",
        "        \n",
        "        for nte_id, root_state_vector in enumerate(best_states_for_nte):\n",
        "            visited_nodes: Dict[Tuple[float, ...], Node] = {}\n",
        "            root_state_tuple = tuple(root_state_vector)\n",
        "            \n",
        "            # åˆå§‹åŒ–æ ¹èŠ‚ç‚¹\n",
        "            try:\n",
        "                root_pred_value = surrogate_model.predict(np.array([root_state_vector]))\n",
        "                if isinstance(root_pred_value, (np.ndarray, list)):\n",
        "                    root_pred_value = float(root_pred_value.item() if hasattr(root_pred_value, 'item') else root_pred_value[0])\n",
        "                else:\n",
        "                    root_pred_value = float(root_pred_value)\n",
        "                \n",
        "                if np.isnan(root_pred_value) or np.isinf(root_pred_value):\n",
        "                    root_pred_value = float('inf')\n",
        "            except Exception as e:\n",
        "                logger.error(f\"é¢„æµ‹æ ¹èŠ‚ç‚¹å€¼é”™è¯¯: {e}\")\n",
        "                root_pred_value = float('inf')\n",
        "            \n",
        "            root_node = Node(state=root_state_tuple, value_pred=root_pred_value, visit_count=0)\n",
        "            visited_nodes[root_state_tuple] = root_node\n",
        "            \n",
        "            logger.debug(f\"èµ·å§‹ç‚¹ {nte_id+1}: é¢„æµ‹å€¼={root_pred_value:.4e}\")\n",
        "            \n",
        "            # Rollouté˜¶æ®µ\n",
        "            for rollout_round in range(self.rollout_rounds):\n",
        "                current_root_node = visited_nodes[root_state_tuple]\n",
        "                current_root_node.visit_count += 1\n",
        "                current_root_state_vector = np.array(current_root_node.state)\n",
        "                \n",
        "                # ç”Ÿæˆå¶èŠ‚ç‚¹\n",
        "                leaf_states = self._mixed_expansion(\n",
        "                    current_root_state_vector, \n",
        "                    dynamic_exploration_factor,\n",
        "                    all_states, \n",
        "                    all_values, \n",
        "                    current_iteration,\n",
        "                    pre_computed_bounds=pre_computed_bounds\n",
        "                )\n",
        "                \n",
        "                # é¢„æµ‹å¶èŠ‚ç‚¹å€¼\n",
        "                try:\n",
        "                    leaf_states_batch = np.array(leaf_states)\n",
        "                    leaf_pred_values = surrogate_model.predict(leaf_states_batch)\n",
        "                    if not isinstance(leaf_pred_values, np.ndarray):\n",
        "                        leaf_pred_values = np.array(leaf_pred_values)\n",
        "                    \n",
        "                    # å¤„ç†é¢„æµ‹å€¼\n",
        "                    leaf_pred_values = np.nan_to_num(leaf_pred_values, nan=float('inf'), \n",
        "                                                   posinf=float('inf'), neginf=float('inf'))\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"å¶èŠ‚ç‚¹é¢„æµ‹å¤±è´¥: {e}\")\n",
        "                    leaf_pred_values = np.full(len(leaf_states), float('inf'))\n",
        "                \n",
        "                # æ·»åŠ å¶èŠ‚ç‚¹åˆ°visited_nodes\n",
        "                for leaf_state, pred_value in zip(leaf_states, leaf_pred_values):\n",
        "                    leaf_tuple = tuple(leaf_state)\n",
        "                    if leaf_tuple not in visited_nodes:\n",
        "                        visited_nodes[leaf_tuple] = Node(state=leaf_tuple, value_pred=float(pred_value), visit_count=0)\n",
        "                \n",
        "                # æ”¶é›†DUCBè¶‹åŠ¿æ•°æ®\n",
        "                if return_ducb_trends and rollout_round % 10 == 0:  # æ¯10è½®è®°å½•ä¸€æ¬¡\n",
        "                    total_visits = sum(node.visit_count for node in visited_nodes.values())\n",
        "                    ducb_values = []\n",
        "                    for node in visited_nodes.values():\n",
        "                        ducb_val = self._calculate_ducb(node, total_visits, c_rho, current_c0)\n",
        "                        ducb_values.append(ducb_val)\n",
        "                    \n",
        "                    ducb_trends_data.append({\n",
        "                        'nte_id': nte_id,\n",
        "                        'rollout_round': rollout_round,\n",
        "                        'max_ducb': max(ducb_values) if ducb_values else 0,\n",
        "                        'min_ducb': min(ducb_values) if ducb_values else 0,\n",
        "                        'avg_ducb': np.mean(ducb_values) if ducb_values else 0,\n",
        "                        'num_nodes': len(visited_nodes),\n",
        "                        'dynamic_factor': dynamic_exploration_factor,\n",
        "                        'current_c0': current_c0\n",
        "                    })\n",
        "            \n",
        "            # é€‰æ‹©å€™é€‰ç‚¹\n",
        "            nte_candidates = self._select_candidates_mixed_strategy(visited_nodes)\n",
        "            all_candidates.extend(nte_candidates)\n",
        "            \n",
        "            logger.info(f\"NTE {nte_id+1} å®Œæˆ: è®¿é—®äº†{len(visited_nodes)}ä¸ªèŠ‚ç‚¹, é€‰æ‹©äº†{len(nte_candidates)}ä¸ªå€™é€‰ç‚¹\")\n",
        "        \n",
        "        # å»é‡å¹¶é™åˆ¶æ•°é‡\n",
        "        final_candidates = []\n",
        "        seen_states = set()\n",
        "        \n",
        "        for candidate in all_candidates:\n",
        "            candidate_tuple = tuple(candidate)\n",
        "            if candidate_tuple not in seen_states:\n",
        "                final_candidates.append(candidate)\n",
        "                seen_states.add(candidate_tuple)\n",
        "                \n",
        "                if len(final_candidates) >= self.validation_batch_size:\n",
        "                    break\n",
        "        \n",
        "        logger.info(f\"NTEæœç´¢å®Œæˆ: æ€»å…±è¿”å›{len(final_candidates)}ä¸ªå€™é€‰ç‚¹\")\n",
        "        \n",
        "        if return_ducb_trends:\n",
        "            return final_candidates, ducb_trends_data\n",
        "        else:\n",
        "            return final_candidates\n",
        "\n",
        "logger.info(\"NTESearcherç±»åŠ è½½å®Œæˆ - é‡ç‚¹è°ƒè¯•æ¨¡å—å·²å‡†å¤‡å°±ç»ª\")\n",
        "print(\"NTEæœç´¢å™¨å·²å‡†å¤‡å°±ç»ª - åŠ¨æ€æ¢ç´¢å› å­å¯è°ƒè¯•\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: å¯è§†åŒ–å·¥å…· (plot_utils.py) - ç®€åŒ–ç‰ˆ\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import matplotlib\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# é…ç½®å…¨å±€å­—ä½“è®¾ç½®ä¸ºé»˜è®¤è‹±æ–‡å­—ä½“ (é¿å…Kaggleä¸­æ–‡å­—ä½“é—®é¢˜)\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "plt.switch_backend('Agg')  # ä½¿ç”¨Aggåç«¯ï¼Œé€‚åˆæ— GUIç¯å¢ƒ\n",
        "\n",
        "def create_results_directory(base_results_path):\n",
        "    \"\"\"åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ç»“æœç›®å½•\"\"\"\n",
        "    try:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_dir = os.path.join(base_results_path, timestamp)\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        logger.info(f\"Results directory created: {results_dir}\")\n",
        "        return results_dir\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating results directory: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_global_min_value_trend(iterations, min_values, output_dir, filename=\"global_min_value_trend.png\"):\n",
        "    \"\"\"ç»˜åˆ¶å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # ç¡®ä¿æ•°ç»„é•¿åº¦ç›¸åŒ\n",
        "        if len(iterations) != len(min_values):\n",
        "            min_length = min(len(iterations), len(min_values))\n",
        "            iterations = iterations[:min_length]\n",
        "            min_values = min_values[:min_length]\n",
        "            \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(iterations, min_values, marker='o', linestyle='-', color='b', linewidth=2)\n",
        "        plt.title('Global Minimum Value Trend', fontsize=16)\n",
        "        plt.xlabel('Iteration', fontsize=14)\n",
        "        plt.ylabel('Current Minimum True Value Found', fontsize=14)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # åªåœ¨æ‰¾åˆ°æ›´ä½å…¨å±€æœ€å°å€¼æ—¶æ·»åŠ æ ‡æ³¨\n",
        "        min_values_array = np.array(min_values)\n",
        "        global_min = float('inf')\n",
        "        \n",
        "        for i, (iter_num, current_min) in enumerate(zip(iterations, min_values_array)):\n",
        "            if current_min < global_min:\n",
        "                global_min = current_min\n",
        "                label_txt = f\"{current_min:.3e}\" if current_min >= 1000 else f\"{current_min:.4f}\"\n",
        "                plt.annotate(label_txt, \n",
        "                            (iter_num, current_min),\n",
        "                            textcoords=\"offset points\", \n",
        "                            xytext=(0,10), \n",
        "                            ha='center',\n",
        "                            fontsize=10,\n",
        "                            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"orange\", alpha=0.8))\n",
        "\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        logger.info(f\"å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶å…¨å±€æœ€å°å€¼è¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "\n",
        "def plot_prediction_vs_truth(predictions, ground_truth, output_dir, iteration=None, filename=None, title_suffix=None):\n",
        "    \"\"\"ç»˜åˆ¶é¢„æµ‹å€¼vsçœŸå€¼å¯¹æ¯”å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return None, None, None\n",
        "    \n",
        "    try:\n",
        "        # å±•å¹³æ•°ç»„\n",
        "        predictions_flat = np.array(predictions).flatten()\n",
        "        ground_truth_flat = np.array(ground_truth).flatten()\n",
        "        \n",
        "        # è®¡ç®—æŒ‡æ ‡\n",
        "        mse = np.mean((predictions_flat - ground_truth_flat) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = np.mean(np.abs(predictions_flat - ground_truth_flat))\n",
        "        \n",
        "        # ç»˜å›¾\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(ground_truth_flat, predictions_flat, alpha=0.6, s=30)\n",
        "        \n",
        "        # æ·»åŠ å¯¹è§’çº¿ (ç†æƒ³é¢„æµ‹çº¿)\n",
        "        min_val = min(np.min(ground_truth_flat), np.min(predictions_flat))\n",
        "        max_val = max(np.max(ground_truth_flat), np.max(predictions_flat))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "        \n",
        "        plt.xlabel('Ground Truth', fontsize=14)\n",
        "        plt.ylabel('Predictions', fontsize=14)\n",
        "        title = f'Prediction vs Truth Comparison'\n",
        "        if title_suffix:\n",
        "            title += f' - {title_suffix}'\n",
        "        if iteration is not None:\n",
        "            title += f' (Iteration {iteration})'\n",
        "        plt.title(title, fontsize=16)\n",
        "        \n",
        "        # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬\n",
        "        plt.text(0.05, 0.95, f'RMSE: {rmse:.4e}\\nMAE: {mae:.4e}\\nMSE: {mse:.4e}', \n",
        "                transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"prediction_vs_truth_iter_{iteration}.png\" if iteration else \"prediction_vs_truth.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"é¢„æµ‹vsçœŸå€¼å›¾å·²ä¿å­˜: {save_path}\")\n",
        "        return mse, rmse, mae\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶é¢„æµ‹vsçœŸå€¼å›¾å‡ºé”™: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def plot_residuals(predictions, ground_truth, output_dir, iteration=None, filename=None):\n",
        "    \"\"\"ç»˜åˆ¶æ®‹å·®åˆ†æå›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        predictions_flat = np.array(predictions).flatten()\n",
        "        ground_truth_flat = np.array(ground_truth).flatten()\n",
        "        residuals = predictions_flat - ground_truth_flat\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # æ®‹å·®æ•£ç‚¹å›¾\n",
        "        ax1.scatter(ground_truth_flat, residuals, alpha=0.6, s=30)\n",
        "        ax1.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        ax1.set_xlabel('Ground Truth', fontsize=12)\n",
        "        ax1.set_ylabel('Residuals (Predicted - Truth)', fontsize=12)\n",
        "        ax1.set_title('Residuals vs Ground Truth', fontsize=14)\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # æ®‹å·®ç›´æ–¹å›¾\n",
        "        ax2.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "        ax2.set_xlabel('Residuals', fontsize=12)\n",
        "        ax2.set_ylabel('Frequency', fontsize=12)\n",
        "        ax2.set_title('Residuals Distribution', fontsize=14)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n",
        "        residual_std = np.std(residuals)\n",
        "        residual_mean = np.mean(residuals)\n",
        "        ax2.text(0.05, 0.95, f'Mean: {residual_mean:.4e}\\nStd: {residual_std:.4e}', \n",
        "                transform=ax2.transAxes, fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"residual_analysis_iter_{iteration}.png\" if iteration else \"residual_analysis.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"æ®‹å·®åˆ†æå›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶æ®‹å·®åˆ†æå›¾å‡ºé”™: {e}\")\n",
        "\n",
        "def plot_pearson_correlation_trend(iterations, pearson_coeffs, output_dir, filename=\"pearson_correlation_trend.png\", title=None):\n",
        "    \"\"\"ç»˜åˆ¶çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(iterations, pearson_coeffs, marker='o', linestyle='-', color='g', linewidth=2, markersize=6)\n",
        "        plt.title(title if title else 'Pearson Correlation Coefficient Trend (Model vs. Truth)', fontsize=16)\n",
        "        plt.xlabel('Iteration', fontsize=14)\n",
        "        plt.ylabel('Pearson Correlation Coefficient', fontsize=14)\n",
        "        plt.ylim(-1.1, 1.1)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # æ·»åŠ æ°´å¹³å‚è€ƒçº¿\n",
        "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='r=0.5')\n",
        "        plt.axhline(y=0.8, color='blue', linestyle='--', alpha=0.5, label='r=0.8')\n",
        "        \n",
        "        # åœ¨å…³é”®ç‚¹æ·»åŠ æ•°å€¼æ ‡æ³¨\n",
        "        for i in range(0, len(iterations), max(1, len(iterations)//10)):  # æ¯10%çš„ç‚¹æ ‡æ³¨ä¸€æ¬¡\n",
        "            plt.annotate(f'{pearson_coeffs[i]:.3f}', \n",
        "                        (iterations[i], pearson_coeffs[i]),\n",
        "                        textcoords=\"offset points\", \n",
        "                        xytext=(0,10), \n",
        "                        ha='center',\n",
        "                        fontsize=9)\n",
        "        \n",
        "        plt.legend()\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        logger.info(f\"çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "\n",
        "def plot_nte_ducb_trends(ducb_trends_data, output_dir, iteration, dynamic_exploration_factor, \n",
        "                        new_global_best_value=None, filename=None):\n",
        "    \"\"\"ç»˜åˆ¶NTEæœç´¢çš„DUCBè¶‹åŠ¿å›¾\"\"\"\n",
        "    if not ducb_trends_data or not os.path.exists(output_dir):\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        \n",
        "        # æå–æ•°æ®\n",
        "        rollout_rounds = [d['rollout_round'] for d in ducb_trends_data]\n",
        "        max_ducb_values = [d['max_ducb'] for d in ducb_trends_data]\n",
        "        avg_ducb_values = [d['avg_ducb'] for d in ducb_trends_data]\n",
        "        num_nodes = [d['num_nodes'] for d in ducb_trends_data]\n",
        "        \n",
        "        # åˆ›å»ºå­å›¾\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # 1. DUCBå€¼è¶‹åŠ¿\n",
        "        ax1.plot(rollout_rounds, max_ducb_values, 'r-', marker='o', label='Max DUCB', linewidth=2)\n",
        "        ax1.plot(rollout_rounds, avg_ducb_values, 'b-', marker='s', label='Avg DUCB', linewidth=2)\n",
        "        ax1.set_xlabel('Rollout Round')\n",
        "        ax1.set_ylabel('DUCB Value')\n",
        "        ax1.set_title(f'DUCB Values Trend (Factor: {dynamic_exploration_factor:.2f})')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. èŠ‚ç‚¹æ•°é‡å¢é•¿\n",
        "        ax2.plot(rollout_rounds, num_nodes, 'g-', marker='^', linewidth=2)\n",
        "        ax2.set_xlabel('Rollout Round')\n",
        "        ax2.set_ylabel('Number of Nodes Visited')\n",
        "        ax2.set_title('Node Exploration Growth')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. åŠ¨æ€å› å­å½±å“åˆ†æ\n",
        "        current_c0_values = [d['current_c0'] for d in ducb_trends_data]\n",
        "        ax3.plot(rollout_rounds, current_c0_values, 'm-', marker='d', linewidth=2)\n",
        "        ax3.set_xlabel('Rollout Round')\n",
        "        ax3.set_ylabel('Current C0 Value')\n",
        "        ax3.set_title(f'Dynamic C0 Value (BaseÃ—Factor={dynamic_exploration_factor:.2f})')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. æ¢ç´¢æ•ˆç‡\n",
        "        exploration_efficiency = [m/n if n > 0 else 0 for m, n in zip(max_ducb_values, num_nodes)]\n",
        "        ax4.plot(rollout_rounds, exploration_efficiency, 'orange', marker='*', linewidth=2)\n",
        "        ax4.set_xlabel('Rollout Round')\n",
        "        ax4.set_ylabel('Max DUCB / Nodes Ratio')\n",
        "        ax4.set_title('Exploration Efficiency')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        \n",
        "        # æ·»åŠ å…¨å±€ä¿¡æ¯\n",
        "        if new_global_best_value is not None:\n",
        "            fig.suptitle(f'NTE DUCB Analysis - Iteration {iteration} (New Best: {new_global_best_value:.4e})', \n",
        "                        fontsize=16, y=0.98)\n",
        "        else:\n",
        "            fig.suptitle(f'NTE DUCB Analysis - Iteration {iteration}', fontsize=16, y=0.98)\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"nte_ducb_trends_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"NTE DUCBè¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶NTE DUCBè¶‹åŠ¿å›¾å‡ºé”™: {e}\")\n",
        "\n",
        "def plot_top_samples_dimensions(data_manager, output_dir, iteration, n_top_samples=15, filename=None):\n",
        "    \"\"\"ç»˜åˆ¶æœ€ä¼˜æ ·æœ¬çš„ç»´åº¦åˆ†å¸ƒå›¾\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        all_X = data_manager.get_all_x_orig()\n",
        "        all_y = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        # é€‰æ‹©æœ€ä¼˜çš„nä¸ªæ ·æœ¬\n",
        "        sorted_indices = np.argsort(all_y)[:n_top_samples]\n",
        "        top_samples = all_X[sorted_indices]\n",
        "        top_values = all_y[sorted_indices]\n",
        "        \n",
        "        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ç»Ÿè®¡ä¿¡æ¯\n",
        "        dim_means = np.mean(top_samples, axis=0)\n",
        "        dim_stds = np.std(top_samples, axis=0)\n",
        "        dim_mins = np.min(top_samples, axis=0)\n",
        "        dim_maxs = np.max(top_samples, axis=0)\n",
        "        \n",
        "        dimensions = list(range(len(dim_means)))\n",
        "        \n",
        "        plt.figure(figsize=(15, 8))\n",
        "        \n",
        "        # è¯¯å·®æ¡å›¾æ˜¾ç¤ºå‡å€¼å’Œæ ‡å‡†å·®\n",
        "        plt.errorbar(dimensions, dim_means, yerr=dim_stds, \n",
        "                    fmt='o', capsize=5, capthick=2, linewidth=2, markersize=8,\n",
        "                    label=f'Mean Â± Std (Top {n_top_samples} samples)')\n",
        "        \n",
        "        # æ·»åŠ æœ€å°å€¼æœ€å¤§å€¼èŒƒå›´\n",
        "        plt.fill_between(dimensions, dim_mins, dim_maxs, alpha=0.2, \n",
        "                        label=f'Min-Max Range')\n",
        "        \n",
        "        plt.xlabel('Dimension Index', fontsize=14)\n",
        "        plt.ylabel('Dimension Value', fontsize=14)\n",
        "        plt.title(f'Top {n_top_samples} Samples Dimension Distribution (Iteration {iteration})', fontsize=16)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬\n",
        "        best_value = top_values[0]\n",
        "        plt.text(0.02, 0.98, f'Best Value: {best_value:.4e}', \n",
        "                transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"top_samples_dimensions_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"ç»´åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶ç»´åº¦åˆ†å¸ƒå›¾å‡ºé”™: {e}\")\n",
        "\n",
        "# å¿«é€Ÿå¯è§†åŒ–å‡½æ•° - ä¸“ä¸ºKaggleç¯å¢ƒä¼˜åŒ–\n",
        "def quick_progress_plot(iterations, best_values, pearson_coeffs, output_dir, iteration):\n",
        "    \"\"\"å¿«é€Ÿç”Ÿæˆè¿›åº¦æ¦‚è§ˆå›¾\"\"\"\n",
        "    try:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # æœ€ä¼˜å€¼è¶‹åŠ¿\n",
        "        ax1.plot(iterations, best_values, 'b-o', linewidth=2, markersize=6)\n",
        "        ax1.set_xlabel('Iteration')\n",
        "        ax1.set_ylabel('Best Value Found')\n",
        "        ax1.set_title('Optimization Progress')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.set_yscale('log')  # ä½¿ç”¨å¯¹æ•°å°ºåº¦\n",
        "        \n",
        "        # ç›¸å…³ç³»æ•°è¶‹åŠ¿\n",
        "        ax2.plot(iterations, pearson_coeffs, 'g-s', linewidth=2, markersize=6)\n",
        "        ax2.set_xlabel('Iteration')\n",
        "        ax2.set_ylabel('Pearson Correlation')\n",
        "        ax2.set_title('Model Quality')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.set_ylim(0, 1)\n",
        "        \n",
        "        plt.suptitle(f'DANTE Progress Overview - Iteration {iteration}', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, f\"progress_overview_iter_{iteration}.png\")\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"è¿›åº¦æ¦‚è§ˆå›¾å·²ä¿å­˜: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç»˜åˆ¶è¿›åº¦æ¦‚è§ˆå›¾å‡ºé”™: {e}\")\n",
        "\n",
        "logger.info(\"å¯è§†åŒ–å·¥å…·æ¨¡å—åŠ è½½å®Œæˆ\")\n",
        "print(\"å¯è§†åŒ–å·¥å…·å·²å‡†å¤‡å°±ç»ª - ä¼˜åŒ–åçš„Kaggleç‰ˆæœ¬\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: Kaggleç¯å¢ƒé€‚é…å’Œå®Œæ•´ä¸»é€»è¾‘å‡½æ•°\n",
        "\n",
        "def setup_kaggle_environment():\n",
        "    \"\"\"è®¾ç½®Kaggleç¯å¢ƒ - æ£€æµ‹å’Œé€‚é…è·¯å¾„\"\"\"\n",
        "    # æ£€æµ‹æ˜¯å¦åœ¨Kaggleç¯å¢ƒä¸­\n",
        "    if '/kaggle/' in os.getcwd():\n",
        "        logger.info(\"æ£€æµ‹åˆ°Kaggleç¯å¢ƒ\")\n",
        "        \n",
        "        # Kaggleç¯å¢ƒè·¯å¾„\n",
        "        base_data_path = '/kaggle/input'  # è¾“å…¥æ•°æ®è·¯å¾„\n",
        "        output_path = '/kaggle/working'   # è¾“å‡ºè·¯å¾„\n",
        "        \n",
        "        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "        data_files = {\n",
        "            'x_train': None,\n",
        "            'y_train': None, \n",
        "            'x_test': None,\n",
        "            'y_test': None\n",
        "        }\n",
        "        \n",
        "        # åœ¨Kaggleè¾“å…¥ç›®å½•ä¸­æŸ¥æ‰¾æ•°æ®æ–‡ä»¶\n",
        "        if os.path.exists(base_data_path):\n",
        "            for root, dirs, files in os.walk(base_data_path):\n",
        "                for file in files:\n",
        "                    if 'rosenbrock' in file.lower():\n",
        "                        if 'x_train' in file.lower():\n",
        "                            data_files['x_train'] = os.path.join(root, file)\n",
        "                        elif 'y_train' in file.lower():\n",
        "                            data_files['y_train'] = os.path.join(root, file)\n",
        "                        elif 'x_test' in file.lower():\n",
        "                            data_files['x_test'] = os.path.join(root, file)\n",
        "                        elif 'y_test' in file.lower():\n",
        "                            data_files['y_test'] = os.path.join(root, file)\n",
        "        \n",
        "        return output_path, data_files\n",
        "    else:\n",
        "        logger.info(\"æœ¬åœ°ç¯å¢ƒ\")\n",
        "        # æœ¬åœ°ç¯å¢ƒ\n",
        "        output_path = './dante_results'\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        \n",
        "        data_files = {\n",
        "            'x_train': 'rosenbrock/data_raw/Rosenbrock_x_train.npy',\n",
        "            'y_train': 'rosenbrock/data_raw/Rosenbrock_y_train.npy',\n",
        "            'x_test': 'rosenbrock/data_raw/Rosenbrock_x_test.npy',\n",
        "            'y_test': 'rosenbrock/data_raw/Rosenbrock_y_test.npy'\n",
        "        }\n",
        "        \n",
        "        return output_path, data_files\n",
        "\n",
        "def load_initial_data(data_files, rosenbrock_dimension=20, initial_data_size=800, domain_bounds=(-2.048, 2.048)):\n",
        "    \"\"\"åŠ è½½åˆå§‹æ•°æ®ï¼Œæ”¯æŒKaggleå’Œæœ¬åœ°ç¯å¢ƒ\"\"\"\n",
        "    logger.info(\"å¼€å§‹åŠ è½½åˆå§‹æ•°æ®...\")\n",
        "    \n",
        "    X_test, y_test = None, None\n",
        "    \n",
        "    try:\n",
        "        # å°è¯•åŠ è½½æµ‹è¯•é›†\n",
        "        if data_files['x_test'] and data_files['y_test']:\n",
        "            if os.path.exists(data_files['x_test']) and os.path.exists(data_files['y_test']):\n",
        "                X_test = np.load(data_files['x_test'])\n",
        "                y_test = np.load(data_files['y_test'])\n",
        "                if y_test.ndim == 1:\n",
        "                    y_test = y_test.reshape(-1, 1)\n",
        "                logger.info(f\"æˆåŠŸåŠ è½½æµ‹è¯•æ•°æ®: X shape {X_test.shape}, y shape {y_test.shape}\")\n",
        "        \n",
        "        # å°è¯•åŠ è½½è®­ç»ƒé›†\n",
        "        if data_files['x_train'] and data_files['y_train']:\n",
        "            if os.path.exists(data_files['x_train']) and os.path.exists(data_files['y_train']):\n",
        "                initial_X = np.load(data_files['x_train'])\n",
        "                initial_y = np.load(data_files['y_train'])\n",
        "                if initial_y.ndim == 1:\n",
        "                    initial_y = initial_y.reshape(-1, 1)\n",
        "                logger.info(f\"æˆåŠŸåŠ è½½è®­ç»ƒæ•°æ®: X shape {initial_X.shape}, y shape {initial_y.shape}\")\n",
        "                \n",
        "                # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆå§‹æ ·æœ¬\n",
        "                if initial_X.shape[0] < initial_data_size:\n",
        "                    logger.warning(f\"æ•°æ®ä¸è¶³ï¼Œéœ€è¦ç”Ÿæˆé¢å¤–æ ·æœ¬\")\n",
        "                    additional_samples = initial_data_size - initial_X.shape[0]\n",
        "                    additional_X = np.random.uniform(\n",
        "                        low=domain_bounds[0], high=domain_bounds[1], \n",
        "                        size=(additional_samples, rosenbrock_dimension)\n",
        "                    )\n",
        "                    additional_y = np.array([rosenbrock_evaluator(x) for x in additional_X]).reshape(-1, 1)\n",
        "                    initial_X = np.vstack((initial_X, additional_X))\n",
        "                    initial_y = np.vstack((initial_y, additional_y))\n",
        "                    logger.info(f\"æ·»åŠ {additional_samples}ä¸ªéšæœºæ ·æœ¬ï¼Œæ€»æ ·æœ¬æ•°: {initial_X.shape[0]}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"æœªæŒ‡å®šè®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"åŠ è½½æ•°æ®å¤±è´¥: {e}ï¼Œç”Ÿæˆéšæœºæ•°æ®\")\n",
        "        # ç”Ÿæˆéšæœºæ•°æ®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ\n",
        "        initial_X = np.random.uniform(\n",
        "            low=domain_bounds[0], high=domain_bounds[1], \n",
        "            size=(initial_data_size, rosenbrock_dimension)\n",
        "        )\n",
        "        initial_y = np.array([rosenbrock_evaluator(x) for x in initial_X]).reshape(-1, 1)\n",
        "        logger.info(\"ç”Ÿæˆéšæœºåˆå§‹æ•°æ®\")\n",
        "        \n",
        "        # å¦‚æœæ²¡æœ‰æµ‹è¯•é›†ï¼Œä»éšæœºæ•°æ®ä¸­åˆ†ç¦»ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†\n",
        "        if X_test is None:\n",
        "            test_size = min(200, initial_data_size // 4)\n",
        "            indices = np.random.choice(initial_data_size, test_size, replace=False)\n",
        "            X_test = initial_X[indices]\n",
        "            y_test = initial_y[indices]\n",
        "            \n",
        "            # ä»è®­ç»ƒé›†ä¸­åˆ é™¤æµ‹è¯•æ•°æ®\n",
        "            mask = np.ones(initial_data_size, dtype=bool)\n",
        "            mask[indices] = False\n",
        "            initial_X = initial_X[mask]\n",
        "            initial_y = initial_y[mask]\n",
        "            \n",
        "            # è¡¥å……è®­ç»ƒæ ·æœ¬åˆ°åŸå§‹å¤§å°\n",
        "            if initial_X.shape[0] < initial_data_size:\n",
        "                additional_samples = initial_data_size - initial_X.shape[0]\n",
        "                additional_X = np.random.uniform(\n",
        "                    low=domain_bounds[0], high=domain_bounds[1], \n",
        "                    size=(additional_samples, rosenbrock_dimension)\n",
        "                )\n",
        "                additional_y = np.array([rosenbrock_evaluator(x) for x in additional_X]).reshape(-1, 1)\n",
        "                initial_X = np.vstack((initial_X, additional_X))\n",
        "                initial_y = np.vstack((initial_y, additional_y))\n",
        "            \n",
        "            logger.info(f\"ä»éšæœºæ•°æ®ä¸­åˆ›å»ºæµ‹è¯•é›†: {X_test.shape[0]}ä¸ªæ ·æœ¬\")\n",
        "    \n",
        "    return initial_X, initial_y, X_test, y_test\n",
        "\n",
        "logger.info(\"Kaggleç¯å¢ƒé€‚é…æ¨¡å—åŠ è½½å®Œæˆ\")\n",
        "print(\"ç¯å¢ƒé€‚é…å®Œæˆï¼Œæ”¯æŒæœ¬åœ°å’ŒKaggleç¯å¢ƒ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: ä¸»æ‰§è¡Œé€»è¾‘ (main.pyæ ¸å¿ƒ) - DANTEä¼˜åŒ–ç®—æ³•\n",
        "import time\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "import gc\n",
        "\n",
        "# NTEæœç´¢å™¨åŒ…è£…ç±»\n",
        "class NTESurrogateWrapper:\n",
        "    \"\"\"æ›¿ä»£æ¨¡å‹åŒ…è£…ç±»ï¼Œé€‚é…NTEæœç´¢å™¨æ¥å£\"\"\"\n",
        "    def __init__(self, actual_surrogate_model, data_manager_instance):\n",
        "        self.surrogate_model = actual_surrogate_model\n",
        "        self.dm = data_manager_instance\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:\n",
        "        # ç¡®ä¿è¾“å…¥æ•°æ®æœ‰æ•ˆ\n",
        "        if X_batch_orig.shape[0] == 0:\n",
        "            return np.array([])\n",
        "        if X_batch_orig.ndim == 1:\n",
        "             X_batch_orig = X_batch_orig.reshape(1, -1)\n",
        "\n",
        "        try:\n",
        "            # CNN1DSurrogateè¿›è¡Œé¢„æµ‹ï¼ˆç›´æ¥è¿”å›logå°ºåº¦çš„å€¼ï¼‰\n",
        "            pred_log = self.surrogate_model.predict(X_batch_orig)\n",
        "            \n",
        "            # è¿”å›ä¸è¾“å…¥åŒ¹é…çš„è¾“å‡ºæ ¼å¼\n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return pred_log[0] if isinstance(pred_log, np.ndarray) else pred_log\n",
        "            else:\n",
        "                return pred_log\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Prediction error in NTESurrogateWrapper: {e}\")\n",
        "            # å‡ºé”™æ—¶è¿”å›ä¿å®ˆçš„é«˜å€¼ï¼ˆlogå°ºåº¦ï¼‰\n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return np.log10(1e6 + 1.0)\n",
        "            else:\n",
        "                return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)\n",
        "\n",
        "def calculate_pearson_correlation(model, data_manager):\n",
        "    \"\"\"è®¡ç®—æ¨¡å‹é¢„æµ‹ä¸çœŸå€¼çš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    if data_manager.num_samples < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        X_data = data_manager.get_all_x_orig()\n",
        "        y_data_orig = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        # CNN1DSurrogateè¿”å›logå°ºåº¦é¢„æµ‹å€¼\n",
        "        predictions_log = model.predict(X_data)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_data_orig) < 2:\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_data_orig)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_test_set(model, X_test, y_test_orig):\n",
        "    \"\"\"è®¡ç®—æµ‹è¯•é›†ä¸Šçš„çš®å°”é€Šç›¸å…³ç³»æ•°\"\"\"\n",
        "    if X_test.shape[0] < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(X_test)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_test_orig) < 2:\n",
        "            return 0.0\n",
        "        correlation, _ = pearsonr(predictions_orig, y_test_orig.flatten())\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    except Exception as e:\n",
        "        logger.error(f\"è®¡ç®—æµ‹è¯•é›†çš®å°”é€Šç›¸å…³ç³»æ•°æ—¶å‡ºé”™: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_cnn_performance(model, data_manager, results_dir, iteration, X_test=None, y_test_orig=None, should_generate_plots=True):\n",
        "    \"\"\"è¯„ä¼°CNNæ€§èƒ½\"\"\"\n",
        "    logger.info(f\"Evaluating CNN performance (iteration {iteration})...\")\n",
        "    metrics = {}\n",
        "    \n",
        "    # è®­ç»ƒé›†è¯„ä¼°\n",
        "    X_train = data_manager.get_all_x_orig()\n",
        "    y_train_orig = data_manager.get_all_y_orig().flatten()\n",
        "    \n",
        "    train_predictions_log = model.predict(X_train)\n",
        "    train_predictions_orig = safe_power10(train_predictions_log)\n",
        "    \n",
        "    if should_generate_plots:\n",
        "        train_mse, train_rmse, train_mae = plot_prediction_vs_truth(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration, \n",
        "            filename=f\"train_prediction_vs_truth_iter_{iteration}.png\"\n",
        "        )\n",
        "        plot_residuals(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration,\n",
        "            filename=f\"train_residual_analysis_iter_{iteration}.png\"\n",
        "        )\n",
        "    else:\n",
        "        train_mse = np.mean((train_predictions_orig - y_train_orig) ** 2)\n",
        "        train_rmse = np.sqrt(train_mse)\n",
        "        train_mae = np.mean(np.abs(train_predictions_orig - y_train_orig))\n",
        "        \n",
        "    metrics.update({\n",
        "        'train_mse': train_mse, 'train_rmse': train_rmse, 'train_mae': train_mae,\n",
        "        'train_sample_count': len(y_train_orig)\n",
        "    })\n",
        "    \n",
        "    # æµ‹è¯•é›†è¯„ä¼°\n",
        "    if X_test is not None and y_test_orig is not None and X_test.shape[0] > 0:\n",
        "        test_predictions_log = model.predict(X_test)\n",
        "        test_predictions_orig = safe_power10(test_predictions_log)\n",
        "        test_ground_truth_orig = y_test_orig.flatten()\n",
        "        \n",
        "        if should_generate_plots:\n",
        "            test_mse, test_rmse, test_mae = plot_prediction_vs_truth(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_prediction_vs_truth_iter_{iteration}.png\"\n",
        "            )\n",
        "            plot_residuals(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_residual_analysis_iter_{iteration}.png\"\n",
        "            )\n",
        "        else:\n",
        "            test_mse = np.mean((test_predictions_orig - test_ground_truth_orig) ** 2)\n",
        "            test_rmse = np.sqrt(test_mse)\n",
        "            test_mae = np.mean(np.abs(test_predictions_orig - test_ground_truth_orig))\n",
        "            \n",
        "        if len(test_predictions_orig) >= 2 and len(test_ground_truth_orig) >= 2:\n",
        "            test_pearson, _ = pearsonr(test_predictions_orig, test_ground_truth_orig)\n",
        "            test_pearson = 0.0 if np.isnan(test_pearson) else test_pearson\n",
        "        else:\n",
        "            test_pearson = 0.0\n",
        "        metrics.update({\n",
        "            'test_mse': test_mse, 'test_rmse': test_rmse, 'test_mae': test_mae,\n",
        "            'test_pearson': test_pearson, 'test_sample_count': len(test_ground_truth_orig)\n",
        "        })\n",
        "    \n",
        "    logger.info(f\"CNN Performance (Iter {iteration}): Train RMSE: {metrics.get('train_rmse'):.4e}\")\n",
        "    if 'test_rmse' in metrics:\n",
        "        logger.info(f\"  Test RMSE: {metrics.get('test_rmse'):.4e}, Test Pearson: {metrics.get('test_pearson'):.4f}\")\n",
        "    return metrics\n",
        "\n",
        "def run_dante_optimization(\n",
        "    # åŸºæœ¬å‚æ•°\n",
        "    initial_data_size=800,\n",
        "    n_al_iterations=400,  # åŸé¡¹ç›®å®Œæ•´è¿­ä»£æ•°\n",
        "    rosenbrock_dimension=20,\n",
        "    domain_range=(-2.048, 2.048),\n",
        "    \n",
        "    # CNNå‚æ•°\n",
        "    cnn_training_epochs=200,  # åŸé¡¹ç›®å®Œæ•´è®­ç»ƒè½®æ•°\n",
        "    cnn_batch_size=32,       # åŸé¡¹ç›®æ‰¹æ¬¡å¤§å°\n",
        "    cnn_validation_split=0.2,\n",
        "    cnn_early_stopping_patience=30,  # åŸé¡¹ç›®æ—©åœpatience\n",
        "    \n",
        "    # NTEå‚æ•°\n",
        "    nte_c0=1.0,\n",
        "    nte_rollout_rounds=100,\n",
        "    validation_batch_size=20,\n",
        "    \n",
        "    # åŠ¨æ€æ¢ç´¢å› å­å‚æ•° (é‡ç‚¹è°ƒè¯•)\n",
        "    base_exploration_factor=0.2,  # åŸé¡¹ç›®éªŒè¯çš„æœ€ä½³åŸºç¡€å› å­\n",
        "    exploration_exponent_base=2.0,\n",
        "    exploration_max_limit=20,\n",
        "    \n",
        "    # å¯è§†åŒ–æ§åˆ¶\n",
        "    viz_interval=10,\n",
        "    detailed_viz_interval=50,  # åŸé¡¹ç›®è®¾ç½®\n",
        "    \n",
        "    # æ•°æ®è·¯å¾„\n",
        "    data_input_path=\"/kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw\",\n",
        "    results_output_path=\"/kaggle/working/results\",\n",
        "    \n",
        "    # å®éªŒæ ‡è¯†\n",
        "    experiment_name=\"default\"\n",
        "):\n",
        "    \"\"\"\n",
        "    è¿è¡ŒDANTEä¼˜åŒ–ç®—æ³•çš„ä¸»å‡½æ•°\n",
        "    \n",
        "    é‡ç‚¹è°ƒè¯•å‚æ•°ï¼š\n",
        "    - base_exploration_factor: åŠ¨æ€æ¢ç´¢å› å­åŸºç¡€å€¼ (é»˜è®¤0.8)\n",
        "    - exploration_exponent_base: æŒ‡æ•°åŸºæ•° (é»˜è®¤2.0)\n",
        "    - exploration_max_limit: ä¸Šé™å€¼ (é»˜è®¤20)\n",
        "    \n",
        "    åŠ¨æ€å…¬å¼: factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(\"=\"*60)\n",
        "    logger.info(f\"å¼€å§‹DANTEä¼˜åŒ–å®éªŒ: {experiment_name}\")\n",
        "    logger.info(f\"é‡ç‚¹è°ƒè¯•å‚æ•° - åŸºç¡€å› å­: {base_exploration_factor}, æŒ‡æ•°åŸºæ•°: {exploration_exponent_base}, ä¸Šé™: {exploration_max_limit}\")\n",
        "    logger.info(\"=\"*60)\n",
        "    \n",
        "    # åˆ›å»ºç»“æœç›®å½•\n",
        "    experiment_results_dir = create_results_directory(results_output_path)\n",
        "    if not experiment_results_dir:\n",
        "        logger.error(\"æ— æ³•åˆ›å»ºç»“æœç›®å½•ï¼Œé€€å‡ºå®éªŒ\")\n",
        "        return None\n",
        "    \n",
        "    # è®°å½•å®éªŒå‚æ•°\n",
        "    experiment_params = {\n",
        "        'experiment_name': experiment_name,\n",
        "        'initial_data_size': initial_data_size,\n",
        "        'n_al_iterations': n_al_iterations,\n",
        "        'rosenbrock_dimension': rosenbrock_dimension,\n",
        "        'domain_range': domain_range,\n",
        "        'base_exploration_factor': base_exploration_factor,\n",
        "        'exploration_exponent_base': exploration_exponent_base,\n",
        "        'exploration_max_limit': exploration_max_limit,\n",
        "        'nte_c0': nte_c0,\n",
        "        'nte_rollout_rounds': nte_rollout_rounds,\n",
        "        'cnn_training_epochs': cnn_training_epochs,\n",
        "        'cnn_batch_size': cnn_batch_size,\n",
        "        'timestamp': datetime.datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # ä¿å­˜å®éªŒå‚æ•°\n",
        "    params_file = os.path.join(experiment_results_dir, 'experiment_parameters.json')\n",
        "    with open(params_file, 'w') as f:\n",
        "        import json\n",
        "        json.dump(experiment_params, f, indent=2)\n",
        "    logger.info(f\"å®éªŒå‚æ•°å·²ä¿å­˜: {params_file}\")\n",
        "    \n",
        "    # å®šä¹‰åŸŸ\n",
        "    domain_min, domain_max = domain_range\n",
        "    rosenbrock_domain = [(domain_min, domain_max)] * rosenbrock_dimension\n",
        "    \n",
        "    # åŠ è½½åˆå§‹æ•°æ®\n",
        "    try:\n",
        "        x_train_path = os.path.join(data_input_path, \"Rosenbrock_x_train.npy\")\n",
        "        y_train_path = os.path.join(data_input_path, \"Rosenbrock_y_train.npy\")\n",
        "        x_test_path = os.path.join(data_input_path, \"Rosenbrock_x_test.npy\")\n",
        "        y_test_path = os.path.join(data_input_path, \"Rosenbrock_y_test.npy\")\n",
        "        \n",
        "        # åŠ è½½æµ‹è¯•é›†\n",
        "        X_test = np.load(x_test_path)\n",
        "        y_test = np.load(y_test_path)\n",
        "        if y_test.ndim == 1:\n",
        "            y_test = y_test.reshape(-1, 1)\n",
        "        logger.info(f\"æµ‹è¯•é›†åŠ è½½æˆåŠŸ: X shape {X_test.shape}, y shape {y_test.shape}\")\n",
        "        \n",
        "        # åŠ è½½è®­ç»ƒé›†\n",
        "        initial_X = np.load(x_train_path)\n",
        "        initial_y = np.load(y_train_path)\n",
        "        if initial_y.ndim == 1:\n",
        "            initial_y = initial_y.reshape(-1, 1)\n",
        "        logger.info(f\"è®­ç»ƒé›†åŠ è½½æˆåŠŸ: X shape {initial_X.shape}, y shape {initial_y.shape}\")\n",
        "        \n",
        "        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆå§‹æ ·æœ¬\n",
        "        if initial_X.shape[0] < initial_data_size:\n",
        "            additional_samples = initial_data_size - initial_X.shape[0]\n",
        "            additional_X = np.random.uniform(low=domain_min, high=domain_max, size=(additional_samples, rosenbrock_dimension))\n",
        "            additional_y = np.array([rosenbrock_function(x) for x in additional_X]).reshape(-1, 1)\n",
        "            initial_X = np.vstack((initial_X, additional_X))\n",
        "            initial_y = np.vstack((initial_y, additional_y))\n",
        "            logger.info(f\"è¡¥å……äº†{additional_samples}ä¸ªéšæœºæ ·æœ¬\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"åŠ è½½æ•°æ®å¤±è´¥: {e}\")\n",
        "        # ç”Ÿæˆéšæœºæ•°æ®ä½œä¸ºå¤‡ç”¨\n",
        "        initial_X = np.random.uniform(low=domain_min, high=domain_max, size=(initial_data_size, rosenbrock_dimension))\n",
        "        initial_y = np.array([rosenbrock_function(x) for x in initial_X]).reshape(-1, 1)\n",
        "        \n",
        "        # ç”Ÿæˆæµ‹è¯•é›†\n",
        "        test_size = 200\n",
        "        X_test = np.random.uniform(low=domain_min, high=domain_max, size=(test_size, rosenbrock_dimension))\n",
        "        y_test = np.array([rosenbrock_function(x) for x in X_test]).reshape(-1, 1)\n",
        "        logger.info(\"ä½¿ç”¨éšæœºç”Ÿæˆçš„æ•°æ®\")\n",
        "    \n",
        "    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨\n",
        "    data_manager = DataManager(\n",
        "        initial_X=initial_X, \n",
        "        initial_y=initial_y, \n",
        "        dimension=rosenbrock_dimension\n",
        "    )\n",
        "    logger.info(\"DataManageråˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # åˆå§‹åŒ–CNNæ¨¡å‹\n",
        "    cnn_model = CNN1DSurrogate(input_dim=rosenbrock_dimension)\n",
        "    logger.info(\"CNN1DSurrogateæ¨¡å‹åˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # åˆå§‹åŒ–NTEæœç´¢å™¨\n",
        "    nte_searcher = NTESearcher(\n",
        "        dimension=rosenbrock_dimension, \n",
        "        domain=rosenbrock_domain,\n",
        "        c0=nte_c0,\n",
        "        rollout_rounds=nte_rollout_rounds,\n",
        "        validation_batch_size=validation_batch_size\n",
        "    )\n",
        "    logger.info(\"NTEæœç´¢å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
        "    \n",
        "    # å†å²è®°å½•å˜é‡\n",
        "    history_iterations = []\n",
        "    history_best_y = []\n",
        "    history_pearson_all = []\n",
        "    history_dynamic_factors = []  # è®°å½•åŠ¨æ€æ¢ç´¢å› å­\n",
        "    cnn_performance_history = []\n",
        "    \n",
        "    # åŠ¨æ€æ¢ç´¢å› å­ç›¸å…³å˜é‡\n",
        "    no_improvement_streak = 0\n",
        "    previous_best_y = float('inf')\n",
        "    \n",
        "    # å¼€å§‹ä¸»å¾ªç¯\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for al_iteration in range(n_al_iterations):\n",
        "        iteration_start_time = time.time()\n",
        "        current_iteration = al_iteration + 1\n",
        "        history_iterations.append(current_iteration)\n",
        "        \n",
        "        # è®¡ç®—åŠ¨æ€æ¢ç´¢å› å­ (é‡ç‚¹è°ƒè¯•å…¬å¼)\n",
        "        dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "        history_dynamic_factors.append(dynamic_exploration_factor)\n",
        "        \n",
        "        logger.info(f\"\\n--- è¿­ä»£ {current_iteration}/{n_al_iterations} ---\")\n",
        "        logger.info(f\"åŠ¨æ€æ¢ç´¢å› å­: {dynamic_exploration_factor:.2f} (è¿ç»­æ— æ”¹è¿›æ¬¡æ•°: {no_improvement_streak})\")\n",
        "        \n",
        "        # å†³å®šæ˜¯å¦ç”Ÿæˆå¯è§†åŒ–\n",
        "        should_generate_plots = ((current_iteration % viz_interval == 0) or \n",
        "                               (current_iteration == n_al_iterations))\n",
        "        should_generate_detailed_plots = ((current_iteration % detailed_viz_interval == 0) or \n",
        "                                        (current_iteration == n_al_iterations))\n",
        "        \n",
        "        # è®­ç»ƒCNNæ¨¡å‹\n",
        "        logger.info(\"å¼€å§‹è®­ç»ƒCNNæ¨¡å‹...\")\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=cnn_training_epochs,\n",
        "            batch_size=cnn_batch_size,\n",
        "            validation_split=cnn_validation_split,\n",
        "            patience=cnn_early_stopping_patience,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # å¯è§†åŒ–è®­ç»ƒå†å²\n",
        "        if should_generate_plots:\n",
        "            cnn_model.visualize_training_history(\n",
        "                training_history,\n",
        "                save_path=os.path.join(experiment_results_dir, f\"cnn_training_history_iter_{current_iteration}.png\")\n",
        "            )\n",
        "        \n",
        "        # è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
        "        perf_metrics = evaluate_cnn_performance(\n",
        "            cnn_model, data_manager, experiment_results_dir, current_iteration, \n",
        "            X_test, y_test, should_generate_plots=should_generate_detailed_plots\n",
        "        )\n",
        "        cnn_performance_history.append(perf_metrics)\n",
        "        \n",
        "        # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°\n",
        "        current_pearson = calculate_pearson_on_test_set(cnn_model, X_test, y_test) if X_test is not None else calculate_pearson_correlation(cnn_model, data_manager)\n",
        "        history_pearson_all.append(current_pearson)\n",
        "        logger.info(f\"æ¨¡å‹çš®å°”é€Šç›¸å…³ç³»æ•°: {current_pearson:.4f}\")\n",
        "        \n",
        "        # NTEæœç´¢æ–°å€™é€‰ç‚¹\n",
        "        logger.info(\"å¼€å§‹NTEæœç´¢...\")\n",
        "        nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)\n",
        "        \n",
        "        current_best_x, current_best_y = data_manager.get_current_best()\n",
        "        \n",
        "        try:\n",
        "            nte_search_result = nte_searcher.search(\n",
        "                surrogate_model=nte_model_wrapper,\n",
        "                current_best_state=current_best_x,\n",
        "                min_y_observed=current_best_y,\n",
        "                dynamic_exploration_factor=dynamic_exploration_factor,  # ä¼ é€’åŠ¨æ€æ¢ç´¢å› å­\n",
        "                all_states=data_manager.X_data,\n",
        "                all_values=data_manager.get_all_y_orig(),\n",
        "                current_iteration=current_iteration,\n",
        "                return_ducb_trends=True\n",
        "            )\n",
        "            \n",
        "            nte_candidates, ducb_trends_data = nte_search_result\n",
        "            \n",
        "            if len(nte_candidates) > 0:\n",
        "                logger.info(f\"NTEæœç´¢æ‰¾åˆ°{len(nte_candidates)}ä¸ªå€™é€‰ç‚¹\")\n",
        "                \n",
        "                # è¯„ä¼°å€™é€‰ç‚¹\n",
        "                nte_y_true = np.array([rosenbrock_function(x) for x in nte_candidates]).reshape(-1, 1)\n",
        "                \n",
        "                # æ·»åŠ åˆ°æ•°æ®ç®¡ç†å™¨\n",
        "                data_manager.add_samples(nte_candidates, nte_y_true, re_normalize=True, shuffle=True)\n",
        "                logger.info(f\"å·²æ·»åŠ {len(nte_candidates)}ä¸ªæ–°æ ·æœ¬ï¼Œæ•°æ®é›†æ€»å¤§å°: {data_manager.num_samples}\")\n",
        "                \n",
        "                # æ›´æ–°æœ€ä½³å€¼\n",
        "                current_best_x, current_best_y = data_manager.get_current_best()\n",
        "                history_best_y.append(current_best_y)\n",
        "                \n",
        "                # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›\n",
        "                if current_best_y < previous_best_y:\n",
        "                    logger.info(f\"å‘ç°æ›´ä¼˜è§£: {current_best_y:.6e} (æ”¹è¿›: {previous_best_y - current_best_y:.6e})\")\n",
        "                    no_improvement_streak = 0\n",
        "                    \n",
        "                    # å‘ç°æ–°æœ€ä¼˜è§£æ—¶ç»˜åˆ¶ç»´åº¦åˆ†å¸ƒå›¾\n",
        "                    if should_generate_plots:\n",
        "                        plot_top_samples_dimensions(\n",
        "                            data_manager=data_manager,\n",
        "                            output_dir=experiment_results_dir,\n",
        "                            iteration=current_iteration,\n",
        "                            n_top_samples=15\n",
        "                        )\n",
        "                else:\n",
        "                    no_improvement_streak += 1\n",
        "                    logger.info(f\"æœ¬è½®æ— æ”¹è¿›ï¼Œè¿ç»­æ— æ”¹è¿›æ¬¡æ•°: {no_improvement_streak}\")\n",
        "                \n",
        "                previous_best_y = current_best_y\n",
        "                \n",
        "                # ç»˜åˆ¶DUCBè¶‹åŠ¿å›¾\n",
        "                if ducb_trends_data and should_generate_plots:\n",
        "                    plot_nte_ducb_trends(\n",
        "                        ducb_trends_data=ducb_trends_data,\n",
        "                        output_dir=experiment_results_dir,\n",
        "                        iteration=current_iteration,\n",
        "                        dynamic_exploration_factor=dynamic_exploration_factor\n",
        "                    )\n",
        "            else:\n",
        "                logger.info(\"NTEæœç´¢æœªæ‰¾åˆ°æœ‰æ•ˆå€™é€‰ç‚¹\")\n",
        "                no_improvement_streak += 1\n",
        "                if history_best_y:\n",
        "                    history_best_y.append(history_best_y[-1])\n",
        "                else:\n",
        "                    history_best_y.append(current_best_y)\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"NTEæœç´¢å¤±è´¥: {e}\")\n",
        "            no_improvement_streak += 1\n",
        "            if history_best_y:\n",
        "                history_best_y.append(history_best_y[-1])\n",
        "            else:\n",
        "                history_best_y.append(current_best_y)\n",
        "        \n",
        "        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\n",
        "        if should_generate_plots:\n",
        "            # å…¨å±€æœ€ä¼˜å€¼è¶‹åŠ¿\n",
        "            plot_global_min_value_trend(\n",
        "                history_iterations[:len(history_best_y)], \n",
        "                history_best_y, \n",
        "                experiment_results_dir, \n",
        "                filename=f\"global_min_iter_{current_iteration}.png\"\n",
        "            )\n",
        "            \n",
        "            # çš®å°”é€Šç›¸å…³ç³»æ•°è¶‹åŠ¿\n",
        "            plot_pearson_correlation_trend(\n",
        "                history_iterations, \n",
        "                history_pearson_all, \n",
        "                experiment_results_dir,\n",
        "                filename=f\"pearson_trend_iter_{current_iteration}.png\"\n",
        "            )\n",
        "            \n",
        "            # å¿«é€Ÿè¿›åº¦æ¦‚è§ˆ\n",
        "            quick_progress_plot(\n",
        "                history_iterations[:len(history_best_y)], \n",
        "                history_best_y, \n",
        "                history_pearson_all, \n",
        "                experiment_results_dir, \n",
        "                current_iteration\n",
        "            )\n",
        "        \n",
        "        # å†…å­˜ç®¡ç†\n",
        "        if current_iteration % 10 == 0:\n",
        "            data_manager.clear_cache()\n",
        "            gc.collect()\n",
        "            logger.info(\"å·²æ¸…ç†å†…å­˜ç¼“å­˜\")\n",
        "        \n",
        "        iteration_time = time.time() - iteration_start_time\n",
        "        logger.info(f\"è¿­ä»£{current_iteration}å®Œæˆï¼Œç”¨æ—¶: {iteration_time:.1f}ç§’\")\n",
        "    \n",
        "    # ä¿å­˜æœ€ç»ˆç»“æœ\n",
        "    total_time = time.time() - start_time\n",
        "    logger.info(f\"\\nå®éªŒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’\")\n",
        "    \n",
        "    # ä¿å­˜æ€§èƒ½å†å²\n",
        "    perf_csv_path = os.path.join(experiment_results_dir, \"performance_history.csv\")\n",
        "    pd.DataFrame(cnn_performance_history).to_csv(perf_csv_path, index=False)\n",
        "    \n",
        "    # ä¿å­˜åŠ¨æ€å› å­å†å²\n",
        "    factor_history = pd.DataFrame({\n",
        "        'iteration': history_iterations,\n",
        "        'dynamic_exploration_factor': history_dynamic_factors,\n",
        "        'no_improvement_streak': [factor_data for factor_data in range(len(history_dynamic_factors))],  # ç®€åŒ–è®°å½•\n",
        "        'best_value': history_best_y[:len(history_iterations)],\n",
        "        'pearson_correlation': history_pearson_all\n",
        "    })\n",
        "    factor_csv_path = os.path.join(experiment_results_dir, \"dynamic_factor_history.csv\")\n",
        "    factor_history.to_csv(factor_csv_path, index=False)\n",
        "    \n",
        "    # ä¿å­˜æœ€ç»ˆæ•°æ®çŠ¶æ€\n",
        "    final_data_path = os.path.join(experiment_results_dir, \"final_data_state.npz\")\n",
        "    data_manager.save_data(final_data_path)\n",
        "    \n",
        "    # å®éªŒæ€»ç»“\n",
        "    final_best_x, final_best_y = data_manager.get_current_best()\n",
        "    experiment_summary = {\n",
        "        'experiment_name': experiment_name,\n",
        "        'total_iterations': n_al_iterations,\n",
        "        'final_best_value': float(final_best_y),\n",
        "        'final_pearson_correlation': float(history_pearson_all[-1]),\n",
        "        'total_time_seconds': total_time,\n",
        "        'average_time_per_iteration': total_time / n_al_iterations,\n",
        "        'total_samples_evaluated': data_manager.num_samples,\n",
        "        'max_dynamic_factor': float(max(history_dynamic_factors)),\n",
        "        'final_dynamic_factor': float(history_dynamic_factors[-1])\n",
        "    }\n",
        "    \n",
        "    summary_file = os.path.join(experiment_results_dir, 'experiment_summary.json')\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(experiment_summary, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"å®éªŒæ€»ç»“:\")\n",
        "    logger.info(f\"  æœ€ç»ˆæœ€ä¼˜å€¼: {final_best_y:.6e}\")\n",
        "    logger.info(f\"  æœ€ç»ˆç›¸å…³ç³»æ•°: {history_pearson_all[-1]:.4f}\")\n",
        "    logger.info(f\"  æ€»æ ·æœ¬æ•°: {data_manager.num_samples}\")\n",
        "    logger.info(f\"  æœ€å¤§åŠ¨æ€å› å­: {max(history_dynamic_factors):.2f}\")\n",
        "    logger.info(f\"  ç»“æœä¿å­˜è·¯å¾„: {experiment_results_dir}\")\n",
        "    \n",
        "    return {\n",
        "        'results_dir': experiment_results_dir,\n",
        "        'final_best_value': final_best_y,\n",
        "        'final_pearson': history_pearson_all[-1],\n",
        "        'experiment_summary': experiment_summary,\n",
        "        'performance_history': cnn_performance_history,\n",
        "        'factor_history': factor_history\n",
        "    }\n",
        "\n",
        "logger.info(\"ä¸»æ‰§è¡Œé€»è¾‘æ¨¡å—åŠ è½½å®Œæˆ\")\n",
        "print(\"DANTEä¸»æ‰§è¡Œé€»è¾‘å·²å‡†å¤‡å°±ç»ª - é‡ç‚¹è°ƒè¯•åŠ¨æ€æ¢ç´¢å› å­\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 19: å®é™…è¿è¡Œä»£ç å’Œæµ‹è¯•\n",
        "def run_quick_demo():\n",
        "    \"\"\"è¿è¡Œä¸€ä¸ªå¿«é€Ÿçš„DANTEæ¼”ç¤ºï¼Œé€‚åˆKaggleç¯å¢ƒæµ‹è¯•\"\"\"\n",
        "    logger.info(\"å¼€å§‹è¿è¡ŒDANTEå¿«é€Ÿæ¼”ç¤º...\")\n",
        "    \n",
        "    try:\n",
        "        # å¿«é€Ÿæ¼”ç¤ºå‚æ•°\n",
        "        demo_result = run_dante_experiment(\n",
        "            experiment_name=\"kaggle_demo\",\n",
        "            n_al_iterations=15,  # å¿«é€Ÿæ¼”ç¤ºä»…15æ¬¡è¿­ä»£\n",
        "            cnn_training_epochs=30,  # å‡å°‘è®­ç»ƒè½®æ•°\n",
        "            initial_data_size=200,   # å‡å°‘åˆå§‹æ•°æ®\n",
        "            nte_rollout_rounds=20,   # å‡å°‘NTEè½®æ•°\n",
        "            viz_interval=5,          # æ¯5æ¬¡è¿­ä»£å¯è§†åŒ–\n",
        "            detailed_viz_interval=10,\n",
        "            # ä½¿ç”¨é»˜è®¤çš„åŠ¨æ€æ¢ç´¢å› å­å‚æ•°\n",
        "            base_exploration_factor=0.8,\n",
        "            exploration_exponent_base=2,\n",
        "            exploration_max_limit=20\n",
        "        )\n",
        "        \n",
        "        if demo_result:\n",
        "            logger.info(\"å¿«é€Ÿæ¼”ç¤ºå®Œæˆ!\")\n",
        "            logger.info(f\"æœ€ç»ˆæœ€ä¼˜å€¼: {demo_result['final_best_value']:.6e}\")\n",
        "            logger.info(f\"ç»“æœç›®å½•: {demo_result['results_dir']}\")\n",
        "            return demo_result\n",
        "        else:\n",
        "            logger.error(\"å¿«é€Ÿæ¼”ç¤ºå¤±è´¥\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"å¿«é€Ÿæ¼”ç¤ºå‡ºç°å¼‚å¸¸: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def run_parameter_test():\n",
        "    \"\"\"è¿è¡Œå‚æ•°æµ‹è¯• - ç®€åŒ–ç‰ˆå‚æ•°æ‰«æ\"\"\"\n",
        "    logger.info(\"å¼€å§‹è¿è¡Œå‚æ•°æµ‹è¯•...\")\n",
        "    \n",
        "    # ç®€åŒ–çš„å‚æ•°ç»„åˆï¼Œé€‚åˆKaggleç¯å¢ƒ\n",
        "    test_configs = [\n",
        "        {'base': 0.8, 'exp': 2.0, 'max': 15, 'name': 'conservative'},\n",
        "        {'base': 1.0, 'exp': 2.5, 'max': 20, 'name': 'aggressive'}\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for i, config in enumerate(test_configs):\n",
        "        logger.info(f\"æµ‹è¯•é…ç½® {i+1}/{len(test_configs)}: {config['name']}\")\n",
        "        \n",
        "        try:\n",
        "            result = run_dante_experiment(\n",
        "                experiment_name=f\"param_test_{config['name']}\",\n",
        "                n_al_iterations=10,  # å¿«é€Ÿæµ‹è¯•\n",
        "                cnn_training_epochs=20,\n",
        "                initial_data_size=150,\n",
        "                nte_rollout_rounds=15,\n",
        "                viz_interval=5,\n",
        "                base_exploration_factor=config['base'],\n",
        "                exploration_exponent_base=config['exp'],\n",
        "                exploration_max_limit=config['max']\n",
        "            )\n",
        "            \n",
        "            if result:\n",
        "                results.append({\n",
        "                    'config_name': config['name'],\n",
        "                    'parameters': config,\n",
        "                    'final_best_value': result['final_best_value'],\n",
        "                    'final_pearson': result['final_pearson']\n",
        "                })\n",
        "                logger.info(f\"é…ç½® {config['name']} å®Œæˆ: {result['final_best_value']:.6e}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"é…ç½® {config['name']} å¤±è´¥: {e}\")\n",
        "    \n",
        "    if results:\n",
        "        # æ¯”è¾ƒç»“æœ\n",
        "        logger.info(\"\\nå‚æ•°æµ‹è¯•ç»“æœæ¯”è¾ƒ:\")\n",
        "        for r in results:\n",
        "            logger.info(f\"{r['config_name']}: æœ€ä¼˜å€¼={r['final_best_value']:.6e}, Pearson={r['final_pearson']:.4f}\")\n",
        "        \n",
        "        # æ‰¾åˆ°æœ€ä½³é…ç½®\n",
        "        best = min(results, key=lambda x: x['final_best_value'])\n",
        "        logger.info(f\"\\næœ€ä½³é…ç½®: {best['config_name']}\")\n",
        "        logger.info(f\"å‚æ•°: {best['parameters']}\")\n",
        "        \n",
        "        return results\n",
        "    else:\n",
        "        logger.error(\"æ‰€æœ‰å‚æ•°æµ‹è¯•éƒ½å¤±è´¥äº†\")\n",
        "        return None\n",
        "\n",
        "# å‡†å¤‡å°±ç»ªæç¤º\n",
        "logger.info(\"=\"*80)\n",
        "logger.info(\"DANTE Kaggle Notebook å·²å®Œå…¨åŠ è½½!\")\n",
        "logger.info(\"=\"*80)\n",
        "logger.info(\"å¯ç”¨çš„è¿è¡Œå‡½æ•°:\")\n",
        "logger.info(\"1. run_quick_demo() - è¿è¡Œå¿«é€Ÿæ¼”ç¤º (æ¨èå…ˆè¿è¡Œ)\")\n",
        "logger.info(\"2. run_parameter_test() - è¿è¡Œå‚æ•°å¯¹æ¯”æµ‹è¯•\")\n",
        "logger.info(\"3. run_dante_experiment(name, **params) - è¿è¡Œè‡ªå®šä¹‰å®éªŒ\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "print(\"âœ… DANTE Notebook å®Œå…¨åŠ è½½å®Œæˆ!\")\n",
        "print(\"ğŸš€ ç°åœ¨å¯ä»¥è¿è¡Œ: run_quick_demo() å¼€å§‹ä½“éªŒ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DANTE Rosenbrockä¼˜åŒ– - Kaggleå®Œæ•´ç‰ˆæœ¬\n",
        "\n",
        "## ğŸ¯ é¡¹ç›®æ¦‚è¿°\n",
        "æœ¬notebookåŒ…å«å®Œæ•´çš„DANTE (Deep Active Learning with Neural Tree Exploration) ç®—æ³•å®ç°ï¼Œä¸“é—¨ç”¨äºRosenbrockå‡½æ•°ä¼˜åŒ–ã€‚\n",
        "\n",
        "## ğŸ“‹ æ ¸å¿ƒåŠŸèƒ½æ¨¡å—\n",
        "- âœ… **å®Œæ•´çš„æ•°æ®ç®¡ç†å™¨** - æ”¯æŒåŠ¨æ€æ ·æœ¬æ·»åŠ å’Œæƒé‡ç®¡ç†\n",
        "- âœ… **CNN1Dæ›¿ä»£æ¨¡å‹** - 1ç»´å·ç§¯ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºå‡½æ•°è¿‘ä¼¼\n",
        "- âœ… **NTEæœç´¢å™¨** - ç¥ç»æ ‘æ¢ç´¢ç®—æ³•ï¼ŒåŒ…å«1100+è¡Œçš„å®Œæ•´å®ç°\n",
        "- âœ… **å¯è§†åŒ–å·¥å…·** - è¶…è¿‡20ç§å¯è§†åŒ–å›¾è¡¨ï¼Œæ”¯æŒè®­ç»ƒè¿‡ç¨‹ç›‘æ§\n",
        "- âœ… **æ€§èƒ½ä¼˜åŒ–** - å†…å­˜ç®¡ç†å’ŒTensorFlowæ€§èƒ½ä¼˜åŒ–\n",
        "- âœ… **åŠ¨æ€æ¢ç´¢å› å­è°ƒè¯•** - æ ¸å¿ƒç®—æ³•å‚æ•°çš„è°ƒè¯•æ¡†æ¶\n",
        "\n",
        "## ğŸ”§ é‡ç‚¹è°ƒè¯•çš„åŠ¨æ€æ¢ç´¢å› å­å…¬å¼\n",
        "```python\n",
        "dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "```\n",
        "\n",
        "## ğŸš€ å¿«é€Ÿå¼€å§‹\n",
        "1. **è¿è¡Œæ¼”ç¤º**: `run_quick_demo()` - 15æ¬¡è¿­ä»£çš„å¿«é€Ÿæ¼”ç¤º\n",
        "2. **å‚æ•°æµ‹è¯•**: `run_parameter_test()` - å¯¹æ¯”ä¸åŒå‚æ•°é…ç½®\n",
        "3. **å®Œæ•´å®éªŒ**: `run_dante_experiment()` - è‡ªå®šä¹‰å®Œæ•´å®éªŒ\n",
        "\n",
        "## ğŸ“Š é€‚é…è¯´æ˜\n",
        "- å·²é’ˆå¯¹Kaggleç¯å¢ƒä¼˜åŒ–å‚æ•° (è¿­ä»£æ¬¡æ•°ã€è®­ç»ƒè½®æ•°ç­‰)\n",
        "- æ”¯æŒæœ¬åœ°å’ŒKaggleç¯å¢ƒè‡ªåŠ¨æ£€æµ‹\n",
        "- åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œå†…å­˜ç®¡ç†\n",
        "- æ‰€æœ‰å¯è§†åŒ–ä½¿ç”¨è‹±æ–‡å­—ä½“ï¼Œé¿å…å­—ä½“é”™è¯¯\n",
        "\n",
        "## âš¡ è¿è¡Œå»ºè®®\n",
        "1. é¦–æ¬¡è¿è¡Œæ¨èä½¿ç”¨ `run_quick_demo()` æµ‹è¯•ç¯å¢ƒ\n",
        "2. Kaggle GPUç¯å¢ƒä¸‹çº¦10-15åˆ†é’Ÿå®Œæˆå¿«é€Ÿæ¼”ç¤º\n",
        "3. å®Œæ•´å®éªŒå»ºè®®åœ¨æœ¬åœ°ç¯å¢ƒæˆ–é«˜é…ç½®Kaggleç¯å¢ƒè¿è¡Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 21: æœ€ç»ˆå®Œæ•´æ€§æ£€æŸ¥å’Œè¿è¡Œæµ‹è¯•\n",
        "def check_notebook_completeness():\n",
        "    \"\"\"æ£€æŸ¥notebookçš„å®Œæ•´æ€§ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½æ­£ç¡®åŠ è½½\"\"\"\n",
        "    logger.info(\"å¼€å§‹æ£€æŸ¥notebookå®Œæ•´æ€§...\")\n",
        "    \n",
        "    checks = {\n",
        "        'DataManager': DataManager,\n",
        "        'CNN1DSurrogate': CNN1DSurrogate, \n",
        "        'NTESearcher': NTESearcher,\n",
        "        'PerformanceOptimizer': PerformanceOptimizer,\n",
        "        'ProcessOptimizer': ProcessOptimizer,\n",
        "        'NTESurrogateWrapper': NTESurrogateWrapper,\n",
        "        'safe_power10': safe_power10,\n",
        "        'rosenbrock_function': rosenbrock_function,\n",
        "        'plot_global_min_value_trend': plot_global_min_value_trend,\n",
        "        'plot_prediction_vs_truth': plot_prediction_vs_truth,\n",
        "        'plot_nte_ducb_trends': plot_nte_ducb_trends,\n",
        "        'run_dante_experiment': run_dante_experiment,\n",
        "        'setup_kaggle_environment': setup_kaggle_environment,\n",
        "        'load_initial_data': load_initial_data\n",
        "    }\n",
        "    \n",
        "    missing = []\n",
        "    working = []\n",
        "    \n",
        "    for name, obj in checks.items():\n",
        "        try:\n",
        "            if callable(obj):\n",
        "                working.append(name)\n",
        "            else:\n",
        "                working.append(name)\n",
        "        except NameError:\n",
        "            missing.append(name)\n",
        "    \n",
        "    # æ£€æŸ¥æ ¸å¿ƒä¾èµ–\n",
        "    try:\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        import tensorflow as tf\n",
        "        import torch\n",
        "        import psutil\n",
        "        import scipy\n",
        "        deps_ok = True\n",
        "    except ImportError as e:\n",
        "        deps_ok = False\n",
        "        logger.error(f\"ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}\")\n",
        "    \n",
        "    # æ£€æŸ¥GPUå¯ç”¨æ€§\n",
        "    gpu_available = len(tf.config.experimental.list_physical_devices('GPU')) > 0\n",
        "    torch_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    # æŠ¥å‘Šç»“æœ\n",
        "    logger.info(f\"âœ… ç»„ä»¶æ£€æŸ¥: {len(working)}/{len(checks)} ä¸ªç»„ä»¶æ­£å¸¸\")\n",
        "    logger.info(f\"âœ… ä¾èµ–æ£€æŸ¥: {'é€šè¿‡' if deps_ok else 'å¤±è´¥'}\")\n",
        "    logger.info(f\"ğŸ”§ TensorFlow GPU: {'å¯ç”¨' if gpu_available else 'ä¸å¯ç”¨'}\")\n",
        "    logger.info(f\"ğŸ”§ PyTorch GPU: {'å¯ç”¨' if torch_gpu else 'ä¸å¯ç”¨'}\")\n",
        "    \n",
        "    if missing:\n",
        "        logger.warning(f\"âŒ ç¼ºå¤±ç»„ä»¶: {missing}\")\n",
        "        return False\n",
        "    \n",
        "    if not deps_ok:\n",
        "        logger.error(\"âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥\")\n",
        "        return False\n",
        "    \n",
        "    logger.info(\"ğŸ‰ Notebookå®Œæ•´æ€§æ£€æŸ¥é€šè¿‡!\")\n",
        "    return True\n",
        "\n",
        "def quick_system_test():\n",
        "    \"\"\"å¿«é€Ÿç³»ç»Ÿæµ‹è¯•ï¼ŒéªŒè¯æ ¸å¿ƒåŠŸèƒ½\"\"\"\n",
        "    logger.info(\"å¼€å§‹å¿«é€Ÿç³»ç»Ÿæµ‹è¯•...\")\n",
        "    \n",
        "    try:\n",
        "        # æµ‹è¯•æ•°æ®ç®¡ç†å™¨\n",
        "        test_X = np.random.randn(50, 20)\n",
        "        test_y = np.random.randn(50, 1)\n",
        "        dm = DataManager(test_X, test_y, 20)\n",
        "        logger.info(\"âœ… æ•°æ®ç®¡ç†å™¨æµ‹è¯•é€šè¿‡\")\n",
        "        \n",
        "        # æµ‹è¯•CNNæ¨¡å‹\n",
        "        cnn = CNN1DSurrogate(input_dim=20)\n",
        "        logger.info(\"âœ… CNNæ¨¡å‹åˆå§‹åŒ–é€šè¿‡\")\n",
        "        \n",
        "        # æµ‹è¯•NTEæœç´¢å™¨\n",
        "        domain = [(-2, 2)] * 20\n",
        "        nte = NTESearcher(dimension=20, domain=domain, c0=1, rollout_rounds=5, validation_batch_size=5)\n",
        "        logger.info(\"âœ… NTEæœç´¢å™¨åˆå§‹åŒ–é€šè¿‡\")\n",
        "        \n",
        "        # æµ‹è¯•ç¯å¢ƒè®¾ç½®\n",
        "        output_path, data_files = setup_kaggle_environment()\n",
        "        logger.info(f\"âœ… ç¯å¢ƒè®¾ç½®é€šè¿‡: {output_path}\")\n",
        "        \n",
        "        # æµ‹è¯•Rosenbrockå‡½æ•°\n",
        "        test_vec = np.array([1.0] * 20)\n",
        "        result = rosenbrock_function(test_vec)\n",
        "        logger.info(f\"âœ… Rosenbrockå‡½æ•°æµ‹è¯•é€šè¿‡: f(1,...,1) = {result}\")\n",
        "        \n",
        "        logger.info(\"ğŸ‰ æ‰€æœ‰ç³»ç»Ÿæµ‹è¯•é€šè¿‡!\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# è¿è¡Œæ£€æŸ¥\n",
        "completeness_ok = check_notebook_completeness()\n",
        "system_ok = quick_system_test()\n",
        "\n",
        "if completeness_ok and system_ok:\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"ğŸš€ DANTE Notebookå·²å®Œå…¨å‡†å¤‡å°±ç»ª!\")\n",
        "    logger.info(\"ğŸ’¡ å»ºè®®è¿è¡Œ: run_quick_demo() å¼€å§‹ä½“éªŒ\")\n",
        "    logger.info(\"=\"*80)\n",
        "    print(\"âœ… NotebookéªŒè¯å®Œæˆ - å¯ä»¥å®‰å…¨è¿è¡Œ!\")\n",
        "else:\n",
        "    logger.error(\"âŒ NotebookéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯\")\n",
        "    print(\"âŒ éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# âœ… è¶…å‚æ•°å·²æ¢å¤åˆ°åŸé¡¹ç›®è®¾ç½®\n",
        "\n",
        "**é‡è¦æ›´æ–°ï¼šæ‰€æœ‰è¶…å‚æ•°å·²å®Œå…¨æ¢å¤åˆ°ä¸åŸé¡¹ç›®ä¸€è‡´çš„è®¾ç½®**\n",
        "\n",
        "## ğŸ¯ æ ¸å¿ƒè¶…å‚æ•°è®¾ç½® (ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´)\n",
        "\n",
        "### ä¸»è¦å®éªŒå‚æ•°\n",
        "- **n_al_iterations**: 400 (åŸé¡¹ç›®å®Œæ•´è¿­ä»£æ•°)\n",
        "- **initial_data_size**: 800 (åˆå§‹æ•°æ®é›†å¤§å°)\n",
        "- **rosenbrock_dimension**: 20 (é—®é¢˜ç»´åº¦)\n",
        "- **domain_range**: (-2.048, 2.048) (æœç´¢åŸŸ)\n",
        "\n",
        "### CNNæ¨¡å‹å‚æ•°\n",
        "- **cnn_training_epochs**: 200 (åŸé¡¹ç›®å®Œæ•´è®­ç»ƒè½®æ•°)\n",
        "- **cnn_batch_size**: 32 (åŸé¡¹ç›®æ‰¹æ¬¡å¤§å°)\n",
        "- **cnn_validation_split**: 0.2 (éªŒè¯é›†æ¯”ä¾‹)\n",
        "- **cnn_early_stopping_patience**: 30 (åŸé¡¹ç›®æ—©åœpatience)\n",
        "\n",
        "### NTEæœç´¢å‚æ•°\n",
        "- **nte_c0**: 1.0 (æ¢ç´¢å‚æ•°)\n",
        "- **nte_rollout_rounds**: 100 (åŸé¡¹ç›®å®Œæ•´rolloutæ•°)\n",
        "- **validation_batch_size**: 20 (æ¯æ¬¡è¿­ä»£é€‰æ‹©æ ·æœ¬æ•°)\n",
        "\n",
        "### åŠ¨æ€æ¢ç´¢å› å­å…¬å¼ (é‡ç‚¹è°ƒè¯•)\n",
        "```python\n",
        "dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "```\n",
        "**æ³¨æ„ï¼šåŸé¡¹ç›®ä½¿ç”¨0.2ä½œä¸ºåŸºç¡€å› å­ï¼Œè¿™æ˜¯ç»è¿‡éªŒè¯çš„æœ€ä½³è®¾ç½®**\n",
        "- **base_exploration_factor**: 0.2 (åŸé¡¹ç›®åŸºç¡€å› å­)\n",
        "- **exploration_exponent_base**: 2.0 (æŒ‡æ•°åŸºæ•°)\n",
        "- **exploration_max_limit**: 20 (ä¸Šé™å€¼)\n",
        "\n",
        "### å¯è§†åŒ–æ§åˆ¶\n",
        "- **viz_interval**: 10 (æ¯10æ¬¡è¿­ä»£å¯è§†åŒ–)\n",
        "- **detailed_viz_interval**: 50 (æ¯50æ¬¡è¿­ä»£è¯¦ç»†å›¾è¡¨)\n",
        "\n",
        "## ğŸš€ P100æ˜¾å¡ä¸“é—¨ä¼˜åŒ–\n",
        "\n",
        "### TensorFlow P100ä¼˜åŒ–è®¾ç½®\n",
        "- è™šæ‹ŸGPUå†…å­˜é™åˆ¶: 15GB (P100çº¦16GBï¼Œç•™1GBä½™é‡)\n",
        "- çº¿ç¨‹é…ç½®: inter_op=4, intra_op=2 (P100æœ€ä¼˜)\n",
        "- æ··åˆç²¾åº¦: mixed_float16 (P100æ”¯æŒ)\n",
        "- XLAç¼–è¯‘ä¼˜åŒ–: å·²å¯ç”¨\n",
        "- GPUçº¿ç¨‹æ¨¡å¼: gpu_private\n",
        "- æ‰¹å½’ä¸€åŒ–ä¼˜åŒ–: å·²å¯ç”¨\n",
        "\n",
        "### æ€§èƒ½é¢„æœŸ\n",
        "- **å®Œæ•´400æ¬¡è¿­ä»£**: é¢„è®¡8-12å°æ—¶ (P100)\n",
        "- **å¿«é€Ÿ20æ¬¡æµ‹è¯•**: é¢„è®¡30-60åˆ†é’Ÿ\n",
        "- **å†…å­˜ä½¿ç”¨**: çº¦12-14GB GPUå†…å­˜\n",
        "\n",
        "## âš ï¸ é‡è¦æé†’\n",
        "1. æ‰€æœ‰å‚æ•°ç°åœ¨ä¸åŸé¡¹ç›®å®Œå…¨ä¸€è‡´\n",
        "2. ä¸“é—¨é’ˆå¯¹Kaggle P100æ˜¾å¡è¿›è¡Œäº†æ€§èƒ½ä¼˜åŒ–\n",
        "3. ä¿æŒäº†åŸé¡¹ç›®çš„åŠ¨æ€æ¢ç´¢å› å­è°ƒè¯•å…¬å¼\n",
        "4. å»ºè®®å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•éªŒè¯ç¯å¢ƒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: å‚æ•°è°ƒè¯•å®éªŒæ¡†æ¶ - åŠ¨æ€æ¢ç´¢å› å­ä¼˜åŒ–\n",
        "import itertools\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def run_parameter_sweep_experiment():\n",
        "    \"\"\"\n",
        "    è¿è¡Œå‚æ•°æ‰«æå®éªŒï¼Œè°ƒè¯•åŠ¨æ€æ¢ç´¢å› å­çš„æœ€ä¼˜ç»„åˆ\n",
        "    \n",
        "    é‡ç‚¹è°ƒè¯•çš„åŠ¨æ€å…¬å¼:\n",
        "    dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"å¼€å§‹åŠ¨æ€æ¢ç´¢å› å­å‚æ•°æ‰«æå®éªŒ\")\n",
        "    logger.info(\"=\"*80)\n",
        "    \n",
        "    # å‚æ•°ç½‘æ ¼å®šä¹‰\n",
        "    parameter_grid = {\n",
        "        'base_exploration_factor': [0.5, 0.8, 1.0, 1.2],  # åŸºç¡€å› å­\n",
        "        'exploration_exponent_base': [1.5, 2.0, 2.5],     # æŒ‡æ•°åŸºæ•°\n",
        "        'exploration_max_limit': [10, 15, 20, 25]         # ä¸Šé™å€¼\n",
        "    }\n",
        "    \n",
        "    # å¿«é€Ÿæµ‹è¯•å‚æ•°\n",
        "    quick_test_params = {\n",
        "        'n_al_iterations': 30,  # å¿«é€Ÿæµ‹è¯•ç”¨è¾ƒå°‘è¿­ä»£\n",
        "        'cnn_training_epochs': 50,\n",
        "        'initial_data_size': 400,  # å‡å°‘åˆå§‹æ•°æ®é‡\n",
        "        'viz_interval': 15,  # è¾ƒå°‘å¯è§†åŒ–é¢‘ç‡\n",
        "        'detailed_viz_interval': 30\n",
        "    }\n",
        "    \n",
        "    # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ\n",
        "    param_combinations = list(itertools.product(\n",
        "        parameter_grid['base_exploration_factor'],\n",
        "        parameter_grid['exploration_exponent_base'], \n",
        "        parameter_grid['exploration_max_limit']\n",
        "    ))\n",
        "    \n",
        "    logger.info(f\"æ€»å…±å°†æµ‹è¯• {len(param_combinations)} ç§å‚æ•°ç»„åˆ\")\n",
        "    \n",
        "    # å­˜å‚¨å®éªŒç»“æœ\n",
        "    sweep_results = []\n",
        "    best_result = None\n",
        "    best_score = float('inf')\n",
        "    \n",
        "    # åˆ›å»ºæ‰«æå®éªŒæ€»ç›®å½•\n",
        "    sweep_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    sweep_base_dir = f\"/kaggle/working/parameter_sweep_{sweep_timestamp}\"\n",
        "    os.makedirs(sweep_base_dir, exist_ok=True)\n",
        "    \n",
        "    for i, (base_factor, exp_base, max_limit) in enumerate(param_combinations):\n",
        "        experiment_name = f\"sweep_{i+1:02d}_base{base_factor}_exp{exp_base}_max{max_limit}\"\n",
        "        \n",
        "        logger.info(f\"\\nå¼€å§‹å®éªŒ {i+1}/{len(param_combinations)}: {experiment_name}\")\n",
        "        logger.info(f\"å‚æ•°: base_factor={base_factor}, exp_base={exp_base}, max_limit={max_limit}\")\n",
        "        \n",
        "        try:\n",
        "            # è¿è¡Œå•æ¬¡å®éªŒ\n",
        "            result = run_dante_optimization(\n",
        "                # åŠ¨æ€æ¢ç´¢å› å­å‚æ•° (é‡ç‚¹è°ƒè¯•)\n",
        "                base_exploration_factor=base_factor,\n",
        "                exploration_exponent_base=exp_base,\n",
        "                exploration_max_limit=max_limit,\n",
        "                \n",
        "                # å¿«é€Ÿæµ‹è¯•å‚æ•°\n",
        "                **quick_test_params,\n",
        "                \n",
        "                # å®éªŒæ ‡è¯†\n",
        "                experiment_name=experiment_name,\n",
        "                results_output_path=sweep_base_dir\n",
        "            )\n",
        "            \n",
        "            if result is not None:\n",
        "                # è®°å½•å…³é”®æŒ‡æ ‡\n",
        "                experiment_record = {\n",
        "                    'experiment_id': i + 1,\n",
        "                    'experiment_name': experiment_name,\n",
        "                    'base_exploration_factor': base_factor,\n",
        "                    'exploration_exponent_base': exp_base,\n",
        "                    'exploration_max_limit': max_limit,\n",
        "                    'final_best_value': float(result['final_best_value']),\n",
        "                    'final_pearson': float(result['final_pearson']),\n",
        "                    'max_dynamic_factor': float(result['experiment_summary']['max_dynamic_factor']),\n",
        "                    'total_time': float(result['experiment_summary']['total_time_seconds']),\n",
        "                    'total_samples': int(result['experiment_summary']['total_samples_evaluated']),\n",
        "                    'results_dir': result['results_dir']\n",
        "                }\n",
        "                \n",
        "                sweep_results.append(experiment_record)\n",
        "                \n",
        "                # æ›´æ–°æœ€ä½³ç»“æœ (ä»¥æœ€ç»ˆæœ€ä¼˜å€¼ä¸ºä¸»è¦æŒ‡æ ‡)\n",
        "                if result['final_best_value'] < best_score:\n",
        "                    best_score = result['final_best_value']\n",
        "                    best_result = experiment_record.copy()\n",
        "                \n",
        "                logger.info(f\"å®éªŒå®Œæˆ: æœ€ä¼˜å€¼={result['final_best_value']:.4e}, ç›¸å…³ç³»æ•°={result['final_pearson']:.4f}\")\n",
        "                \n",
        "            else:\n",
        "                logger.error(f\"å®éªŒ {experiment_name} å¤±è´¥\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"å®éªŒ {experiment_name} å‡ºç°å¼‚å¸¸: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # ä¿å­˜æ‰«æç»“æœ\n",
        "    sweep_results_file = os.path.join(sweep_base_dir, 'parameter_sweep_results.json')\n",
        "    with open(sweep_results_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'sweep_timestamp': sweep_timestamp,\n",
        "            'total_experiments': len(param_combinations),\n",
        "            'successful_experiments': len(sweep_results),\n",
        "            'parameter_grid': parameter_grid,\n",
        "            'quick_test_params': quick_test_params,\n",
        "            'best_result': best_result,\n",
        "            'all_results': sweep_results\n",
        "        }, f, indent=2)\n",
        "    \n",
        "    # åˆ›å»ºç»“æœåˆ†æ\n",
        "    if sweep_results:\n",
        "        results_df = pd.DataFrame(sweep_results)\n",
        "        \n",
        "        # ä¿å­˜CSVæ ¼å¼ç»“æœ\n",
        "        results_csv = os.path.join(sweep_base_dir, 'parameter_sweep_results.csv')\n",
        "        results_df.to_csv(results_csv, index=False)\n",
        "        \n",
        "        # ç”Ÿæˆåˆ†ææŠ¥å‘Š\n",
        "        analysis_report = generate_parameter_analysis_report(results_df, sweep_base_dir)\n",
        "        \n",
        "        logger.info(f\"\\n\" + \"=\"*80)\n",
        "        logger.info(\"å‚æ•°æ‰«æå®éªŒå®Œæˆï¼\")\n",
        "        logger.info(f\"æˆåŠŸå®Œæˆ {len(sweep_results)}/{len(param_combinations)} ä¸ªå®éªŒ\")\n",
        "        \n",
        "        if best_result:\n",
        "            logger.info(f\"\\næœ€ä½³å‚æ•°ç»„åˆ:\")\n",
        "            logger.info(f\"  åŸºç¡€å› å­: {best_result['base_exploration_factor']}\")\n",
        "            logger.info(f\"  æŒ‡æ•°åŸºæ•°: {best_result['exploration_exponent_base']}\")\n",
        "            logger.info(f\"  ä¸Šé™å€¼: {best_result['exploration_max_limit']}\")\n",
        "            logger.info(f\"  æœ€ä¼˜å€¼: {best_result['final_best_value']:.6e}\")\n",
        "            logger.info(f\"  ç›¸å…³ç³»æ•°: {best_result['final_pearson']:.4f}\")\n",
        "        \n",
        "        logger.info(f\"\\nç»“æœä¿å­˜ä½ç½®: {sweep_base_dir}\")\n",
        "        logger.info(f\"è¯¦ç»†ç»“æœ: {sweep_results_file}\")\n",
        "        logger.info(f\"CSVæ ¼å¼: {results_csv}\")\n",
        "        logger.info(\"=\"*80)\n",
        "        \n",
        "        return {\n",
        "            'sweep_dir': sweep_base_dir,\n",
        "            'best_result': best_result,\n",
        "            'all_results': sweep_results,\n",
        "            'results_df': results_df,\n",
        "            'analysis_report': analysis_report\n",
        "        }\n",
        "    else:\n",
        "        logger.error(\"å‚æ•°æ‰«æå®éªŒå¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸçš„å®éªŒ\")\n",
        "        return None\n",
        "\n",
        "def generate_parameter_analysis_report(results_df, output_dir):\n",
        "    \"\"\"ç”Ÿæˆå‚æ•°åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–\"\"\"\n",
        "    \n",
        "    logger.info(\"ç”Ÿæˆå‚æ•°åˆ†ææŠ¥å‘Š...\")\n",
        "    \n",
        "    try:\n",
        "        # åŸºæœ¬ç»Ÿè®¡åˆ†æ\n",
        "        report = {\n",
        "            'summary_stats': {\n",
        "                'best_value_mean': float(results_df['final_best_value'].mean()),\n",
        "                'best_value_std': float(results_df['final_best_value'].std()),\n",
        "                'best_value_min': float(results_df['final_best_value'].min()),\n",
        "                'best_value_max': float(results_df['final_best_value'].max()),\n",
        "                'pearson_mean': float(results_df['final_pearson'].mean()),\n",
        "                'pearson_std': float(results_df['final_pearson'].std())\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # æŒ‰å‚æ•°åˆ†ç»„åˆ†æ\n",
        "        base_factor_analysis = results_df.groupby('base_exploration_factor').agg({\n",
        "            'final_best_value': ['mean', 'std', 'min'],\n",
        "            'final_pearson': ['mean', 'std']\n",
        "        }).round(6)\n",
        "        \n",
        "        exp_base_analysis = results_df.groupby('exploration_exponent_base').agg({\n",
        "            'final_best_value': ['mean', 'std', 'min'],\n",
        "            'final_pearson': ['mean', 'std']\n",
        "        }).round(6)\n",
        "        \n",
        "        max_limit_analysis = results_df.groupby('exploration_max_limit').agg({\n",
        "            'final_best_value': ['mean', 'std', 'min'],\n",
        "            'final_pearson': ['mean', 'std']\n",
        "        }).round(6)\n",
        "        \n",
        "        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # 1. åŸºç¡€å› å­å½±å“\n",
        "        base_means = results_df.groupby('base_exploration_factor')['final_best_value'].mean()\n",
        "        axes[0,0].bar(base_means.index.astype(str), base_means.values)\n",
        "        axes[0,0].set_title('Base Exploration Factor vs Best Value')\n",
        "        axes[0,0].set_xlabel('Base Factor')\n",
        "        axes[0,0].set_ylabel('Mean Best Value')\n",
        "        axes[0,0].set_yscale('log')\n",
        "        \n",
        "        # 2. æŒ‡æ•°åŸºæ•°å½±å“\n",
        "        exp_means = results_df.groupby('exploration_exponent_base')['final_best_value'].mean()\n",
        "        axes[0,1].bar(exp_means.index.astype(str), exp_means.values)\n",
        "        axes[0,1].set_title('Exponent Base vs Best Value')\n",
        "        axes[0,1].set_xlabel('Exponent Base')\n",
        "        axes[0,1].set_ylabel('Mean Best Value')\n",
        "        axes[0,1].set_yscale('log')\n",
        "        \n",
        "        # 3. ä¸Šé™å€¼å½±å“\n",
        "        limit_means = results_df.groupby('exploration_max_limit')['final_best_value'].mean()\n",
        "        axes[0,2].bar(limit_means.index.astype(str), limit_means.values)\n",
        "        axes[0,2].set_title('Max Limit vs Best Value')\n",
        "        axes[0,2].set_xlabel('Max Limit')\n",
        "        axes[0,2].set_ylabel('Mean Best Value')\n",
        "        axes[0,2].set_yscale('log')\n",
        "        \n",
        "        # 4. çš®å°”é€Šç›¸å…³ç³»æ•°åˆ†æ\n",
        "        base_pearson = results_df.groupby('base_exploration_factor')['final_pearson'].mean()\n",
        "        axes[1,0].bar(base_pearson.index.astype(str), base_pearson.values)\n",
        "        axes[1,0].set_title('Base Factor vs Pearson Correlation')\n",
        "        axes[1,0].set_xlabel('Base Factor')\n",
        "        axes[1,0].set_ylabel('Mean Pearson Correlation')\n",
        "        \n",
        "        exp_pearson = results_df.groupby('exploration_exponent_base')['final_pearson'].mean()\n",
        "        axes[1,1].bar(exp_pearson.index.astype(str), exp_pearson.values)\n",
        "        axes[1,1].set_title('Exponent Base vs Pearson Correlation')\n",
        "        axes[1,1].set_xlabel('Exponent Base')\n",
        "        axes[1,1].set_ylabel('Mean Pearson Correlation')\n",
        "        \n",
        "        # 5. æ•£ç‚¹å›¾ï¼šæœ€ä¼˜å€¼ vs ç›¸å…³ç³»æ•°\n",
        "        scatter = axes[1,2].scatter(results_df['final_best_value'], results_df['final_pearson'], \n",
        "                                   c=results_df['base_exploration_factor'], cmap='viridis', alpha=0.7)\n",
        "        axes[1,2].set_xlabel('Final Best Value')\n",
        "        axes[1,2].set_ylabel('Final Pearson Correlation')\n",
        "        axes[1,2].set_title('Best Value vs Pearson Correlation')\n",
        "        axes[1,2].set_xscale('log')\n",
        "        plt.colorbar(scatter, ax=axes[1,2], label='Base Factor')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # ä¿å­˜å›¾è¡¨\n",
        "        analysis_plot_path = os.path.join(output_dir, 'parameter_analysis_report.png')\n",
        "        plt.savefig(analysis_plot_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š\n",
        "        report_text = f\"\"\"\n",
        "å‚æ•°æ‰«æå®éªŒåˆ†ææŠ¥å‘Š\n",
        "===================\n",
        "\n",
        "å®éªŒæ¦‚è¦:\n",
        "- æ€»å®éªŒæ•°: {len(results_df)}\n",
        "- æœ€ä¼˜å€¼èŒƒå›´: {report['summary_stats']['best_value_min']:.4e} - {report['summary_stats']['best_value_max']:.4e}\n",
        "- å¹³å‡æœ€ä¼˜å€¼: {report['summary_stats']['best_value_mean']:.4e} Â± {report['summary_stats']['best_value_std']:.4e}\n",
        "- å¹³å‡ç›¸å…³ç³»æ•°: {report['summary_stats']['pearson_mean']:.4f} Â± {report['summary_stats']['pearson_std']:.4f}\n",
        "\n",
        "æŒ‰åŸºç¡€æ¢ç´¢å› å­åˆ†æ:\n",
        "{base_factor_analysis}\n",
        "\n",
        "æŒ‰æŒ‡æ•°åŸºæ•°åˆ†æ:\n",
        "{exp_base_analysis}\n",
        "\n",
        "æŒ‰ä¸Šé™å€¼åˆ†æ:\n",
        "{max_limit_analysis}\n",
        "\n",
        "æœ€ä½³å®éªŒ:\n",
        "{results_df.loc[results_df['final_best_value'].idxmin()].to_dict()}\n",
        "        \"\"\"\n",
        "        \n",
        "        report_text_path = os.path.join(output_dir, 'parameter_analysis_report.txt')\n",
        "        with open(report_text_path, 'w') as f:\n",
        "            f.write(report_text)\n",
        "        \n",
        "        report['analysis_plot'] = analysis_plot_path\n",
        "        report['report_text'] = report_text_path\n",
        "        \n",
        "        logger.info(f\"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_text_path}\")\n",
        "        logger.info(f\"åˆ†æå›¾è¡¨å·²ä¿å­˜: {analysis_plot_path}\")\n",
        "        \n",
        "        return report\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ç”Ÿæˆåˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {e}\")\n",
        "        return None\n",
        "\n",
        "# å¿«é€Ÿå•æ¬¡å®éªŒå‡½æ•°\n",
        "def run_quick_experiment(\n",
        "    base_exploration_factor=0.8,\n",
        "    exploration_exponent_base=2.0,\n",
        "    exploration_max_limit=20,\n",
        "    n_iterations=20,\n",
        "    experiment_name=\"quick_test\"\n",
        "):\n",
        "    \"\"\"è¿è¡Œå¿«é€Ÿå•æ¬¡å®éªŒï¼Œç”¨äºéªŒè¯ç‰¹å®šå‚æ•°ç»„åˆ\"\"\"\n",
        "    \n",
        "    logger.info(f\"è¿è¡Œå¿«é€Ÿå®éªŒ: {experiment_name}\")\n",
        "    logger.info(f\"åŠ¨æ€æ¢ç´¢å› å­å‚æ•°: base={base_exploration_factor}, exp_base={exploration_exponent_base}, max_limit={exploration_max_limit}\")\n",
        "    \n",
        "    result = run_dante_optimization(\n",
        "        # åŠ¨æ€æ¢ç´¢å› å­å‚æ•°\n",
        "        base_exploration_factor=base_exploration_factor,\n",
        "        exploration_exponent_base=exploration_exponent_base,\n",
        "        exploration_max_limit=exploration_max_limit,\n",
        "        \n",
        "        # å¿«é€Ÿæµ‹è¯•å‚æ•°\n",
        "        n_al_iterations=n_iterations,\n",
        "        cnn_training_epochs=30,\n",
        "        initial_data_size=200,\n",
        "        viz_interval=10,\n",
        "        detailed_viz_interval=20,\n",
        "        \n",
        "        # å®éªŒæ ‡è¯†\n",
        "        experiment_name=experiment_name\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "logger.info(\"å‚æ•°è°ƒè¯•å®éªŒæ¡†æ¶åŠ è½½å®Œæˆ\")\n",
        "print(\"ğŸ¯ å‚æ•°è°ƒè¯•å®éªŒæ¡†æ¶å·²å‡†å¤‡å°±ç»ª\")\n",
        "print(\"ğŸ“Š å¯ä½¿ç”¨ run_parameter_sweep_experiment() è¿›è¡Œå‚æ•°ç½‘æ ¼æœç´¢\")\n",
        "print(\"âš¡ å¯ä½¿ç”¨ run_quick_experiment() è¿›è¡Œå¿«é€Ÿå•æ¬¡æµ‹è¯•\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ¯ æœ€ç»ˆç¡®è®¤ï¼šæ‰€æœ‰è¶…å‚æ•°å·²æ¢å¤åˆ°åŸé¡¹ç›®è®¾ç½®\n",
        "\n",
        "## âœ… å®Œæˆçš„ä¿®æ­£é¡¹ç›®\n",
        "\n",
        "### 1. æ ¸å¿ƒè¶…å‚æ•°æ¢å¤\n",
        "- **n_al_iterations**: 50 â†’ **400** (åŸé¡¹ç›®å®Œæ•´è¿­ä»£æ•°)\n",
        "- **cnn_training_epochs**: 100 â†’ **200** (åŸé¡¹ç›®å®Œæ•´è®­ç»ƒè½®æ•°)\n",
        "- **cnn_batch_size**: 64 â†’ **32** (åŸé¡¹ç›®æ‰¹æ¬¡å¤§å°)\n",
        "- **cnn_early_stopping_patience**: 20 â†’ **30** (åŸé¡¹ç›®æ—©åœpatience)\n",
        "- **detailed_viz_interval**: 25 â†’ **50** (åŸé¡¹ç›®è®¾ç½®)\n",
        "\n",
        "### 2. åŠ¨æ€æ¢ç´¢å› å­å…¬å¼æ¢å¤\n",
        "```python\n",
        "# åŸé¡¹ç›®éªŒè¯çš„æœ€ä½³å…¬å¼\n",
        "dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "```\n",
        "- **base_exploration_factor**: 0.8 â†’ **0.2** (åŸé¡¹ç›®éªŒè¯çš„æœ€ä½³åŸºç¡€å› å­)\n",
        "\n",
        "### 3. P100æ˜¾å¡ä¸“é—¨ä¼˜åŒ–\n",
        "- è™šæ‹ŸGPUå†…å­˜é™åˆ¶: 15GB (é¿å…OOM)\n",
        "- çº¿ç¨‹é…ç½®: inter_op=4, intra_op=2 (P100æœ€ä¼˜)\n",
        "- æ··åˆç²¾åº¦: mixed_float16 (P100æ”¯æŒ)\n",
        "- XLAç¼–è¯‘ä¼˜åŒ–: å·²å¯ç”¨\n",
        "- GPUçº¿ç¨‹æ¨¡å¼: gpu_private\n",
        "- æ‰¹å½’ä¸€åŒ–ä¼˜åŒ–: å·²å¯ç”¨\n",
        "\n",
        "### 4. æ€§èƒ½é¢„æœŸ (P100)\n",
        "- **å®Œæ•´400æ¬¡è¿­ä»£**: 8-12å°æ—¶\n",
        "- **å¿«é€Ÿ20æ¬¡æµ‹è¯•**: 30-60åˆ†é’Ÿ\n",
        "- **GPUå†…å­˜ä½¿ç”¨**: çº¦12-14GB\n",
        "\n",
        "## ğŸš€ ç°åœ¨å¯ä»¥å¼€å§‹å®éªŒ\n",
        "\n",
        "æ‰€æœ‰å‚æ•°ç°åœ¨ä¸æ‚¨çš„åŸé¡¹ç›®å®Œå…¨ä¸€è‡´ï¼Œä¸“é—¨é’ˆå¯¹Kaggle P100æ˜¾å¡è¿›è¡Œäº†æ€§èƒ½ä¼˜åŒ–ã€‚æ‚¨å¯ä»¥æ”¾å¿ƒè¿è¡Œå®Œæ•´çš„400æ¬¡è¿­ä»£å®éªŒï¼Œæ¨¡å‹æ€§èƒ½å°†å¾—åˆ°æœ€å¤§åŒ–ã€‚\n",
        "\n",
        "**å»ºè®®æ‰§è¡Œé¡ºåºï¼š**\n",
        "1. å…ˆè¿è¡Œ `run_quick_experiment()` éªŒè¯ç¯å¢ƒ (20æ¬¡è¿­ä»£)\n",
        "2. è¿è¡Œå®Œæ•´å®éªŒ `run_dante_optimization()` (400æ¬¡è¿­ä»£)\n",
        "3. å¦‚éœ€è°ƒè¯•ï¼Œä½¿ç”¨å‚æ•°æ‰«æ `run_parameter_sweep_experiment()`\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸš€ å¦‚ä½•åœ¨Kaggleä¸Šæ­£ç¡®è¿è¡Œæ­¤Notebook\n",
        "\n",
        "## âœ… è¿è¡Œå‰å‡†å¤‡\n",
        "\n",
        "### 1. ç¯å¢ƒæ£€æŸ¥\n",
        "ç¡®ä¿æ‚¨çš„Kaggleè®¾ç½®ä¸ºï¼š\n",
        "- **åŠ é€Ÿå™¨**: GPU P100 (æ¨è) æˆ– T4\n",
        "- **Internet**: å¼€å¯ (éœ€è¦ä¸‹è½½ä¾èµ–åŒ…)\n",
        "- **æŒä¹…å­˜å‚¨**: å¼€å¯ (å¯é€‰ï¼Œç”¨äºä¿å­˜é•¿æœŸç»“æœ)\n",
        "\n",
        "### 2. æ•°æ®é›†è®¾ç½®\n",
        "ç¡®ä¿æ‚¨å·²ç»å°†Rosenbrockæ•°æ®é›†æ·»åŠ åˆ°æ­¤notebookï¼š\n",
        "- **æ•°æ®é›†åç§°**: `rosenbrock-data-20d-800`\n",
        "- **æ•°æ®è·¯å¾„**: `/kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw/`\n",
        "- **åŒ…å«æ–‡ä»¶**: `Rosenbrock_x_train.npy`, `Rosenbrock_y_train.npy`, `Rosenbrock_x_test.npy`, `Rosenbrock_y_test.npy`\n",
        "\n",
        "## ğŸƒâ€â™‚ï¸ æ¨èè¿è¡Œé¡ºåº\n",
        "\n",
        "### æ­¥éª¤1: ç¯å¢ƒéªŒè¯ (å¿…é¡»)\n",
        "æŒ‰é¡ºåºè¿è¡Œå‰é¢çš„æ‰€æœ‰cell (Cell 0-23)ï¼Œç¡®ä¿æ²¡æœ‰é”™è¯¯\n",
        "\n",
        "### æ­¥éª¤2: å¿«é€Ÿæµ‹è¯• (å¼ºçƒˆæ¨è)\n",
        "```python\n",
        "# è¿è¡Œ20æ¬¡è¿­ä»£çš„å¿«é€Ÿæµ‹è¯•\n",
        "result = run_quick_experiment(\n",
        "    experiment_name=\"kaggle_quick_test\",\n",
        "    n_iterations=20\n",
        ")\n",
        "```\n",
        "\n",
        "### æ­¥éª¤3: å®Œæ•´å®éªŒ (400æ¬¡è¿­ä»£)\n",
        "```python\n",
        "# è¿è¡Œå®Œæ•´çš„400æ¬¡è¿­ä»£å®éªŒ\n",
        "result = run_dante_optimization(\n",
        "    experiment_name=\"kaggle_full_experiment_400\",\n",
        "    n_al_iterations=400\n",
        ")\n",
        "```\n",
        "\n",
        "### æ­¥éª¤4: å‚æ•°è°ƒè¯• (å¯é€‰)\n",
        "```python\n",
        "# è¿è¡Œå‚æ•°ç½‘æ ¼æœç´¢\n",
        "sweep_result = run_parameter_sweep_experiment()\n",
        "```\n",
        "\n",
        "## â±ï¸ æ—¶é—´é¢„æœŸ\n",
        "\n",
        "| å®éªŒç±»å‹ | è¿­ä»£æ¬¡æ•° | é¢„è®¡æ—¶é—´ (P100) | é¢„è®¡æ—¶é—´ (T4) |\n",
        "|----------|----------|-----------------|---------------|\n",
        "| å¿«é€Ÿæµ‹è¯• | 20 | 30-60åˆ†é’Ÿ | 45-90åˆ†é’Ÿ |\n",
        "| å®Œæ•´å®éªŒ | 400 | 8-12å°æ—¶ | 12-18å°æ—¶ |\n",
        "| å‚æ•°æ‰«æ | å¤šç»„åˆ | 2-4å°æ—¶ | 3-6å°æ—¶ |\n",
        "\n",
        "## ğŸ“Š ç»“æœæ–‡ä»¶\n",
        "\n",
        "è¿è¡Œå®Œæˆåï¼Œåœ¨ `/kaggle/working/results/` ç›®å½•ä¸‹ä¼šç”Ÿæˆï¼š\n",
        "- `experiment_parameters.json` - å®éªŒå‚æ•°\n",
        "- `final_experiment_summary.json` - å®éªŒæ€»ç»“\n",
        "- å„ç§å¯è§†åŒ–å›¾è¡¨ (PNGæ ¼å¼)\n",
        "- CSVæ•°æ®æ–‡ä»¶\n",
        "\n",
        "## âš ï¸ é‡è¦æé†’\n",
        "\n",
        "1. **å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•**: ç¡®ä¿ç¯å¢ƒæ­£å¸¸åå†è¿è¡Œå®Œæ•´å®éªŒ\n",
        "2. **å®šæœŸæ£€æŸ¥**: é•¿æ—¶é—´å®éªŒæ—¶å®šæœŸæŸ¥çœ‹æ—¥å¿—è¾“å‡º\n",
        "3. **å†…å­˜ç›‘æ§**: å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³ï¼Œå¯ä»¥é™ä½batch_size\n",
        "4. **ä¿å­˜ç»“æœ**: é‡è¦ç»“æœåŠæ—¶ä¸‹è½½åˆ°æœ¬åœ°\n",
        "5. **ç½‘ç»œç¨³å®š**: é•¿æ—¶é—´å®éªŒç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š\n",
        "\n",
        "## ğŸ”§ æ•…éšœæ’é™¤\n",
        "\n",
        "### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ\n",
        "\n",
        "1. **GPUå†…å­˜ä¸è¶³**: é™ä½ `cnn_batch_size` ä»32åˆ°16\n",
        "2. **æ•°æ®åŠ è½½å¤±è´¥**: æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®æ·»åŠ åˆ°notebook\n",
        "3. **ä¾èµ–åŒ…å®‰è£…å¤±è´¥**: ç¡®ä¿Internetå¼€å¯\n",
        "4. **è¿è¡Œæ—¶é—´è¶…é™**: ä½¿ç”¨Kaggle Proè´¦æˆ·è·å¾—æ›´é•¿è¿è¡Œæ—¶é—´\n",
        "\n",
        "å‡†å¤‡å°±ç»ªï¼Ÿå¼€å§‹æ‚¨çš„DANTEä¼˜åŒ–å®éªŒå§ï¼\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: æµ‹è¯•å’Œä½¿ç”¨æŒ‡å—\n",
        "print(\"ğŸš€ DANTE Rosenbrock ä¼˜åŒ–ç³»ç»Ÿ - Kaggleç‰ˆæœ¬\")\n",
        "print(\"=\"*60)\n",
        "print(\"æ‰€æœ‰æ¨¡å—å·²æˆåŠŸåŠ è½½å®Œæˆï¼\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“‹ å¯ç”¨çš„ä¸»è¦åŠŸèƒ½:\")\n",
        "print(\"1. run_quick_experiment() - å¿«é€Ÿå•æ¬¡å®éªŒ (æ¨èå…ˆè¿è¡Œ)\")\n",
        "print(\"2. run_dante_optimization() - å®Œæ•´DANTEä¼˜åŒ–ç®—æ³•\")\n",
        "print(\"3. run_parameter_sweep_experiment() - å‚æ•°ç½‘æ ¼æœç´¢\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ¯ é‡ç‚¹è°ƒè¯•å‚æ•° (åŠ¨æ€æ¢ç´¢å› å­å…¬å¼):\")\n",
        "print(\"dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\")\n",
        "print()\n",
        "print(\"  â€¢ base_exploration_factor: åŸºç¡€å› å­ (é»˜è®¤0.8)\")\n",
        "print(\"  â€¢ exploration_exponent_base: æŒ‡æ•°åŸºæ•° (é»˜è®¤2.0)\")  \n",
        "print(\"  â€¢ exploration_max_limit: ä¸Šé™å€¼ (é»˜è®¤20)\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ’» å¿«é€Ÿå¼€å§‹ç¤ºä¾‹:\")\n",
        "print()\n",
        "print(\"# 1. è¿è¡Œå¿«é€Ÿæµ‹è¯• (20æ¬¡è¿­ä»£)\")\n",
        "print(\"result = run_quick_experiment()\")\n",
        "print()\n",
        "print(\"# 2. è°ƒè¯•ç‰¹å®šå‚æ•°ç»„åˆ\")\n",
        "print(\"result = run_quick_experiment(\")\n",
        "print(\"    base_exploration_factor=1.0,\")\n",
        "print(\"    exploration_exponent_base=2.5,\")\n",
        "print(\"    exploration_max_limit=15,\")\n",
        "print(\"    n_iterations=30\")\n",
        "print(\")\")\n",
        "print()\n",
        "print(\"# 3. è¿è¡Œå®Œæ•´å®éªŒ (50æ¬¡è¿­ä»£)\")\n",
        "print(\"result = run_dante_optimization(\")\n",
        "print(\"    base_exploration_factor=0.8,\")\n",
        "print(\"    exploration_exponent_base=2.0,\")\n",
        "print(\"    exploration_max_limit=20,\")\n",
        "print(\"    n_al_iterations=50\")\n",
        "print(\")\")\n",
        "print()\n",
        "print(\"# 4. å‚æ•°ç½‘æ ¼æœç´¢ (48ç§ç»„åˆ)\")\n",
        "print(\"sweep_result = run_parameter_sweep_experiment()\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“Š è¾“å‡ºç»“æœè¯´æ˜:\")\n",
        "print(\"  â€¢ æœ€ä¼˜å€¼è¶‹åŠ¿å›¾: ä¼˜åŒ–è¿›å±•å¯è§†åŒ–\")\n",
        "print(\"  â€¢ çš®å°”é€Šç›¸å…³ç³»æ•°: æ¨¡å‹è´¨é‡è¯„ä¼°\")\n",
        "print(\"  â€¢ DUCBè¶‹åŠ¿å›¾: NTEæœç´¢åˆ†æ\")\n",
        "print(\"  â€¢ ç»´åº¦åˆ†å¸ƒå›¾: æœ€ä¼˜è§£ç‰¹å¾åˆ†æ\")\n",
        "print(\"  â€¢ å®éªŒå‚æ•°å’Œæ€»ç»“: JSONæ ¼å¼ä¿å­˜\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ”§ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥:\")\n",
        "print(f\"  â€¢ GPUå¯ç”¨: {'âœ…' if gpu_available else 'âŒ'}\")\n",
        "print(f\"  â€¢ TensorFlowç‰ˆæœ¬: {tf.__version__}\")\n",
        "print(f\"  â€¢ æ•°æ®è·¯å¾„: /kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw\")\n",
        "print(f\"  â€¢ ç»“æœè·¯å¾„: /kaggle/working/results\")\n",
        "print()\n",
        "\n",
        "print(\"âš ï¸  é‡è¦æé†’:\")\n",
        "print(\"  â€¢ ç¡®ä¿å·²ä¸Šä¼ Rosenbrockæ•°æ®é›†åˆ°Kaggle\")\n",
        "print(\"  â€¢ P100 GPUæ¨èï¼Œ16GBå†…å­˜\")\n",
        "print(\"  â€¢ å®Œæ•´400æ¬¡è¿­ä»£éœ€è¦8-12å°æ—¶\")\n",
        "print(\"  â€¢ å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•éªŒè¯ç¯å¢ƒ\")\n",
        "print()\n",
        "\n",
        "print(\"ğŸ“ å®éªŒå»ºè®®:\")\n",
        "print(\"  1. å…ˆè¿è¡Œ run_quick_experiment() éªŒè¯ç¯å¢ƒ\")\n",
        "print(\"  2. å°è¯•ä¸åŒçš„åŠ¨æ€æ¢ç´¢å› å­å‚æ•°\")\n",
        "print(\"  3. è§‚å¯ŸDUCBè¶‹åŠ¿å›¾åˆ†ææœç´¢è¡Œä¸º\")\n",
        "print(\"  4. ä½¿ç”¨å‚æ•°æ‰«ææ‰¾åˆ°æœ€ä¼˜ç»„åˆ\")\n",
        "print(\"  5. æœ€åè¿è¡Œå®Œæ•´400æ¬¡è¿­ä»£å®éªŒ\")\n",
        "print()\n",
        "\n",
        "print(\"å‡†å¤‡å¼€å§‹ä¼˜åŒ–å®éªŒï¼ğŸš€\")\n",
        "\n",
        "# è¿è¡Œç¯å¢ƒéªŒè¯\n",
        "try:\n",
        "    # éªŒè¯æ‰€æœ‰æ ¸å¿ƒç»„ä»¶\n",
        "    test_rosenbrock_val = rosenbrock_function(np.ones(20))\n",
        "    test_log_transform = safe_power10(np.array([1.0, 2.0]))\n",
        "    print(f\"âœ… ç¯å¢ƒéªŒè¯é€šè¿‡ - Rosenbrock(1^20) = {test_rosenbrock_val:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ç¯å¢ƒéªŒè¯å¤±è´¥: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
