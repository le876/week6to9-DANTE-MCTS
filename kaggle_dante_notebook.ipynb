{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DANTE Rosenbrock 优化任务 - Kaggle版本\n",
        "\n",
        "## 项目概述\n",
        "基于CNN代理模型的DANTE优化算法，用于解决20维Rosenbrock函数优化问题。\n",
        "主要目标是调试和优化NTE搜索器中的动态探索因子参数。\n",
        "\n",
        "## 当前配置 (已恢复到原项目设置)\n",
        "- **维度**: 20D Rosenbrock函数\n",
        "- **初始样本**: 800个\n",
        "- **迭代次数**: 400次主动学习 (原项目完整设置)\n",
        "- **每次迭代样本**: 20个新样本\n",
        "- **CNN训练轮数**: 200 epochs (原项目设置)\n",
        "- **重点调试**: `dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)` (原项目验证的最佳公式)\n",
        "\n",
        "## P100显卡优化\n",
        "- 专门针对Kaggle P100显卡进行了性能优化\n",
        "- 虚拟GPU内存限制: 15GB\n",
        "- 混合精度训练: mixed_float16\n",
        "- XLA编译优化: 已启用\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: 环境设置与GPU配置\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import torch\n",
        "import random\n",
        "from scipy.stats import pearsonr\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "import psutil\n",
        "\n",
        "# GPU配置优化\n",
        "print(\"=== GPU环境配置 ===\")\n",
        "\n",
        "# TensorFlow GPU设置\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # 启用内存增长\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"TensorFlow检测到{len(gpus)}个GPU设备\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"  GPU {i}: {gpu}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"GPU设置错误: {e}\")\n",
        "else:\n",
        "    print(\"未检测到GPU设备，将使用CPU\")\n",
        "\n",
        "# PyTorch GPU设置\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(f\"PyTorch将使用设备: {device}\")\n",
        "    print(f\"CUDA版本: {torch.version.cuda}\")\n",
        "    print(f\"GPU设备名称: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(f\"PyTorch将使用CPU\")\n",
        "\n",
        "# 可视化设置\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 150\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'  # 使用默认字体避免中文问题\n",
        "\n",
        "# 配置日志记录器\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"环境配置完成\")\n",
        "\n",
        "# Kaggle环境路径设置 - 更新为实际数据集路径\n",
        "DATA_PATH = \"/kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw\"\n",
        "RESULTS_PATH = \"/kaggle/working/results\"\n",
        "\n",
        "# 创建结果目录\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "logger.info(f\"数据路径: {DATA_PATH}\")\n",
        "logger.info(f\"结果路径: {RESULTS_PATH}\")\n",
        "\n",
        "# 验证数据文件是否存在\n",
        "data_files = [\n",
        "    \"Rosenbrock_x_train.npy\",\n",
        "    \"Rosenbrock_y_train.npy\", \n",
        "    \"Rosenbrock_x_test.npy\",\n",
        "    \"Rosenbrock_y_test.npy\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== 数据文件检查 ===\")\n",
        "for file in data_files:\n",
        "    file_path = os.path.join(DATA_PATH, file)\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"✅ {file} - 找到\")\n",
        "        if file.endswith('.npy'):\n",
        "            data_shape = np.load(file_path).shape\n",
        "            print(f\"   数据形状: {data_shape}\")\n",
        "    else:\n",
        "        print(f\"❌ {file} - 未找到\")\n",
        "        logger.warning(f\"数据文件未找到: {file_path}\")\n",
        "\n",
        "print(\"\\n=== 环境配置完成 ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: 核心工具函数 (utils.py)\n",
        "import numpy as np\n",
        "\n",
        "def rosenbrock_function(x):\n",
        "    \"\"\"\n",
        "    Calculates the Rosenbrock function value for a given input vector x.\n",
        "    The function is typically evaluated on the hypercube xi ∈ [-5, 10] for all i, \n",
        "    although it may be evaluated for xi ∈ [-2.048, 2.048] as well.\n",
        "    For our DANTE optimization, we usually normalize inputs to [0,1] or work within specific bounds.\n",
        "    Args:\n",
        "        x (np.ndarray): Input vector of shape (n,) or (batch_size, n).\n",
        "    Returns:\n",
        "        float or np.ndarray: Rosenbrock function value(s).\n",
        "    \"\"\"\n",
        "    if not isinstance(x, np.ndarray):\n",
        "        x = np.array(x)\n",
        "\n",
        "    if x.ndim == 1:\n",
        "        # Single input vector\n",
        "        n = len(x)\n",
        "        if n < 2:\n",
        "            raise ValueError(\"Rosenbrock function requires at least 2 dimensions.\")\n",
        "        sum_val = 0\n",
        "        for i in range(n - 1):\n",
        "            sum_val += 100 * (x[i+1] - x[i]**2)**2 + (x[i] - 1)**2\n",
        "        return sum_val\n",
        "    elif x.ndim == 2:\n",
        "        # Batch of input vectors\n",
        "        n = x.shape[1]\n",
        "        if n < 2:\n",
        "            raise ValueError(\"Rosenbrock function requires at least 2 dimensions for each vector in the batch.\")\n",
        "        results = [] \n",
        "        for i in range(x.shape[0]):\n",
        "            sum_val = 0\n",
        "            for j in range(n - 1):\n",
        "                sum_val += 100 * (x[i, j+1] - x[i, j]**2)**2 + (x[i, j] - 1)**2\n",
        "            results.append(sum_val)\n",
        "        return np.array(results)\n",
        "    else:\n",
        "        raise ValueError(\"Input x must be a 1D or 2D NumPy array.\")\n",
        "\n",
        "\n",
        "def get_rosenbrock_bounds(dimension):\n",
        "    \"\"\"\n",
        "    Returns the typical bounds for the Rosenbrock function for a given dimension.\n",
        "    Often [-5, 10] or [-2.048, 2.048]. Let's use [-5, 10] for now.\n",
        "    \"\"\"\n",
        "    return [(-5.0, 10.0)] * dimension\n",
        "\n",
        "# 外部评估器\n",
        "def rosenbrock_evaluator(x_vector):\n",
        "    \"\"\"计算给定向量x的Rosenbrock函数值。\"\"\"\n",
        "    return rosenbrock_function(x_vector)\n",
        "\n",
        "# 定义LOG_EPSILON常量\n",
        "LOG_EPSILON = 1.0  # 使用10为底的对数变换: log10(y + 1)\n",
        "\n",
        "# NTE相关常量\n",
        "def safe_power10(log_values):\n",
        "    \"\"\"安全地计算10的幂，避免溢出\"\"\"\n",
        "    # 限制输入范围以避免溢出\n",
        "    log_values_clipped = np.clip(log_values, -10, 10)\n",
        "    return 10.0 ** log_values_clipped - LOG_EPSILON\n",
        "\n",
        "NTE_C0 = 1\n",
        "NTE_ROLLOUT_ROUNDS = 100\n",
        "\n",
        "logger.info(\"核心工具函数加载完成\")\n",
        "print(\"测试Rosenbrock函数...\")\n",
        "test_x = np.array([1, 1, 1])\n",
        "test_result = rosenbrock_function(test_x)\n",
        "print(f\"Rosenbrock([1,1,1]) = {test_result} (期望值: 0)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: 数据管理模块 (data_manager.py)\n",
        "import numpy as np\n",
        "import torch\n",
        "import logging\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "class DataManager:\n",
        "    def __init__(self, initial_X=None, initial_y=None, dimension=None, data_file_path=None):\n",
        "        \"\"\"\n",
        "        Initializes the DataManager.\n",
        "        Args:\n",
        "            initial_X (np.ndarray, optional): Initial X data. Defaults to None.\n",
        "            initial_y (np.ndarray, optional): Initial y data (original scale). Defaults to None.\n",
        "            dimension (int, optional): Dimension of the X data.\n",
        "            data_file_path (str, optional): Path to an .npz file to load data from.\n",
        "        \"\"\"\n",
        "        self.X_data = np.array([])\n",
        "        self.y_data_orig = np.array([])  # Stores original y values\n",
        "        self.y_data_log = np.array([])   # Stores log-transformed y values\n",
        "        self.dimension = dimension\n",
        "        self.mean_X = None\n",
        "        self.std_X = None\n",
        "        \n",
        "        # 添加域范围属性，用于NTE搜索\n",
        "        self.x_min = -2.048  # Rosenbrock函数的默认下限\n",
        "        self.x_max = 2.048   # Rosenbrock函数的默认上限\n",
        "        \n",
        "        # 添加样本权重属性\n",
        "        self.last_batch_weights = None\n",
        "        \n",
        "        # 添加数据缓存\n",
        "        self._normalized_cache = {}\n",
        "        self._cache_hash = None\n",
        "\n",
        "        if data_file_path and os.path.exists(data_file_path):\n",
        "            logger.info(f\"Loading data from {data_file_path}\")\n",
        "            try:\n",
        "                data = np.load(data_file_path)\n",
        "                self.X_data = data.get('X_data', np.array([]))\n",
        "                loaded_y_orig = data.get('y_data', np.array([])) \n",
        "                \n",
        "                if self.X_data.ndim == 1 and self.X_data.shape[0] > 0:\n",
        "                    self.X_data = self.X_data.reshape(1, -1)\n",
        "                if loaded_y_orig.ndim == 1 and loaded_y_orig.shape[0] > 0:\n",
        "                     loaded_y_orig = loaded_y_orig.reshape(-1,1)\n",
        "\n",
        "                self.y_data_orig = loaded_y_orig\n",
        "                if self.y_data_orig.shape[0] > 0:\n",
        "                    self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "                else:\n",
        "                    self.y_data_log = np.empty((0,1))\n",
        "\n",
        "                self.mean_X = data.get('mean_X', None)\n",
        "                self.std_X = data.get('std_X', None)\n",
        "                \n",
        "                if self.X_data.shape[0] > 0:\n",
        "                    self.dimension = self.X_data.shape[1]\n",
        "                    logger.info(f\"Loaded {self.X_data.shape[0]} samples.\")\n",
        "                    if self.mean_X is not None and self.std_X is not None:\n",
        "                        logger.info(\"Loaded existing normalization parameters for X.\")\n",
        "                    else:\n",
        "                        logger.info(\"Normalization parameters not found, will compute them.\")\n",
        "                        self.normalize_X()\n",
        "                else:\n",
        "                    logger.warning(f\"No data found in {data_file_path} or data is empty.\")\n",
        "                    if initial_X is not None and initial_y is not None:\n",
        "                        logger.info(\"Falling back to provided initial_X and initial_y.\")\n",
        "                        self.X_data = initial_X.copy()\n",
        "                        self.y_data_orig = initial_y.copy()\n",
        "                        if self.y_data_orig.ndim == 1: \n",
        "                            self.y_data_orig = self.y_data_orig.reshape(-1, 1)\n",
        "                        self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "                        if self.dimension is None and self.X_data.shape[0] > 0:\n",
        "                            self.dimension = self.X_data.shape[1]\n",
        "                        self.normalize_X()\n",
        "                    elif self.dimension is None:\n",
        "                        raise ValueError(\"Dimension must be provided if not loading from file and no initial data.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error loading data from {data_file_path}: {e}\")\n",
        "                if initial_X is not None and initial_y is not None:\n",
        "                    logger.info(\"Falling back to provided initial_X and initial_y due to loading error.\")\n",
        "                    self.X_data = initial_X.copy()\n",
        "                    self.y_data_orig = initial_y.copy()\n",
        "                    if self.y_data_orig.ndim == 1: \n",
        "                        self.y_data_orig = self.y_data_orig.reshape(-1, 1)\n",
        "                    self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "                    if self.dimension is None and self.X_data.shape[0] > 0:\n",
        "                        self.dimension = self.X_data.shape[1]\n",
        "                    self.normalize_X()\n",
        "                elif self.dimension is None:\n",
        "                    raise ValueError(\"Dimension must be provided if loading fails and no initial data.\")\n",
        "        \n",
        "        elif initial_X is not None and initial_y is not None:\n",
        "            logger.info(\"Initializing with provided initial_X and initial_y.\")\n",
        "            self.X_data = initial_X.copy()\n",
        "            self.y_data_orig = initial_y.copy()\n",
        "            if self.X_data.ndim == 1: \n",
        "                self.X_data = self.X_data.reshape(1, -1)\n",
        "            if self.y_data_orig.ndim == 1: \n",
        "                self.y_data_orig = self.y_data_orig.reshape(-1, 1)\n",
        "            self.y_data_log = np.log10(self.y_data_orig + LOG_EPSILON)\n",
        "            if self.dimension is None and self.X_data.shape[0] > 0:\n",
        "                 self.dimension = self.X_data.shape[1]\n",
        "            self.normalize_X()\n",
        "        elif self.dimension is None:\n",
        "             raise ValueError(\"Dimension must be provided if no data source is specified.\")\n",
        "        else:\n",
        "            logger.info(f\"DataManager initialized empty. Dimension set to {self.dimension}.\")\n",
        "            self.X_data = np.empty((0, self.dimension if self.dimension else 0))\n",
        "            self.y_data_orig = np.empty((0, 1))\n",
        "            self.y_data_log = np.empty((0, 1))\n",
        "            \n",
        "        self._update_best()\n",
        "\n",
        "    def normalize_X(self):\n",
        "        \"\"\"改进的规范化方法，使用鲁棒性缩放以处理异常值\"\"\"\n",
        "        if self.X_data.shape[0] > 1:\n",
        "            self.mean_X = np.mean(self.X_data, axis=0)\n",
        "            self.std_X = np.std(self.X_data, axis=0)\n",
        "            \n",
        "            # 处理近零方差的特征\n",
        "            epsilon = 1e-6\n",
        "            self.std_X = np.where(self.std_X < epsilon, 1.0, self.std_X)\n",
        "            \n",
        "            # 检查是否有需要鲁棒性处理的极端值\n",
        "            X_normalized = (self.X_data - self.mean_X) / self.std_X\n",
        "            extremes = np.abs(X_normalized) > 5.0\n",
        "            \n",
        "            if np.any(extremes):\n",
        "                # 使用分位数方法处理极端值\n",
        "                q1 = np.percentile(self.X_data, 25, axis=0)\n",
        "                q3 = np.percentile(self.X_data, 75, axis=0)\n",
        "                iqr = q3 - q1\n",
        "                \n",
        "                # 将IQR为0的维度设为1，防止除零\n",
        "                iqr = np.where(iqr < epsilon, 1.0, iqr)\n",
        "                \n",
        "                # 基于IQR的规范化参数\n",
        "                self.mean_X = np.median(self.X_data, axis=0)\n",
        "                self.std_X = iqr / 1.35\n",
        "                \n",
        "                logger.info(\"使用鲁棒性规范化 (基于分位数) 处理极端值\")\n",
        "            else:\n",
        "                logger.info(\"使用标准规范化 (均值和标准差)\")\n",
        "        elif self.X_data.shape[0] == 1:\n",
        "            self.mean_X = self.X_data[0]\n",
        "            self.std_X = np.ones_like(self.X_data[0])\n",
        "            logger.info(\"X normalization parameters set for single data point.\")\n",
        "        else:\n",
        "            self.mean_X = None\n",
        "            self.std_X = None\n",
        "            logger.info(\"No data to calculate X normalization parameters.\")\n",
        "            \n",
        "    def get_normalized_X(self, X_samples):\n",
        "        \"\"\"使用存储的参数对X样本进行规范化，支持缓存优化\"\"\"\n",
        "        # 生成数据哈希用于缓存\n",
        "        data_hash = hashlib.md5(X_samples.tobytes()).hexdigest()\n",
        "        \n",
        "        # 检查缓存\n",
        "        if data_hash in self._normalized_cache:\n",
        "            logger.debug(\"使用缓存的规范化数据\")\n",
        "            return self._normalized_cache[data_hash]\n",
        "        \n",
        "        if self.mean_X is None or self.std_X is None:\n",
        "            if self.X_data.shape[0] > 0:\n",
        "                 logger.warning(\"Normalization params not available, attempting to calculate them now.\")\n",
        "                 self.normalize_X()\n",
        "                 if self.mean_X is None or self.std_X is None:\n",
        "                    raise ValueError(\"Normalization parameters could not be calculated.\")\n",
        "            else:\n",
        "                logger.warning(\"No stored normalization parameters. Normalizing based on input samples.\")\n",
        "                if X_samples.shape[0] > 1:\n",
        "                    mean_temp = np.mean(X_samples, axis=0)\n",
        "                    std_temp = np.std(X_samples, axis=0)\n",
        "                    std_temp[std_temp < 1e-6] = 1.0\n",
        "                    normalized = (X_samples - mean_temp) / std_temp\n",
        "                else:\n",
        "                    normalized = np.zeros_like(X_samples)\n",
        "                \n",
        "                # 缓存结果（限制缓存大小）\n",
        "                if len(self._normalized_cache) < 10:\n",
        "                    self._normalized_cache[data_hash] = normalized.copy()\n",
        "                return normalized\n",
        "\n",
        "        # 应用规范化，并截断极端值到合理范围\n",
        "        normalized = (X_samples - self.mean_X) / self.std_X\n",
        "        normalized = np.clip(normalized, -10.0, 10.0)\n",
        "        \n",
        "        # 缓存结果（限制缓存大小）\n",
        "        if len(self._normalized_cache) < 10:\n",
        "            self._normalized_cache[data_hash] = normalized.copy()\n",
        "        \n",
        "        return normalized\n",
        "\n",
        "    def get_train_data_for_cnn(self):\n",
        "        if self.X_data.shape[0] == 0:\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "        \n",
        "        normalized_X = self.get_normalized_X(self.X_data)\n",
        "        \n",
        "        if self.dimension != 20:\n",
        "            raise ValueError(f\"DataManager expected dimension 20 for CNN reshape, got {self.dimension}\")\n",
        "        \n",
        "        reshaped_X = normalized_X.reshape(-1, 1, 4, 5)\n",
        "        \n",
        "        # Return log-transformed y for training\n",
        "        return torch.tensor(reshaped_X, dtype=torch.float32), torch.tensor(self.y_data_log, dtype=torch.float32)\n",
        "\n",
        "    def add_samples(self, new_X, new_y_orig, re_normalize=False, shuffle=True):\n",
        "        \"\"\"添加新样本到数据集中\"\"\"\n",
        "        if new_X.ndim == 1: \n",
        "            new_X = new_X.reshape(1, -1)\n",
        "        if new_y_orig.ndim == 1: \n",
        "            new_y_orig = new_y_orig.reshape(-1, 1)\n",
        "        \n",
        "        new_y_log = np.log10(new_y_orig + LOG_EPSILON)\n",
        "\n",
        "        if self.X_data.shape[0] == 0:\n",
        "            self.X_data = new_X.copy()\n",
        "            self.y_data_orig = new_y_orig.copy()\n",
        "            self.y_data_log = new_y_log.copy()\n",
        "            if self.dimension is None: \n",
        "                self.dimension = new_X.shape[1]\n",
        "        else:\n",
        "            if new_X.shape[1] != self.dimension:\n",
        "                raise ValueError(f\"New X samples have dimension {new_X.shape[1]}, expected {self.dimension}\")\n",
        "            \n",
        "            # 先合并数据\n",
        "            self.X_data = np.vstack((self.X_data, new_X))\n",
        "            self.y_data_orig = np.vstack((self.y_data_orig, new_y_orig))\n",
        "            self.y_data_log = np.vstack((self.y_data_log, new_y_log))\n",
        "            \n",
        "            # 如果需要打乱，则生成随机排列\n",
        "            if shuffle:\n",
        "                n_samples = self.X_data.shape[0]\n",
        "                permutation = np.random.permutation(n_samples)\n",
        "                self.X_data = self.X_data[permutation]\n",
        "                self.y_data_orig = self.y_data_orig[permutation]\n",
        "                self.y_data_log = self.y_data_log[permutation]\n",
        "                logger.info(f\"数据集已打乱，确保新样本随机分布\")\n",
        "        \n",
        "        if re_normalize or self.mean_X is None:\n",
        "            self.normalize_X()\n",
        "            \n",
        "        self._update_best()\n",
        "        logger.info(f\"Added {new_X.shape[0]} new samples. Total samples: {self.X_data.shape[0]}\")\n",
        "\n",
        "    def _update_best(self):\n",
        "        if self.y_data_orig.shape[0] > 0:\n",
        "            min_idx = np.argmin(self.y_data_orig)\n",
        "            self.best_y = self.y_data_orig[min_idx, 0]\n",
        "            self.best_x = self.X_data[min_idx]\n",
        "        else:\n",
        "            self.best_y = float('inf')\n",
        "            self.best_x = None\n",
        "\n",
        "    def get_current_best(self):\n",
        "        return self.best_x, self.best_y\n",
        "\n",
        "    def save_data(self, file_path):\n",
        "        \"\"\"保存当前数据和规范化参数到.npz文件\"\"\"\n",
        "        try:\n",
        "            save_dict = {\n",
        "                'X_data': self.X_data,\n",
        "                'y_data': self.y_data_orig\n",
        "            }\n",
        "            if self.mean_X is not None:\n",
        "                save_dict['mean_X'] = self.mean_X\n",
        "            if self.std_X is not None:\n",
        "                save_dict['std_X'] = self.std_X\n",
        "            \n",
        "            np.savez(file_path, **save_dict)\n",
        "            logger.info(f\"DataManager state saved to {file_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving DataManager state to {file_path}: {e}\")\n",
        "\n",
        "    @property\n",
        "    def num_samples(self):\n",
        "        return self.X_data.shape[0]\n",
        "\n",
        "    def get_sorted_data_by_y(self, top_n=None):\n",
        "        \"\"\"获取按y值排序的数据（升序，即最优先）\"\"\"\n",
        "        if self.X_data.shape[0] == 0:\n",
        "            return np.array([]), np.array([])\n",
        "            \n",
        "        sorted_indices = np.argsort(self.y_data_orig, axis=0).flatten()\n",
        "        \n",
        "        sorted_X = self.X_data[sorted_indices]\n",
        "        sorted_y_orig = self.y_data_orig[sorted_indices]\n",
        "        \n",
        "        if top_n is not None:\n",
        "            return sorted_X[:top_n], sorted_y_orig[:top_n]\n",
        "        else:\n",
        "            return sorted_X, sorted_y_orig\n",
        "    \n",
        "    def get_best_samples_range(self, n_best=60):\n",
        "        \"\"\"获取最优的N个样本的原始Y值的范围\"\"\"\n",
        "        if self.num_samples == 0:\n",
        "            return float('nan'), float('nan')\n",
        "        \n",
        "        _, sorted_y_orig = self.get_sorted_data_by_y(top_n=n_best)\n",
        "        \n",
        "        if sorted_y_orig.shape[0] == 0:\n",
        "            return float('nan'), float('nan')\n",
        "            \n",
        "        min_y = np.min(sorted_y_orig)\n",
        "        max_y = np.max(sorted_y_orig)\n",
        "        return min_y, max_y\n",
        "\n",
        "    def get_all_y_orig(self):\n",
        "        \"\"\"Returns all original y data.\"\"\"\n",
        "        return self.y_data_orig.copy()\n",
        "\n",
        "    def get_all_y_log(self):\n",
        "        \"\"\"Returns all log-transformed y data.\"\"\"\n",
        "        return self.y_data_log.copy()\n",
        "\n",
        "    def get_all_x_orig(self):\n",
        "        \"\"\"Returns all original X data.\"\"\"\n",
        "        return self.X_data.copy()\n",
        "    \n",
        "    def clear_cache(self):\n",
        "        \"\"\"清理数据缓存以释放内存\"\"\"\n",
        "        self._normalized_cache.clear()\n",
        "        self._cache_hash = None\n",
        "        logger.info(\"已清理DataManager缓存\")\n",
        "\n",
        "logger.info(\"DataManager类加载完成\")\n",
        "print(\"DataManager模块已准备就绪\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: 性能优化模块 (简化版)\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "class PerformanceOptimizer:\n",
        "    \"\"\"性能优化器，负责优化TensorFlow训练性能\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cpu_count = psutil.cpu_count(logical=True)\n",
        "        self.physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "        self.memory_cache = {}\n",
        "        self.is_configured = False\n",
        "        logger.info(f\"检测到CPU核心数: 物理={self.physical_cpu_count}, 逻辑={self.cpu_count}\")\n",
        "    \n",
        "    def configure_tensorflow_performance(self):\n",
        "        \"\"\"配置TensorFlow性能设置\"\"\"\n",
        "        if self.is_configured:\n",
        "            return\n",
        "            \n",
        "        logger.info(\"开始配置TensorFlow性能优化...\")\n",
        "        \n",
        "        # 配置GPU内存增长\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                logger.info(f\"已配置GPU内存增长，检测到{len(gpus)}个GPU\")\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPU内存配置失败: {e}\")\n",
        "        \n",
        "        # 配置CPU并行度\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(self.cpu_count)\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(self.physical_cpu_count)\n",
        "        \n",
        "        # 配置环境变量\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "        os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "        \n",
        "        self.is_configured = True\n",
        "        logger.info(\"TensorFlow性能优化配置完成\")\n",
        "    \n",
        "    def create_optimized_dataset(self, X_data, y_data, batch_size=32, shuffle=True, cache=True):\n",
        "        \"\"\"创建优化的tf.data.Dataset\"\"\"\n",
        "        logger.info(f\"创建优化的数据集，样本数: {len(X_data)}, 批次大小: {batch_size}\")\n",
        "        \n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
        "        \n",
        "        if cache and len(X_data) < 50000:\n",
        "            dataset = dataset.cache()\n",
        "            logger.info(\"已启用数据集内存缓存\")\n",
        "        \n",
        "        if shuffle:\n",
        "            buffer_size = min(len(X_data), 10000)\n",
        "            dataset = dataset.shuffle(buffer_size=buffer_size)\n",
        "        \n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "        return dataset\n",
        "    \n",
        "    def clear_memory_cache(self):\n",
        "        \"\"\"清理内存缓存\"\"\"\n",
        "        self.memory_cache.clear()\n",
        "        gc.collect()\n",
        "        logger.info(\"已清理内存缓存\")\n",
        "    \n",
        "    def cache_data(self, key, data):\n",
        "        \"\"\"缓存数据到内存\"\"\"\n",
        "        self.memory_cache[key] = data\n",
        "    \n",
        "    def get_cached_data(self, key):\n",
        "        \"\"\"从缓存获取数据\"\"\"\n",
        "        return self.memory_cache.get(key, None)\n",
        "    \n",
        "    def optimize_memory_usage(self):\n",
        "        \"\"\"优化内存使用\"\"\"\n",
        "        gc.collect()\n",
        "        if tf.config.experimental.list_physical_devices('GPU'):\n",
        "            try:\n",
        "                tf.keras.backend.clear_session()\n",
        "            except:\n",
        "                pass\n",
        "        logger.info(\"已执行内存优化清理\")\n",
        "    \n",
        "    def monitor_performance(self, stage_name=\"\"):\n",
        "        \"\"\"监控性能指标\"\"\"\n",
        "        try:\n",
        "            process = psutil.Process()\n",
        "            memory_info = process.memory_info()\n",
        "            memory_mb = memory_info.rss / 1024 / 1024\n",
        "            logger.info(f\"[{stage_name}] 内存使用: {memory_mb:.1f}MB\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"性能监控失败: {e}\")\n",
        "\n",
        "# 创建全局性能优化器实例\n",
        "performance_optimizer = PerformanceOptimizer()\n",
        "\n",
        "logger.info(\"性能优化模块加载完成\")\n",
        "print(\"性能优化模块已准备就绪\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: NTE Searcher (完整版) - 关键调试模块\n",
        "\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "from typing import List, Tuple, Dict, Any, Union, Callable\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# NTE搜索器常量\n",
        "NTE_C0 = 1\n",
        "NTE_ROLLOUT_ROUNDS = 100\n",
        "VALIDATION_BATCH_SIZE_NTE = 20\n",
        "\n",
        "def safe_power10(log_values, clip_min=-30, clip_max=25):\n",
        "    \"\"\"安全地计算10的幂，避免数值溢出\"\"\"\n",
        "    clipped_values = np.clip(log_values, clip_min, clip_max)\n",
        "    return 10.0 ** clipped_values\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    \"\"\"NTE搜索树的节点\"\"\"\n",
        "    state: Tuple[float, ...]  # 状态向量（元组用于哈希）\n",
        "    value_pred: float = float('inf')  # 替代模型预测值\n",
        "    visit_count: int = 0  # 访问次数\n",
        "\n",
        "class NTESearcher:\n",
        "    \"\"\"Neural Tree Expansion搜索器 - 完整实现，包含所有原始功能\"\"\"\n",
        "    \n",
        "    def __init__(self, dimension, domain=None, domain_mins=None, domain_maxs=None, \n",
        "                 c0=NTE_C0, rollout_rounds=NTE_ROLLOUT_ROUNDS, \n",
        "                 validation_batch_size=VALIDATION_BATCH_SIZE_NTE,\n",
        "                 rho_scaling_factor_heuristic=lambda min_y: 1.0):\n",
        "        \"\"\"初始化NTE搜索器\"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.base_c0 = c0\n",
        "        self.rollout_rounds = rollout_rounds\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "        self.rho_scaling_factor_heuristic = rho_scaling_factor_heuristic\n",
        "        \n",
        "        # 处理域边界 - 保持向后兼容\n",
        "        if domain_mins is not None and domain_maxs is not None:\n",
        "            self.domain_mins = np.array(domain_mins)\n",
        "            self.domain_maxs = np.array(domain_maxs)\n",
        "        elif domain is not None:\n",
        "            domain_array = np.array(domain)\n",
        "            self.domain_mins = domain_array[:, 0]\n",
        "            self.domain_maxs = domain_array[:, 1]\n",
        "        else:\n",
        "            self.domain_mins = np.array([-2.048] * dimension)\n",
        "            self.domain_maxs = np.array([2.048] * dimension)\n",
        "            \n",
        "        # 状态追踪变量\n",
        "        self.last_move_successful = False\n",
        "        self.last_move_deterministic = False\n",
        "        self.last_move_type = None\n",
        "        self.last_move_direction = None\n",
        "        self.last_move_dim_index = -1\n",
        "\n",
        "print(\"✓ NTE Searcher (完整版第1部分) 已加载\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: NTE Searcher 完整实现 - 混合扩展策略\n",
        "\n",
        "def _calculate_dynamic_search_space(self, all_states=None, all_values=None, iteration=1):\n",
        "    \"\"\"计算动态搜索空间，考虑已知样本分布和迭代进度\"\"\"\n",
        "    if all_states is None or len(all_states) == 0:\n",
        "        return self.domain_mins.copy(), self.domain_maxs.copy()\n",
        "        \n",
        "    # 基于当前数据集动态调整搜索范围\n",
        "    data_mins = np.min(all_states, axis=0)\n",
        "    data_maxs = np.max(all_states, axis=0)\n",
        "    data_ranges = data_maxs - data_mins\n",
        "    \n",
        "    # 计算扩展因子：早期迭代更广泛探索，后期更集中\n",
        "    base_expansion = 0.2\n",
        "    iteration_factor = max(0.1, 1.0 - 0.01 * iteration)\n",
        "    expansion_factor = base_expansion * iteration_factor\n",
        "    \n",
        "    # 基于函数值分布调整搜索区域\n",
        "    if all_values is not None and len(all_values) > 0:\n",
        "        # 找到最优10%的样本，重点关注这些区域\n",
        "        sorted_indices = np.argsort(all_values.flatten())\n",
        "        top_10_percent = max(1, len(sorted_indices) // 10)\n",
        "        best_samples = all_states[sorted_indices[:top_10_percent]]\n",
        "        \n",
        "        if len(best_samples) > 0:\n",
        "            best_mins = np.min(best_samples, axis=0)\n",
        "            best_maxs = np.max(best_samples, axis=0)\n",
        "            best_ranges = best_maxs - best_mins\n",
        "            \n",
        "            # 在最优区域周围扩展\n",
        "            best_expansion = 0.5  # 在最优区域周围扩展50%\n",
        "            extended_best_mins = best_mins - best_expansion * best_ranges\n",
        "            extended_best_maxs = best_maxs + best_expansion * best_ranges\n",
        "            \n",
        "            # 合并全局搜索区域和最优区域\n",
        "            extended_mins = np.minimum(data_mins - expansion_factor * data_ranges, extended_best_mins)\n",
        "            extended_maxs = np.maximum(data_maxs + expansion_factor * data_ranges, extended_best_maxs)\n",
        "        else:\n",
        "            extended_mins = data_mins - expansion_factor * data_ranges\n",
        "            extended_maxs = data_maxs + expansion_factor * data_ranges\n",
        "    else:\n",
        "        extended_mins = data_mins - expansion_factor * data_ranges\n",
        "        extended_maxs = data_maxs + expansion_factor * data_ranges\n",
        "    \n",
        "    # 确保在原始域内\n",
        "    dynamic_mins = np.maximum(extended_mins, self.domain_mins)\n",
        "    dynamic_maxs = np.minimum(extended_maxs, self.domain_maxs)\n",
        "    \n",
        "    # 确保搜索区域不会退化为点\n",
        "    for i in range(self.dimension):\n",
        "        if dynamic_maxs[i] - dynamic_mins[i] < 0.01:\n",
        "            center = (dynamic_mins[i] + dynamic_maxs[i]) / 2\n",
        "            dynamic_mins[i] = max(self.domain_mins[i], center - 0.005)\n",
        "            dynamic_maxs[i] = min(self.domain_maxs[i], center + 0.005)\n",
        "    \n",
        "    return dynamic_mins, dynamic_maxs\n",
        "\n",
        "def _mixed_expansion(self, state_vector, dynamic_factor=1.0, all_states=None, \n",
        "                    all_values=None, iteration=1, pre_computed_bounds=None):\n",
        "    \"\"\"完整的混合扩展策略，包括随机移动和确定性移动\"\"\"\n",
        "    leaf_states = []\n",
        "    current_state = state_vector.copy()\n",
        "\n",
        "    if self.dimension <= 0:\n",
        "        return []\n",
        "\n",
        "    # 计算动态搜索区间\n",
        "    if pre_computed_bounds is not None:\n",
        "        dynamic_mins, dynamic_maxs = pre_computed_bounds\n",
        "    else:\n",
        "        dynamic_mins, dynamic_maxs = self._calculate_dynamic_search_space(all_states, all_values, iteration)\n",
        "    \n",
        "    # 计算自适应步长\n",
        "    base_step = 0.05\n",
        "    adaptive_step = base_step * (0.5 + 0.5 * (dynamic_factor - 1.0))\n",
        "    \n",
        "    # 调整探索概率\n",
        "    stochastic_prob = 0.67  # 默认67%概率使用随机移动\n",
        "    \n",
        "    if self.last_move_successful and self.last_move_deterministic:\n",
        "        stochastic_prob = 0.0  # 如果上次确定性移动成功，继续使用确定性移动\n",
        "        logger.debug(\"上次确定性移动成功，继续使用确定性移动(100%)\")\n",
        "    else:\n",
        "        logger.debug(f\"使用标准概率: {stochastic_prob*100:.1f}%随机移动, {(1-stochastic_prob)*100:.1f}%确定性移动\")\n",
        "    \n",
        "    # 混合扩展策略：生成维度数量的叶节点\n",
        "    for _ in range(self.dimension): \n",
        "        new_state = current_state.copy()\n",
        "        \n",
        "        use_stochastic = random.random() < stochastic_prob\n",
        "        \n",
        "        if use_stochastic: \n",
        "            # 随机移动\n",
        "            move_type = random.choices(\n",
        "                ['single_var', 'd_div_5_vars', 'd_div_10_vars'],\n",
        "                weights=[0.5, 0.25, 0.25],\n",
        "                k=1\n",
        "            )[0]\n",
        "            \n",
        "            self.last_move_deterministic = False\n",
        "            \n",
        "            if move_type == 'single_var':\n",
        "                # 随机改变1个变量\n",
        "                idx = random.randint(0, self.dimension - 1)\n",
        "                new_state[idx] = random.uniform(dynamic_mins[idx], dynamic_maxs[idx])\n",
        "                \n",
        "            elif move_type == 'd_div_5_vars':\n",
        "                # 随机改变d/5个变量\n",
        "                num_vars_to_change = max(1, int(self.dimension / 5))\n",
        "                indices = random.sample(range(self.dimension), num_vars_to_change)\n",
        "                for idx in indices:\n",
        "                    new_state[idx] = random.uniform(dynamic_mins[idx], dynamic_maxs[idx])\n",
        "                    \n",
        "            elif move_type == 'd_div_10_vars':\n",
        "                # 随机改变d/10个变量\n",
        "                num_vars_to_change = max(1, int(self.dimension / 10))\n",
        "                indices = random.sample(range(self.dimension), num_vars_to_change)\n",
        "                for idx in indices:\n",
        "                    new_state[idx] = random.uniform(dynamic_mins[idx], dynamic_maxs[idx])\n",
        "        else:\n",
        "            # 确定性移动\n",
        "            self.last_move_deterministic = True\n",
        "            self.last_move_type = \"multi_dim_step\"\n",
        "            \n",
        "            # 选择修改的维度数量\n",
        "            dim_choice = random.choices(\n",
        "                ['single_dim', 'd_div_5_dims', 'd_div_10_dims', 'd_div_2_dims'],\n",
        "                weights=[0.91, 0.03, 0.03, 0.03],\n",
        "                k=1\n",
        "            )[0]\n",
        "            \n",
        "            # 确定要修改的维度索引\n",
        "            if dim_choice == 'single_dim':\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    indices = [self.last_move_dim_index]\n",
        "                else:\n",
        "                    indices = [random.randint(0, self.dimension - 1)]\n",
        "                logger.debug(f\"确定性移动: 修改单个维度\")\n",
        "                \n",
        "            elif dim_choice == 'd_div_5_dims':\n",
        "                num_dims = max(4, int(self.dimension / 5))\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    remaining_dims = [i for i in range(self.dimension) if i != self.last_move_dim_index]\n",
        "                    additional_dims = random.sample(remaining_dims, min(num_dims - 1, len(remaining_dims)))\n",
        "                    indices = [self.last_move_dim_index] + additional_dims\n",
        "                else:\n",
        "                    indices = random.sample(range(self.dimension), num_dims)\n",
        "                logger.debug(f\"确定性移动: 修改d/5={num_dims}个维度\")\n",
        "                \n",
        "            elif dim_choice == 'd_div_10_dims':\n",
        "                num_dims = max(20, int(self.dimension / 10))\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    remaining_dims = [i for i in range(self.dimension) if i != self.last_move_dim_index]\n",
        "                    additional_dims = random.sample(remaining_dims, min(num_dims - 1, len(remaining_dims)))\n",
        "                    indices = [self.last_move_dim_index] + additional_dims\n",
        "                else:\n",
        "                    indices = random.sample(range(self.dimension), num_dims)\n",
        "                logger.debug(f\"确定性移动: 修改d/10={num_dims}个维度\")\n",
        "            \n",
        "            elif dim_choice == 'd_div_2_dims':\n",
        "                num_dims = 2\n",
        "                if self.last_move_successful and self.last_move_dim_index >= 0:\n",
        "                    remaining_dims = [i for i in range(self.dimension) if i != self.last_move_dim_index]\n",
        "                    additional_dims = random.sample(remaining_dims, min(num_dims - 1, len(remaining_dims)))\n",
        "                    indices = [self.last_move_dim_index] + additional_dims\n",
        "                else:\n",
        "                    indices = random.sample(range(self.dimension), num_dims)\n",
        "                logger.debug(f\"确定性移动: 修改2个维度\")\n",
        "            \n",
        "            # 记录第一个维度\n",
        "            if indices:\n",
        "                self.last_move_dim_index = indices[0]\n",
        "            \n",
        "            # 对选定的维度进行小步调整\n",
        "            for idx in indices:\n",
        "                if self.last_move_successful and idx == self.last_move_dim_index:\n",
        "                    direction = self.last_move_direction\n",
        "                else:\n",
        "                    direction = random.choice([-1, 1])\n",
        "                \n",
        "                if idx == indices[0]:\n",
        "                    self.last_move_direction = direction\n",
        "                \n",
        "                step_size = 0.1 \n",
        "                new_state[idx] = current_state[idx] + direction * step_size\n",
        "        \n",
        "        # 确保状态在动态搜索区间内\n",
        "        np.clip(new_state, dynamic_mins, dynamic_maxs, out=new_state)\n",
        "        leaf_states.append(new_state)\n",
        "\n",
        "    # 确保生成足够的叶节点\n",
        "    if not leaf_states: \n",
        "        rand_state = np.array([random.uniform(dynamic_mins[i], dynamic_maxs[i]) for i in range(self.dimension)])\n",
        "        return [rand_state] * self.dimension if self.dimension > 0 else []\n",
        "    \n",
        "    while len(leaf_states) < self.dimension and self.dimension > 0:\n",
        "        leaf_states.append(leaf_states[-1].copy())\n",
        "\n",
        "    return leaf_states[:self.dimension]\n",
        "\n",
        "# 将方法添加到NTESearcher类中\n",
        "NTESearcher._calculate_dynamic_search_space = _calculate_dynamic_search_space\n",
        "NTESearcher._mixed_expansion = _mixed_expansion\n",
        "\n",
        "print(\"✓ NTE Searcher 核心扩展方法已添加\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: 环境配置验证与基本功能测试\n",
        "import traceback\n",
        "\n",
        "def comprehensive_environment_test():\n",
        "    \"\"\"执行全面的环境配置验证和基本功能测试\"\"\"\n",
        "    print(\"🚀 开始执行DANTE环境配置验证...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    test_results = {\n",
        "        'gpu_config': False,\n",
        "        'data_loading': False,\n",
        "        'model_creation': False,\n",
        "        'model_training': False,\n",
        "        'nte_search': False,\n",
        "        'visualization': False\n",
        "    }\n",
        "    \n",
        "    # 1. GPU配置验证\n",
        "    print(\"📊 1. GPU配置验证\")\n",
        "    try:\n",
        "        print(f\"   PyTorch CUDA可用: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"   GPU设备数量: {torch.cuda.device_count()}\")\n",
        "            print(f\"   当前GPU: {torch.cuda.get_device_name()}\")\n",
        "            print(f\"   GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "        \n",
        "        print(f\"   TensorFlow GPU可用: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            print(f\"   TensorFlow GPU数量: {len(tf.config.list_physical_devices('GPU'))}\")\n",
        "        \n",
        "        test_results['gpu_config'] = True\n",
        "        print(\"   ✅ GPU配置验证通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ GPU配置验证失败: {e}\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 2. 数据加载与管理验证\n",
        "    print(\"📁 2. 数据加载与管理验证\")\n",
        "    try:\n",
        "        # 生成测试数据\n",
        "        test_dimension = 20\n",
        "        test_samples = 100\n",
        "        \n",
        "        print(f\"   生成测试数据: {test_samples}个样本, {test_dimension}维\")\n",
        "        X_test = np.random.uniform(-2.048, 2.048, (test_samples, test_dimension))\n",
        "        y_test = np.array([rosenbrock_function(x) for x in X_test]).reshape(-1, 1)\n",
        "        \n",
        "        print(f\"   Rosenbrock函数值范围: {y_test.min():.2e} - {y_test.max():.2e}\")\n",
        "        \n",
        "        # 测试DataManager\n",
        "        data_manager = DataManager(\n",
        "            initial_X=X_test[:80], \n",
        "            initial_y=y_test[:80], \n",
        "            dimension=test_dimension\n",
        "        )\n",
        "        \n",
        "        print(f\"   DataManager初始化成功: {data_manager.num_samples}个样本\")\n",
        "        print(f\"   数据标准化测试: X范围{data_manager.X_normalized.min():.3f}-{data_manager.X_normalized.max():.3f}\")\n",
        "        print(f\"   对数变换测试: y_log范围{data_manager.y_log.min():.3f}-{data_manager.y_log.max():.3f}\")\n",
        "        \n",
        "        test_results['data_loading'] = True\n",
        "        print(\"   ✅ 数据加载与管理验证通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ 数据加载与管理验证失败: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 3. CNN模型创建验证\n",
        "    print(\"🧠 3. CNN模型创建验证\")\n",
        "    try:\n",
        "        cnn_model = CNN1DSurrogate(input_dim=test_dimension)\n",
        "        print(f\"   CNN1D模型创建成功\")\n",
        "        print(f\"   模型参数数量: {sum(p.numel() for p in cnn_model.model.parameters() if p.requires_grad):,}\")\n",
        "        \n",
        "        # 测试预测功能\n",
        "        test_input = X_test[:5]\n",
        "        predictions = cnn_model.predict(test_input)\n",
        "        print(f\"   模型预测测试: 输入{test_input.shape} -> 输出{predictions.shape}\")\n",
        "        print(f\"   预测值范围: {predictions.min():.3f} - {predictions.max():.3f}\")\n",
        "        \n",
        "        test_results['model_creation'] = True\n",
        "        print(\"   ✅ CNN模型创建验证通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ CNN模型创建验证失败: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 4. 模型训练验证（快速测试）\n",
        "    print(\"🏃 4. 模型训练验证（快速测试）\")\n",
        "    try:\n",
        "        # 快速训练测试（仅5个epoch）\n",
        "        print(\"   开始快速训练测试（5个epochs）...\")\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=5,\n",
        "            batch_size=16,\n",
        "            validation_split=0.2,\n",
        "            patience=10,\n",
        "            verbose=1\n",
        "        )\n",
        "        \n",
        "        final_loss = training_history['loss'][-1]\n",
        "        print(f\"   训练完成，最终损失: {final_loss:.4f}\")\n",
        "        \n",
        "        # 测试训练后的预测\n",
        "        post_train_predictions = cnn_model.predict(test_input)\n",
        "        print(f\"   训练后预测值范围: {post_train_predictions.min():.3f} - {post_train_predictions.max():.3f}\")\n",
        "        \n",
        "        test_results['model_training'] = True\n",
        "        print(\"   ✅ 模型训练验证通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ 模型训练验证失败: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 5. NTE搜索验证\n",
        "    print(\"🔍 5. NTE搜索验证\")\n",
        "    try:\n",
        "        # 创建NTE搜索器\n",
        "        nte_searcher = NTESearcher(\n",
        "            dimension=test_dimension,\n",
        "            domain=[(-2.048, 2.048) for _ in range(test_dimension)],\n",
        "            c0=1,\n",
        "            rollout_rounds=10,  # 减少轮次以加快测试\n",
        "            validation_batch_size=5   # 减少批次大小\n",
        "        )\n",
        "        \n",
        "        print(f\"   NTESearcher创建成功\")\n",
        "        \n",
        "        # 创建简化的surrogate wrapper用于测试\n",
        "        class TestSurrogateWrapper:\n",
        "            def __init__(self, model):\n",
        "                self.model = model\n",
        "            \n",
        "            def predict(self, X_batch):\n",
        "                if X_batch.shape[0] == 0:\n",
        "                    return np.array([])\n",
        "                return self.model.predict(X_batch)\n",
        "        \n",
        "        wrapper = TestSurrogateWrapper(cnn_model)\n",
        "        current_best_x, current_best_y = data_manager.get_current_best()\n",
        "        \n",
        "        print(f\"   开始NTE搜索测试（小规模）...\")\n",
        "        print(f\"   当前最优值: {current_best_y:.4e}\")\n",
        "        \n",
        "        # 执行简化的NTE搜索\n",
        "        search_result = nte_searcher.search(\n",
        "            surrogate_model=wrapper,\n",
        "            current_best_state=current_best_x,\n",
        "            min_y_observed=current_best_y,\n",
        "            dynamic_exploration_factor=1.0,\n",
        "            all_states=data_manager.X_data,\n",
        "            all_values=data_manager.get_all_y_orig(),\n",
        "            previous_iteration_samples=None,\n",
        "            current_iteration=0,\n",
        "            return_ducb_trends=True\n",
        "        )\n",
        "        \n",
        "        candidates, ducb_trends = search_result\n",
        "        print(f\"   NTE搜索完成: 找到{len(candidates)}个候选点\")\n",
        "        \n",
        "        if len(candidates) > 0:\n",
        "            # 评估候选点\n",
        "            candidate_values = np.array([rosenbrock_function(x) for x in candidates])\n",
        "            print(f\"   候选点真值范围: {candidate_values.min():.4e} - {candidate_values.max():.4e}\")\n",
        "            print(f\"   最佳候选点值: {candidate_values.min():.4e}\")\n",
        "        \n",
        "        test_results['nte_search'] = True\n",
        "        print(\"   ✅ NTE搜索验证通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ NTE搜索验证失败: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # 6. 可视化功能验证\n",
        "    print(\"📈 6. 可视化功能验证\")\n",
        "    try:\n",
        "        # 测试基本可视化功能\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        # 创建测试图形\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        test_x = np.linspace(0, 10, 50)\n",
        "        test_y = np.sin(test_x)\n",
        "        plt.plot(test_x, test_y, 'b-', label='Test Plot')\n",
        "        plt.xlabel('X Values')\n",
        "        plt.ylabel('Y Values')\n",
        "        plt.title('Visualization Test')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 在Kaggle环境中显示图形\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        \n",
        "        print(\"   ✅ 基本可视化功能正常\")\n",
        "        \n",
        "        # 测试plot_utils中的函数\n",
        "        if len(candidates) > 0:\n",
        "            try:\n",
        "                # 测试预测vs真值图\n",
        "                pred_vs_true_metrics = plot_prediction_vs_truth(\n",
        "                    cnn_model.predict(candidates), \n",
        "                    candidate_values, \n",
        "                    \"/kaggle/working\", \n",
        "                    iteration=0,\n",
        "                    filename=\"test_pred_vs_true.png\"\n",
        "                )\n",
        "                print(f\"   预测vs真值图测试成功: RMSE={pred_vs_true_metrics[1]:.4e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   高级可视化测试警告: {e}\")\n",
        "        \n",
        "        test_results['visualization'] = True\n",
        "        print(\"   ✅ 可视化功能验证通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ 可视化功能验证失败: {e}\")\n",
        "        traceback.print_exc()\n",
        "    \n",
        "    print()\n",
        "    print(\"🔧 环境配置验证完成!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # 输出测试总结\n",
        "    passed_tests = sum(test_results.values())\n",
        "    total_tests = len(test_results)\n",
        "    \n",
        "    print(f\"📊 测试结果汇总: {passed_tests}/{total_tests} 项测试通过\")\n",
        "    print()\n",
        "    \n",
        "    for test_name, passed in test_results.items():\n",
        "        status = \"✅\" if passed else \"❌\"\n",
        "        print(f\"   {status} {test_name.replace('_', ' ').title()}\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    if passed_tests == total_tests:\n",
        "        print(\"🎉 所有环境配置验证通过！DANTE已准备好进行完整训练。\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"⚠️  有 {total_tests - passed_tests} 项测试未通过，请检查相关配置后再进行完整训练。\")\n",
        "        return False\n",
        "\n",
        "# 执行环境验证\n",
        "print(\"开始DANTE环境配置验证...\")\n",
        "validation_success = comprehensive_environment_test()\n",
        "\n",
        "if validation_success:\n",
        "    print(\"\\n🚀 环境验证成功！可以开始完整的DANTE训练流程。\")\n",
        "else:\n",
        "    print(\"\\n⚠️  环境验证未完全通过，建议先解决问题后再进行训练。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: 主要DANTE训练执行逻辑\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gc\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 配置日志记录器\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def calculate_pearson_correlation(model, data_manager):\n",
        "    \"\"\"计算模型在所有数据上的皮尔逊相关系数\"\"\"\n",
        "    if data_manager.num_samples < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        X_data = data_manager.get_all_x_orig()\n",
        "        y_data_orig = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        predictions_log = model.predict(X_data)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_data_orig) < 2:\n",
        "            logger.warning(\"数据点不足，无法计算皮尔逊相关系数\")\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_data_orig)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"计算皮尔逊相关系数时出错: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_best_samples(model, data_manager, n_best=60):\n",
        "    \"\"\"计算模型在最佳样本上的皮尔逊相关系数\"\"\"\n",
        "    all_X = data_manager.get_all_x_orig()\n",
        "    all_y_orig = data_manager.get_all_y_orig()\n",
        "    \n",
        "    if all_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "    \n",
        "    # 按y值排序（升序）取最佳样本\n",
        "    sorted_indices = np.argsort(all_y_orig.flatten())\n",
        "    n_best = min(n_best, len(sorted_indices))\n",
        "    sorted_X = all_X[sorted_indices[:n_best]]\n",
        "    sorted_y_orig = all_y_orig[sorted_indices[:n_best]]\n",
        "    \n",
        "    min_y_orig_for_range = np.min(sorted_y_orig)\n",
        "    max_y_orig_for_range = np.max(sorted_y_orig)\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(sorted_X)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if np.any(np.isinf(predictions_orig)) or np.any(np.isnan(predictions_orig)):\n",
        "            logger.warning(\"预测值中存在无穷大或NaN值\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "        true_values_orig = sorted_y_orig.flatten()\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(true_values_orig) < 2:\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "        \n",
        "        if np.var(predictions_orig) == 0 or np.var(true_values_orig) == 0:\n",
        "            logger.warning(\"预测值或真实值没有变化，无法计算相关系数\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, true_values_orig)\n",
        "        if np.isnan(correlation) or np.isinf(correlation):\n",
        "            logger.warning(\"Pearson相关系数计算结果为NaN或无穷大\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "        return (correlation if not np.isnan(correlation) else 0.0), min_y_orig_for_range, max_y_orig_for_range\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"计算Pearson相关系数时出错: {e}\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "\n",
        "def create_weighted_samples(num_total_samples, y_data_orig_for_weighting, \n",
        "                          nte_samples_X=None, nte_samples_y_orig=None, \n",
        "                          initial_lowest_indices=None, initial_highest_indices=None):\n",
        "    \"\"\"为数据集创建样本权重\"\"\"\n",
        "    sample_weights = np.ones(num_total_samples)  # 默认权重为1\n",
        "\n",
        "    if initial_lowest_indices is not None:\n",
        "        sample_weights[initial_lowest_indices] = 2.0\n",
        "        logger.info(f\"为初始数据集中 {len(initial_lowest_indices)} 个最低y值样本设置权重为2.0\")\n",
        "\n",
        "    if initial_highest_indices is not None:\n",
        "        sample_weights[initial_highest_indices] = 3.0\n",
        "        logger.info(f\"为初始数据集中 {len(initial_highest_indices)} 个最高y值样本设置权重为3.0\")\n",
        "\n",
        "    # 为NTE样本设置权重\n",
        "    if nte_samples_X is not None and nte_samples_y_orig is not None:\n",
        "        num_nte_samples = nte_samples_X.shape[0]\n",
        "        start_index_for_nte = num_total_samples - num_nte_samples\n",
        "        \n",
        "        if num_nte_samples > 0:\n",
        "            nte_specific_weights = np.ones(num_nte_samples) * 4.0\n",
        "            sample_weights[start_index_for_nte:] = nte_specific_weights\n",
        "            logger.info(f\"为新加入的 {num_nte_samples} 个NTE样本设置权重为4.0\")\n",
        "\n",
        "    final_weights = torch.tensor(sample_weights, dtype=torch.float32)\n",
        "    logger.info(f\"最终样本权重统计: 最小值={final_weights.min().item():.1f}, 最大值={final_weights.max().item():.1f}, 平均值={final_weights.mean().item():.1f}\")\n",
        "    \n",
        "    return final_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: 主训练函数和启动脚本\n",
        "def save_iteration_samples_to_csv(X_batch, y_batch, results_dir, iteration):\n",
        "    \"\"\"将每次迭代的样本保存到CSV文件\"\"\"\n",
        "    try:\n",
        "        csv_filename = os.path.join(results_dir, f\"iteration_{iteration}_samples.csv\")\n",
        "        \n",
        "        data_dict = {}\n",
        "        for i in range(X_batch.shape[1]):\n",
        "            data_dict[f'x{i+1}'] = X_batch[:, i]\n",
        "        data_dict['y_true'] = y_batch.flatten()\n",
        "        \n",
        "        df = pd.DataFrame(data_dict)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        \n",
        "        logger.info(f\"第{iteration}轮迭代的{len(y_batch)}个样本已保存到: {csv_filename}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"保存样本到CSV失败: {e}\")\n",
        "\n",
        "def main_dante_training():\n",
        "    \"\"\"DANTE主训练流程\"\"\"\n",
        "    logger.info(\"开始DANTE Rosenbrock函数优化...\")\n",
        "    \n",
        "    # === 参数配置 ===\n",
        "    ROSENBROCK_DIMENSION = 20\n",
        "    domain_min, domain_max = -2.048, 2.048 \n",
        "    ROSENBROCK_DOMAIN = [(domain_min, domain_max)] * ROSENBROCK_DIMENSION\n",
        "    \n",
        "    INITIAL_DATA_SIZE = 800\n",
        "    N_AL_ITERATIONS = 100  # 在Kaggle上使用较少的迭代次数进行测试\n",
        "    CNN_TRAINING_EPOCHS = 200\n",
        "    CNN_EARLY_STOPPING_PATIENCE = 30\n",
        "    CNN_VALIDATION_SPLIT = 0.2\n",
        "    CNN_BATCH_SIZE = 32\n",
        "    NTE_C0 = 1\n",
        "    NTE_ROLLOUT_ROUNDS = 100\n",
        "    VALIDATION_BATCH_SIZE = 20\n",
        "    \n",
        "    logger.info(f\"DANTE配置:\")\n",
        "    logger.info(f\"  维度: {ROSENBROCK_DIMENSION}\")\n",
        "    logger.info(f\"  初始数据大小: {INITIAL_DATA_SIZE}\")\n",
        "    logger.info(f\"  主动学习迭代次数: {N_AL_ITERATIONS}\")\n",
        "    logger.info(f\"  CNN训练轮数: {CNN_TRAINING_EPOCHS}\")\n",
        "    \n",
        "    # 检测GPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"使用设备: {device}\")\n",
        "    \n",
        "    # === 创建结果目录 ===\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_dir = f\"/kaggle/working/dante_results_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    logger.info(f\"结果将保存到: {results_dir}\")\n",
        "    \n",
        "    # === 初始化历史记录变量 ===\n",
        "    cnn_performance_history = []\n",
        "    history_pearson_all = []\n",
        "    history_pearson_best_samples = []\n",
        "    history_iterations = []\n",
        "    history_best_y = []\n",
        "    history_nte_candidates_count = []\n",
        "    \n",
        "    no_improvement_streak = 0\n",
        "    previous_best_y = float('inf')\n",
        "    previous_iteration_samples_x = None\n",
        "    \n",
        "    viz_interval = 20  # 每20次迭代可视化一次\n",
        "    logger.info(f\"可视化间隔设置为: 每{viz_interval}次迭代输出一次结果\")\n",
        "    \n",
        "    # === 1. 数据初始化 ===\n",
        "    logger.info(\"生成初始数据...\")\n",
        "    initial_X = np.random.uniform(low=domain_min, high=domain_max, size=(INITIAL_DATA_SIZE, ROSENBROCK_DIMENSION))\n",
        "    initial_y = np.array([rosenbrock_function(x) for x in initial_X]).reshape(-1, 1)\n",
        "    logger.info(\"初始数据生成完成\")\n",
        "    \n",
        "    # === 2. DataManager初始化 ===\n",
        "    data_manager = DataManager(\n",
        "        initial_X=initial_X, \n",
        "        initial_y=initial_y, \n",
        "        dimension=ROSENBROCK_DIMENSION,\n",
        "        data_file_path=None\n",
        "    )\n",
        "    logger.info(\"DataManager初始化完成\")\n",
        "    \n",
        "    # === 3. 初始样本权重设定 ===\n",
        "    if data_manager.num_samples > 0:\n",
        "        all_y_orig = data_manager.get_all_y_orig().flatten()\n",
        "        sorted_indices = np.argsort(all_y_orig)\n",
        "        \n",
        "        lowest_20_indices = sorted_indices[:20] if len(sorted_indices) >= 20 else sorted_indices\n",
        "        highest_20_indices = sorted_indices[-20:] if len(sorted_indices) >= 20 else np.array([])\n",
        "\n",
        "        initial_weights = create_weighted_samples(\n",
        "            num_total_samples=data_manager.num_samples,\n",
        "            y_data_orig_for_weighting=all_y_orig,\n",
        "            initial_lowest_indices=lowest_20_indices,\n",
        "            initial_highest_indices=highest_20_indices\n",
        "        )\n",
        "        data_manager.last_batch_weights = initial_weights\n",
        "    else:\n",
        "        data_manager.last_batch_weights = torch.empty(0, dtype=torch.float32)\n",
        "    \n",
        "    # === 4. CNN模型初始化 ===\n",
        "    cnn_model = CNN1DSurrogate(input_dim=ROSENBROCK_DIMENSION)\n",
        "    logger.info(\"CNN1DSurrogate模型初始化完成\")\n",
        "    \n",
        "    # === 5. 主动学习主循环 ===\n",
        "    for al_iteration in range(N_AL_ITERATIONS):\n",
        "        should_generate_plots = ((al_iteration + 1) % viz_interval == 0) or (al_iteration == N_AL_ITERATIONS - 1)\n",
        "        current_iteration_number = al_iteration + 1\n",
        "        history_iterations.append(current_iteration_number)\n",
        "        \n",
        "        logger.info(f\"\\n--- 训练CNN 1D模型 (迭代 {current_iteration_number}/{N_AL_ITERATIONS}) ---\")\n",
        "        \n",
        "        # === 5a. 训练CNN模型 ===\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=CNN_TRAINING_EPOCHS,\n",
        "            batch_size=CNN_BATCH_SIZE,\n",
        "            validation_split=CNN_VALIDATION_SPLIT,\n",
        "            patience=CNN_EARLY_STOPPING_PATIENCE,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # === 5b. 评估模型性能 ===\n",
        "        if should_generate_plots:\n",
        "            cnn_model.visualize_training_history(\n",
        "                training_history,\n",
        "                save_path=os.path.join(results_dir, f\"cnn_1d_training_history_iter_{current_iteration_number}.png\")\n",
        "            )\n",
        "        \n",
        "        # 计算皮尔逊相关系数\n",
        "        current_pearson = calculate_pearson_correlation(cnn_model, data_manager)\n",
        "        history_pearson_all.append(current_pearson)\n",
        "        logger.info(f\"模型皮尔逊相关系数: {current_pearson:.4f}\")\n",
        "        \n",
        "        current_pearson_best_samples, min_y_best, max_y_best = calculate_pearson_on_best_samples(cnn_model, data_manager)\n",
        "        history_pearson_best_samples.append(current_pearson_best_samples)\n",
        "        logger.info(f\"最佳{min(60, data_manager.num_samples)}个样本皮尔逊相关系数: {current_pearson_best_samples:.4f}\")\n",
        "        \n",
        "        # === 5c. NTE搜索新候选点 ===\n",
        "        logger.info(\"使用NTE搜索新候选样本...\")\n",
        "        \n",
        "        # NTE Surrogate Wrapper\n",
        "        class NTESurrogateWrapper:\n",
        "            def __init__(self, actual_surrogate_model, data_manager_instance):\n",
        "                self.surrogate_model = actual_surrogate_model\n",
        "                self.dm = data_manager_instance\n",
        "                self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "            def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:\n",
        "                if X_batch_orig.shape[0] == 0:\n",
        "                    return np.array([])\n",
        "                if X_batch_orig.ndim == 1:\n",
        "                     X_batch_orig = X_batch_orig.reshape(1, -1)\n",
        "\n",
        "                try:\n",
        "                    pred_log = self.surrogate_model.predict(X_batch_orig)\n",
        "                    \n",
        "                    if X_batch_orig.shape[0] == 1:\n",
        "                        return pred_log[0] if isinstance(pred_log, np.ndarray) else pred_log\n",
        "                    else:\n",
        "                        return pred_log\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"NTESurrogateWrapper预测错误: {e}\")\n",
        "                    \n",
        "                    if X_batch_orig.shape[0] == 1:\n",
        "                        return np.log10(1e6 + 1.0)\n",
        "                    else:\n",
        "                        return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)\n",
        "        \n",
        "        nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)\n",
        "        \n",
        "        # 初始化NTE搜索器\n",
        "        nte_searcher = NTESearcher(\n",
        "            dimension=ROSENBROCK_DIMENSION, \n",
        "            domain=[(domain_min, domain_max) for _ in range(ROSENBROCK_DIMENSION)],\n",
        "            c0=NTE_C0,\n",
        "            rollout_rounds=NTE_ROLLOUT_ROUNDS,\n",
        "            validation_batch_size=VALIDATION_BATCH_SIZE\n",
        "        )\n",
        "        \n",
        "        current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "        \n",
        "        # 🔧 关键调试参数：计算动态探索因子\n",
        "        dynamic_exploration_factor = 0.8 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "        logger.info(f\"🔧 使用动态探索因子: {dynamic_exploration_factor:.2f} (基于连续无改进次数: {no_improvement_streak})\")\n",
        "        \n",
        "        try:\n",
        "            logger.info(\"=\" * 50)\n",
        "            logger.info(f\"开始第 {current_iteration_number} 轮NTE搜索...\")\n",
        "            logger.info(\"=\" * 50)\n",
        "            \n",
        "            # 执行NTE搜索\n",
        "            nte_search_result = nte_searcher.search(\n",
        "                surrogate_model=nte_model_wrapper,\n",
        "                current_best_state=current_best_x_orig if current_best_x_orig is not None else None,\n",
        "                min_y_observed=current_best_y_overall,\n",
        "                dynamic_exploration_factor=dynamic_exploration_factor,\n",
        "                all_states=data_manager.X_data if data_manager.num_samples > 0 else None,\n",
        "                all_values=data_manager.get_all_y_orig() if data_manager.num_samples > 0 else None,\n",
        "                previous_iteration_samples=previous_iteration_samples_x,\n",
        "                current_iteration=al_iteration,\n",
        "                return_ducb_trends=True\n",
        "            )\n",
        "            \n",
        "            nte_selected_samples_x, ducb_trends_data = nte_search_result\n",
        "            \n",
        "            if isinstance(nte_selected_samples_x, list):\n",
        "                nte_selected_samples_x = np.array(nte_selected_samples_x)\n",
        "            \n",
        "            logger.info(f\"NTE搜索完成，返回 {len(nte_selected_samples_x)} 个候选点\")\n",
        "            \n",
        "            if nte_selected_samples_x.shape[0] > 0:\n",
        "                logger.info(f\"评估 {nte_selected_samples_x.shape[0]} 个新候选样本...\")\n",
        "                nte_selected_samples_y_true = np.array([rosenbrock_function(x) for x in nte_selected_samples_x]).reshape(-1, 1)\n",
        "                \n",
        "                num_samples_before_adding = data_manager.num_samples\n",
        "                data_manager.add_samples(nte_selected_samples_x, nte_selected_samples_y_true, re_normalize=True, shuffle=True)\n",
        "                logger.info(f\"添加了 {nte_selected_samples_x.shape[0]} 个新样本。DataManager现在有 {data_manager.num_samples} 个样本。\")\n",
        "                \n",
        "                # 更新样本权重\n",
        "                if data_manager.num_samples > num_samples_before_adding:\n",
        "                    new_batch_weights = np.ones(data_manager.num_samples) \n",
        "                    if data_manager.last_batch_weights is not None and len(data_manager.last_batch_weights) == num_samples_before_adding:\n",
        "                        new_batch_weights[:num_samples_before_adding] = data_manager.last_batch_weights.numpy() \n",
        "                    \n",
        "                    new_nte_sample_weights = np.ones(nte_selected_samples_x.shape[0]) * 4.0\n",
        "                    new_batch_weights[num_samples_before_adding:] = new_nte_sample_weights\n",
        "                    data_manager.last_batch_weights = torch.tensor(new_batch_weights, dtype=torch.float32)\n",
        "                \n",
        "                # 保存迭代结果\n",
        "                if should_generate_plots:\n",
        "                    save_iteration_samples_to_csv(nte_selected_samples_x, nte_selected_samples_y_true, results_dir, current_iteration_number)\n",
        "                \n",
        "                previous_iteration_samples_x = nte_selected_samples_x.copy()\n",
        "                \n",
        "                # 更新最佳值\n",
        "                current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "                history_best_y.append(current_best_y_overall)\n",
        "                logger.info(f\"更新后全局最小函数值: {current_best_y_overall:.6e} @ 迭代 {current_iteration_number}\")\n",
        "                \n",
        "                history_nte_candidates_count.append(nte_selected_samples_x.shape[0])\n",
        "                \n",
        "                # 检查是否有改进\n",
        "                new_global_best_found = current_best_y_overall < previous_best_y\n",
        "                if new_global_best_found:\n",
        "                    logger.info(f\"🎉 发现新的全局最优解: {current_best_y_overall:.6e}\")\n",
        "                    \n",
        "            else:\n",
        "                logger.info(\"NTE未返回有效候选点\")\n",
        "                history_nte_candidates_count.append(0)\n",
        "                if history_best_y:\n",
        "                    history_best_y.append(history_best_y[-1])\n",
        "                else:\n",
        "                    history_best_y.append(current_best_y_overall)\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"NTE搜索过程中出错: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            nte_selected_samples_x = np.array([])\n",
        "        \n",
        "        # === 5d. 更新改进统计 ===\n",
        "        iteration_improved = False\n",
        "        if 'new_global_best_found' in locals() and new_global_best_found:\n",
        "            iteration_improved = True\n",
        "        elif len(nte_selected_samples_x) == 0:\n",
        "            iteration_improved = False\n",
        "        else:\n",
        "            iteration_improved = current_best_y_overall < previous_best_y\n",
        "            \n",
        "        if iteration_improved:\n",
        "            no_improvement_streak = 0\n",
        "            logger.info(f\"本次迭代有改进，重置 no_improvement_streak = 0\")\n",
        "        else:\n",
        "            no_improvement_streak += 1\n",
        "            logger.info(f\"本次迭代无改进，no_improvement_streak 增加到 {no_improvement_streak}\")\n",
        "            \n",
        "        previous_best_y = current_best_y_overall\n",
        "        \n",
        "        # === 5e. 生成可视化图表 ===\n",
        "        if should_generate_plots:\n",
        "            logger.info(f\"在迭代 {current_iteration_number} 生成可视化图表...\")\n",
        "            \n",
        "            # 全局最小值趋势图\n",
        "            if history_best_y:\n",
        "                try:\n",
        "                    plot_global_min_value_trend(history_iterations[:len(history_best_y)], history_best_y, results_dir, \n",
        "                                              filename=f\"global_min_iter_{current_iteration_number}.png\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"绘制全局最小值趋势图失败: {e}\")\n",
        "            \n",
        "            # 皮尔逊相关系数趋势图\n",
        "            if history_pearson_all:\n",
        "                try:\n",
        "                    plot_pearson_correlation_trend(history_iterations, history_pearson_all, results_dir,\n",
        "                                                 filename=f\"pearson_corr_iter_{current_iteration_number}.png\",\n",
        "                                                 title=\"Pearson Correlation Trend\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"绘制皮尔逊相关系数趋势图失败: {e}\")\n",
        "            \n",
        "            # 最佳样本皮尔逊相关系数趋势图\n",
        "            if history_pearson_best_samples:\n",
        "                try:\n",
        "                    plot_best_samples_pearson_trend(history_iterations, history_pearson_best_samples, results_dir,\n",
        "                                                  filename=f\"best_samples_pearson_iter_{current_iteration_number}.png\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"绘制最佳样本皮尔逊相关系数趋势图失败: {e}\")\n",
        "            \n",
        "            # NTE性能图\n",
        "            if history_nte_candidates_count:\n",
        "                try:\n",
        "                    plot_nte_performance(history_iterations, history_nte_candidates_count, results_dir,\n",
        "                                       filename=f\"nte_perf_iter_{current_iteration_number}.png\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"绘制NTE性能图失败: {e}\")\n",
        "        \n",
        "        # === 5f. 内存清理 ===\n",
        "        if current_iteration_number % 10 == 0:\n",
        "            data_manager.clear_cache()\n",
        "            gc.collect()\n",
        "            logger.info(f\"第{current_iteration_number}轮迭代: 已清理缓存\")\n",
        "    \n",
        "    # === 6. 生成最终结果 ===\n",
        "    logger.info(\"生成最终图表...\")\n",
        "    \n",
        "    # 最终全局最小值趋势图\n",
        "    if history_best_y:\n",
        "        try:\n",
        "            plot_global_min_value_trend(history_iterations[:len(history_best_y)], history_best_y, results_dir, \n",
        "                                      filename=\"global_min_value_trend_final.png\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"绘制最终全局最小值趋势图失败: {e}\")\n",
        "    \n",
        "    # 保存最终DataManager状态\n",
        "    final_data_path = os.path.join(results_dir, \"final_data_manager_state.npz\")\n",
        "    data_manager.save_data(final_data_path)\n",
        "    logger.info(f\"最终DataManager状态已保存到 {final_data_path}\")\n",
        "    \n",
        "    # 保存性能历史\n",
        "    try:\n",
        "        perf_data = {\n",
        "            'iterations': history_iterations,\n",
        "            'best_y_values': history_best_y,\n",
        "            'pearson_all': history_pearson_all,\n",
        "            'pearson_best_samples': history_pearson_best_samples,\n",
        "            'nte_candidates_count': history_nte_candidates_count\n",
        "        }\n",
        "        \n",
        "        # 确保所有列表长度一致\n",
        "        min_length = min(len(v) if isinstance(v, list) else 0 for v in perf_data.values())\n",
        "        for k, v in perf_data.items():\n",
        "            if isinstance(v, list) and len(v) > min_length:\n",
        "                perf_data[k] = v[:min_length]\n",
        "        \n",
        "        perf_csv_path = os.path.join(results_dir, \"performance_history.csv\")\n",
        "        pd.DataFrame(perf_data).to_csv(perf_csv_path, index=False)\n",
        "        logger.info(f\"性能历史已保存至: {perf_csv_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"保存性能历史失败: {e}\")\n",
        "    \n",
        "    logger.info(f\"🎉 DANTE训练完成！\")\n",
        "    logger.info(f\"最终最佳解: y = {current_best_y_overall:.4e}\")\n",
        "    logger.info(f\"所有结果已保存在: {results_dir}\")\n",
        "    \n",
        "    return results_dir, current_best_y_overall\n",
        "\n",
        "# 提供训练启动选项\n",
        "def start_dante_training():\n",
        "    \"\"\"启动DANTE训练的便捷函数\"\"\"\n",
        "    print(\"🚀 准备启动DANTE训练...\")\n",
        "    print(\"⚠️  注意：完整训练可能需要几个小时时间\")\n",
        "    print(\"📊 建议先运行环境验证确保一切正常\")\n",
        "    \n",
        "    return main_dante_training()\n",
        "\n",
        "# 显示使用说明\n",
        "print(\"📋 DANTE训练使用说明:\")\n",
        "print(\"1. 首先运行 Cell 7 进行环境验证\")\n",
        "print(\"2. 环境验证通过后，运行以下代码开始训练:\")\n",
        "print(\"   results_dir, best_value = start_dante_training()\")\n",
        "print(\"3. 训练过程中的所有结果将保存到 /kaggle/working/dante_results_[timestamp]/ 目录\")\n",
        "print(\"4. 可以通过修改 main_dante_training() 函数中的参数来调整训练配置\")\n",
        "print(\"\\n🔧 关键调试参数:\")\n",
        "print(\"   - dynamic_exploration_factor: 0.8 + 1 * min(20, 2 ** no_improvement_streak)\")\n",
        "print(\"   - N_AL_ITERATIONS: 当前设置为100次迭代（可根据需要调整）\")\n",
        "print(\"   - NTE_ROLLOUT_ROUNDS: NTE内部搜索轮次\")\n",
        "print(\"   - VALIDATION_BATCH_SIZE: 每次迭代选择的样本数量\")\n",
        "print(\"\\n✅ 准备就绪！运行环境验证后即可开始训练。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: 环境配置验证与功能测试\n",
        "print(\"=\"*60)\n",
        "print(\"DANTE Kaggle环境配置验证与功能测试\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. 系统环境检查\n",
        "print(\"\\n1. 系统环境检查:\")\n",
        "print(f\"   Python版本: {sys.version}\")\n",
        "print(f\"   TensorFlow版本: {tf.__version__}\")\n",
        "print(f\"   PyTorch版本: {torch.__version__}\")\n",
        "print(f\"   Numpy版本: {np.__version__}\")\n",
        "print(f\"   Pandas版本: {pd.__version__}\")\n",
        "\n",
        "# 2. GPU/计算资源检查\n",
        "print(\"\\n2. 计算资源检查:\")\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(f\"   TensorFlow检测到 {len(gpus)} 个GPU:\")\n",
        "        for i, gpu in enumerate(gpus):\n",
        "            print(f\"     GPU {i}: {gpu}\")\n",
        "            # 配置GPU内存增长\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    else:\n",
        "        print(\"   未检测到GPU，将使用CPU训练\")\n",
        "    \n",
        "    print(f\"   PyTorch CUDA可用: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"   PyTorch GPU设备: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   CUDA版本: {torch.version.cuda}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   GPU检查时出错: {e}\")\n",
        "\n",
        "# 3. 数据路径检查\n",
        "print(\"\\n3. 数据路径检查:\")\n",
        "input_paths = [\n",
        "    \"/kaggle/input/rosenbrock-dante-data\",\n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_train.npy\",\n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_train.npy\",\n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_test.npy\", \n",
        "    \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_test.npy\"\n",
        "]\n",
        "\n",
        "for path in input_paths:\n",
        "    exists = os.path.exists(path)\n",
        "    print(f\"   {path}: {'✓存在' if exists else '✗缺失'}\")\n",
        "    \n",
        "# 4. 核心组件实例化测试\n",
        "print(\"\\n4. 核心组件实例化测试:\")\n",
        "\n",
        "try:\n",
        "    # 测试Rosenbrock函数\n",
        "    test_x = np.array([1.0] * 20)  # 全局最优解\n",
        "    test_result = rosenbrock_function(test_x)\n",
        "    print(f\"   Rosenbrock函数测试: f([1]*20) = {test_result:.2e} {'✓正确' if abs(test_result) < 1e-10 else '✗错误'}\")\n",
        "    \n",
        "    # 生成测试数据\n",
        "    print(\"   生成测试数据集...\")\n",
        "    test_X = np.random.uniform(-2.048, 2.048, size=(100, 20))\n",
        "    test_y = np.array([rosenbrock_function(x) for x in test_X]).reshape(-1, 1)\n",
        "    print(f\"   测试数据: X.shape={test_X.shape}, y.shape={test_y.shape}\")\n",
        "    print(f\"   y值范围: [{test_y.min():.2e}, {test_y.max():.2e}]\")\n",
        "    \n",
        "    # 测试DataManager\n",
        "    print(\"   测试DataManager...\")\n",
        "    test_dm = DataManager(test_X[:50], test_y[:50], dimension=20)\n",
        "    print(f\"   DataManager初始化成功: {test_dm.num_samples}个样本\")\n",
        "    \n",
        "    # 测试性能优化器\n",
        "    print(\"   测试性能优化器...\")\n",
        "    test_optimizer = PerformanceOptimizer()\n",
        "    test_optimizer.configure_tensorflow_performance()\n",
        "    print(\"   性能优化器配置完成 ✓\")\n",
        "    \n",
        "    # 测试CNN1DSurrogate\n",
        "    print(\"   测试CNN1DSurrogate...\")\n",
        "    test_cnn = CNN1DSurrogate(input_dim=20)\n",
        "    print(\"   CNN模型初始化成功 ✓\")\n",
        "    \n",
        "    # 快速训练测试（1个epoch）\n",
        "    print(\"   快速训练测试（1个epoch）...\")\n",
        "    training_history = test_cnn.train(\n",
        "        data_manager=test_dm,\n",
        "        epochs=1,\n",
        "        batch_size=16,\n",
        "        validation_split=0.2,\n",
        "        patience=10,\n",
        "        verbose=0\n",
        "    )\n",
        "    print(\"   快速训练完成 ✓\")\n",
        "    \n",
        "    # 测试预测\n",
        "    test_pred = test_cnn.predict(test_X[:5])\n",
        "    print(f\"   预测测试: 输入5个样本，输出shape={test_pred.shape}\")\n",
        "    print(f\"   预测值范围: [{test_pred.min():.2e}, {test_pred.max():.2e}]\")\n",
        "    \n",
        "    # 测试NTE搜索器初始化\n",
        "    print(\"   测试NTE搜索器...\")\n",
        "    test_nte = NTESearcher(\n",
        "        dimension=20,\n",
        "        domain=[(-2.048, 2.048)] * 20,\n",
        "        c0=1.0,\n",
        "        rollout_rounds=10,  # 减少轮数用于测试\n",
        "        validation_batch_size=5\n",
        "    )\n",
        "    print(\"   NTE搜索器初始化成功 ✓\")\n",
        "    \n",
        "    print(\"\\n✓ 所有核心组件测试通过！\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ 组件测试失败: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# 5. 内存和存储检查\n",
        "print(\"\\n5. 资源检查:\")\n",
        "try:\n",
        "    # 检查可用内存\n",
        "    import psutil\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"   可用内存: {memory.available / (1024**3):.1f} GB / {memory.total / (1024**3):.1f} GB\")\n",
        "    \n",
        "    # 检查磁盘空间\n",
        "    disk = psutil.disk_usage('/kaggle/working')\n",
        "    print(f\"   可用磁盘: {disk.free / (1024**3):.1f} GB / {disk.total / (1024**3):.1f} GB\")\n",
        "    \n",
        "    # 检查CPU\n",
        "    print(f\"   CPU核心数: {psutil.cpu_count()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   资源检查出错: {e}\")\n",
        "\n",
        "# 6. 生成配置摘要\n",
        "print(\"\\n6. 运行配置摘要:\")\n",
        "print(\"   DANTE优化配置:\")\n",
        "print(\"   - 目标函数: 20维Rosenbrock函数\")\n",
        "print(\"   - 初始样本数: 800\")\n",
        "print(\"   - 主动学习迭代数: 400\")  \n",
        "print(\"   - 每次迭代批量大小: 20\")\n",
        "print(\"   - CNN训练epochs: 200\")\n",
        "print(\"   - NTE rollout轮数: 100\")\n",
        "print(\"   - 动态探索因子: 0.8 + 1 * min(20, 2^no_improvement_streak)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"环境验证完成！准备开始DANTE训练...\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: DANTE主训练循环\n",
        "def run_dante_optimization():\n",
        "    \"\"\"运行DANTE优化主循环\"\"\"\n",
        "    print(\"开始DANTE优化...\")\n",
        "    \n",
        "    # ===== 配置参数 =====\n",
        "    ROSENBROCK_DIMENSION = 20\n",
        "    DOMAIN_MIN, DOMAIN_MAX = -2.048, 2.048\n",
        "    ROSENBROCK_DOMAIN = [(DOMAIN_MIN, DOMAIN_MAX)] * ROSENBROCK_DIMENSION\n",
        "    \n",
        "    # 训练参数 - 可调整用于调试动态探索因子\n",
        "    INITIAL_DATA_SIZE = 800\n",
        "    N_AL_ITERATIONS = 400\n",
        "    CNN_TRAINING_EPOCHS = 200\n",
        "    CNN_EARLY_STOPPING_PATIENCE = 30\n",
        "    CNN_VALIDATION_SPLIT = 0.2\n",
        "    CNN_BATCH_SIZE = 32\n",
        "    NTE_C0 = 1\n",
        "    NTE_ROLLOUT_ROUNDS = 100\n",
        "    VALIDATION_BATCH_SIZE = 20\n",
        "    \n",
        "    # 可视化间隔（减少输出量）\n",
        "    viz_interval = 10\n",
        "    \n",
        "    print(f\"配置参数:\")\n",
        "    print(f\"- 维度: {ROSENBROCK_DIMENSION}D\")\n",
        "    print(f\"- 初始样本: {INITIAL_DATA_SIZE}\")\n",
        "    print(f\"- AL迭代: {N_AL_ITERATIONS}\")\n",
        "    print(f\"- 每批样本: {VALIDATION_BATCH_SIZE}\")\n",
        "    print(f\"- CNN训练epochs: {CNN_TRAINING_EPOCHS}\")\n",
        "    print(f\"- NTE rollout轮数: {NTE_ROLLOUT_ROUNDS}\")\n",
        "    \n",
        "    # ===== 加载数据 =====\n",
        "    print(\"\\n加载数据...\")\n",
        "    try:\n",
        "        # 尝试从Kaggle数据集加载\n",
        "        X_train_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_train.npy\"\n",
        "        y_train_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_train.npy\"\n",
        "        X_test_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_x_test.npy\"\n",
        "        y_test_path = \"/kaggle/input/rosenbrock-dante-data/Rosenbrock_y_test.npy\"\n",
        "        \n",
        "        if os.path.exists(X_train_path) and os.path.exists(y_train_path):\n",
        "            initial_X = np.load(X_train_path)\n",
        "            initial_y = np.load(y_train_path)\n",
        "            if initial_y.ndim == 1:\n",
        "                initial_y = initial_y.reshape(-1, 1)\n",
        "            print(f\"从Kaggle数据集加载训练数据: {initial_X.shape}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"训练数据不存在\")\n",
        "            \n",
        "        if os.path.exists(X_test_path) and os.path.exists(y_test_path):\n",
        "            X_test = np.load(X_test_path)\n",
        "            y_test = np.load(y_test_path)\n",
        "            if y_test.ndim == 1:\n",
        "                y_test = y_test.reshape(-1, 1)\n",
        "            print(f\"从Kaggle数据集加载测试数据: {X_test.shape}\")\n",
        "        else:\n",
        "            X_test, y_test = None, None\n",
        "            print(\"测试数据不存在，将使用训练数据的一部分\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"数据加载失败: {e}\")\n",
        "        print(\"生成随机数据...\")\n",
        "        initial_X = np.random.uniform(DOMAIN_MIN, DOMAIN_MAX, size=(INITIAL_DATA_SIZE, ROSENBROCK_DIMENSION))\n",
        "        initial_y = np.array([rosenbrock_function(x) for x in initial_X]).reshape(-1, 1)\n",
        "        \n",
        "        # 生成测试集\n",
        "        test_size = 200\n",
        "        X_test = np.random.uniform(DOMAIN_MIN, DOMAIN_MAX, size=(test_size, ROSENBROCK_DIMENSION))\n",
        "        y_test = np.array([rosenbrock_function(x) for x in X_test]).reshape(-1, 1)\n",
        "        print(f\"生成随机数据: 训练{initial_X.shape}, 测试{X_test.shape}\")\n",
        "    \n",
        "    # 确保有足够的初始样本\n",
        "    if initial_X.shape[0] < INITIAL_DATA_SIZE:\n",
        "        additional_needed = INITIAL_DATA_SIZE - initial_X.shape[0]\n",
        "        additional_X = np.random.uniform(DOMAIN_MIN, DOMAIN_MAX, size=(additional_needed, ROSENBROCK_DIMENSION))\n",
        "        additional_y = np.array([rosenbrock_function(x) for x in additional_X]).reshape(-1, 1)\n",
        "        initial_X = np.vstack((initial_X, additional_X))\n",
        "        initial_y = np.vstack((initial_y, additional_y))\n",
        "        print(f\"补充{additional_needed}个随机样本，总计{initial_X.shape[0]}个\")\n",
        "    \n",
        "    # ===== 初始化核心组件 =====\n",
        "    print(\"\\n初始化组件...\")\n",
        "    \n",
        "    # 数据管理器\n",
        "    data_manager = DataManager(initial_X, initial_y, dimension=ROSENBROCK_DIMENSION)\n",
        "    print(f\"DataManager初始化: {data_manager.num_samples}个样本\")\n",
        "    \n",
        "    # 性能优化器\n",
        "    performance_optimizer = PerformanceOptimizer()\n",
        "    performance_optimizer.configure_tensorflow_performance()\n",
        "    \n",
        "    # CNN替代模型\n",
        "    cnn_model = CNN1DSurrogate(input_dim=ROSENBROCK_DIMENSION)\n",
        "    print(\"CNN1DSurrogate模型初始化完成\")\n",
        "    \n",
        "    # NTE搜索器\n",
        "    nte_searcher = NTESearcher(\n",
        "        dimension=ROSENBROCK_DIMENSION,\n",
        "        domain=ROSENBROCK_DOMAIN,\n",
        "        c0=NTE_C0,\n",
        "        rollout_rounds=NTE_ROLLOUT_ROUNDS,\n",
        "        validation_batch_size=VALIDATION_BATCH_SIZE\n",
        "    )\n",
        "    print(\"NTESearcher初始化完成\")\n",
        "    \n",
        "    # ===== 历史记录变量 =====\n",
        "    history_iterations = []\n",
        "    history_best_y = []\n",
        "    history_pearson_all = []\n",
        "    history_pearson_best_samples = []\n",
        "    history_nte_candidates_count = []\n",
        "    cnn_performance_history = []\n",
        "    \n",
        "    # 优化相关变量\n",
        "    no_improvement_streak = 0\n",
        "    previous_best_y = float('inf')\n",
        "    previous_iteration_samples_x = None\n",
        "    \n",
        "    # 获取初始最优值\n",
        "    current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "    print(f\"初始最优值: {current_best_y_overall:.6e}\")\n",
        "    \n",
        "    # ===== 主训练循环 =====\n",
        "    print(f\"\\n开始{N_AL_ITERATIONS}次主动学习迭代...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for al_iteration in range(N_AL_ITERATIONS):\n",
        "        current_iteration_number = al_iteration + 1\n",
        "        history_iterations.append(current_iteration_number)\n",
        "        \n",
        "        # 决定是否生成详细可视化\n",
        "        should_generate_plots = ((current_iteration_number % viz_interval == 0) or \n",
        "                                (current_iteration_number == N_AL_ITERATIONS))\n",
        "        \n",
        "        if current_iteration_number % 50 == 0 or current_iteration_number <= 5:\n",
        "            print(f\"\\n--- 迭代 {current_iteration_number}/{N_AL_ITERATIONS} ---\")\n",
        "        \n",
        "        # === 1. 训练CNN替代模型 ===\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=CNN_TRAINING_EPOCHS,\n",
        "            batch_size=CNN_BATCH_SIZE,\n",
        "            validation_split=CNN_VALIDATION_SPLIT,\n",
        "            patience=CNN_EARLY_STOPPING_PATIENCE,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # === 2. 评估模型性能 ===\n",
        "        if X_test is not None:\n",
        "            current_pearson = calculate_pearson_on_test_set(cnn_model, X_test, y_test, data_manager)\n",
        "        else:\n",
        "            current_pearson = calculate_pearson_correlation(cnn_model, data_manager)\n",
        "        \n",
        "        history_pearson_all.append(current_pearson)\n",
        "        \n",
        "        # 计算最佳样本上的皮尔逊相关系数\n",
        "        current_pearson_best_samples, _, _ = calculate_pearson_on_best_samples(cnn_model, data_manager)\n",
        "        history_pearson_best_samples.append(current_pearson_best_samples)\n",
        "        \n",
        "        # === 3. NTE搜索新候选点 ===\n",
        "        # 计算动态探索因子（关键调试参数）\n",
        "        dynamic_exploration_factor = 0.8 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "        \n",
        "        if current_iteration_number % 50 == 0 or current_iteration_number <= 5:\n",
        "            print(f\"动态探索因子: {dynamic_exploration_factor:.2f} (无改进连续次数: {no_improvement_streak})\")\n",
        "        \n",
        "        # NTE搜索包装器\n",
        "        class NTESurrogateWrapper:\n",
        "            def __init__(self, model, dm):\n",
        "                self.model = model\n",
        "                self.dm = dm\n",
        "            \n",
        "            def predict(self, X_batch):\n",
        "                if X_batch.shape[0] == 0:\n",
        "                    return np.array([])\n",
        "                if X_batch.ndim == 1:\n",
        "                    X_batch = X_batch.reshape(1, -1)\n",
        "                \n",
        "                try:\n",
        "                    pred_log = self.model.predict(X_batch)\n",
        "                    return pred_log[0] if X_batch.shape[0] == 1 else pred_log\n",
        "                except Exception as e:\n",
        "                    print(f\"预测错误: {e}\")\n",
        "                    return np.log10(1e6 + 1.0) if X_batch.shape[0] == 1 else np.ones(X_batch.shape[0]) * np.log10(1e6 + 1.0)\n",
        "        \n",
        "        nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)\n",
        "        \n",
        "        try:\n",
        "            # 执行NTE搜索\n",
        "            nte_search_result = nte_searcher.search(\n",
        "                surrogate_model=nte_model_wrapper,\n",
        "                current_best_state=current_best_x_orig,\n",
        "                min_y_observed=current_best_y_overall,\n",
        "                dynamic_exploration_factor=dynamic_exploration_factor,\n",
        "                all_states=data_manager.X_data,\n",
        "                all_values=data_manager.get_all_y_orig(),\n",
        "                previous_iteration_samples=previous_iteration_samples_x,\n",
        "                current_iteration=al_iteration,\n",
        "                return_ducb_trends=True\n",
        "            )\n",
        "            \n",
        "            nte_selected_samples_x, ducb_trends_data = nte_search_result\n",
        "            \n",
        "            if isinstance(nte_selected_samples_x, list):\n",
        "                nte_selected_samples_x = np.array(nte_selected_samples_x)\n",
        "            \n",
        "            if nte_selected_samples_x.shape[0] > 0:\n",
        "                # 评估新样本\n",
        "                nte_selected_samples_y_true = np.array([rosenbrock_function(x) for x in nte_selected_samples_x]).reshape(-1, 1)\n",
        "                \n",
        "                # 添加到数据管理器\n",
        "                data_manager.add_samples(nte_selected_samples_x, nte_selected_samples_y_true, re_normalize=True, shuffle=True)\n",
        "                \n",
        "                # 更新权重（为新样本设置较高权重）\n",
        "                num_new_samples = nte_selected_samples_x.shape[0]\n",
        "                new_weights = np.ones(data_manager.num_samples)\n",
        "                new_weights[-num_new_samples:] = 4.0  # 新样本权重为4.0\n",
        "                data_manager.last_batch_weights = torch.tensor(new_weights, dtype=torch.float32)\n",
        "                \n",
        "                history_nte_candidates_count.append(num_new_samples)\n",
        "                previous_iteration_samples_x = nte_selected_samples_x.copy()\n",
        "                \n",
        "                # 更新最优值\n",
        "                current_best_x_orig, current_best_y_overall = data_manager.get_current_best()\n",
        "                history_best_y.append(current_best_y_overall)\n",
        "                \n",
        "                # 检查是否有改进\n",
        "                iteration_improved = current_best_y_overall < previous_best_y\n",
        "                if iteration_improved:\n",
        "                    no_improvement_streak = 0\n",
        "                    if current_iteration_number % 10 == 0 or current_iteration_number <= 10:\n",
        "                        print(f\"发现更优解: {current_best_y_overall:.6e}\")\n",
        "                else:\n",
        "                    no_improvement_streak += 1\n",
        "                \n",
        "                previous_best_y = current_best_y_overall\n",
        "                \n",
        "            else:\n",
        "                history_nte_candidates_count.append(0)\n",
        "                history_best_y.append(history_best_y[-1] if history_best_y else current_best_y_overall)\n",
        "                no_improvement_streak += 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"NTE搜索失败: {e}\")\n",
        "            history_nte_candidates_count.append(0)\n",
        "            history_best_y.append(history_best_y[-1] if history_best_y else current_best_y_overall)\n",
        "            no_improvement_streak += 1\n",
        "        \n",
        "        # === 4. 性能记录和可视化 ===\n",
        "        perf_metrics = {\n",
        "            'iteration': current_iteration_number,\n",
        "            'best_y': current_best_y_overall,\n",
        "            'pearson_test': current_pearson,\n",
        "            'pearson_best_samples': current_pearson_best_samples,\n",
        "            'num_samples': data_manager.num_samples,\n",
        "            'no_improvement_streak': no_improvement_streak,\n",
        "            'dynamic_exploration_factor': dynamic_exploration_factor\n",
        "        }\n",
        "        cnn_performance_history.append(perf_metrics)\n",
        "        \n",
        "        # 定期可视化\n",
        "        if should_generate_plots:\n",
        "            print(f\"生成第{current_iteration_number}轮可视化图表...\")\n",
        "            \n",
        "            # 全局最小值趋势\n",
        "            plot_global_min_value_trend(\n",
        "                history_iterations[:len(history_best_y)], \n",
        "                history_best_y, \n",
        "                \"/kaggle/working\", \n",
        "                filename=f\"global_min_iter_{current_iteration_number}.png\"\n",
        "            )\n",
        "            \n",
        "            # 皮尔逊相关系数趋势\n",
        "            plot_pearson_correlation_trend(\n",
        "                history_iterations, \n",
        "                history_pearson_all, \n",
        "                \"/kaggle/working\",\n",
        "                filename=f\"pearson_trend_iter_{current_iteration_number}.png\",\n",
        "                title=\"Test Set Pearson Correlation\"\n",
        "            )\n",
        "            \n",
        "            # 最佳样本皮尔逊趋势\n",
        "            plot_best_samples_pearson_trend(\n",
        "                history_iterations, \n",
        "                history_pearson_best_samples, \n",
        "                \"/kaggle/working\",\n",
        "                filename=f\"best_samples_pearson_iter_{current_iteration_number}.png\"\n",
        "            )\n",
        "            \n",
        "            # NTE性能\n",
        "            plot_nte_performance(\n",
        "                history_iterations, \n",
        "                history_nte_candidates_count, \n",
        "                \"/kaggle/working\",\n",
        "                filename=f\"nte_performance_iter_{current_iteration_number}.png\"\n",
        "            )\n",
        "        \n",
        "        # 内存清理\n",
        "        if current_iteration_number % 10 == 0:\n",
        "            data_manager.clear_cache()\n",
        "            performance_optimizer.clear_memory_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "        \n",
        "        # 进度报告\n",
        "        if current_iteration_number % 50 == 0:\n",
        "            print(f\"进度: {current_iteration_number}/{N_AL_ITERATIONS} ({100*current_iteration_number/N_AL_ITERATIONS:.1f}%)\")\n",
        "            print(f\"当前最优: {current_best_y_overall:.6e}\")\n",
        "            print(f\"样本总数: {data_manager.num_samples}\")\n",
        "            print(f\"无改进连续次数: {no_improvement_streak}\")\n",
        "    \n",
        "    # ===== 训练完成 =====\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DANTE训练完成!\")\n",
        "    print(f\"最终最优值: {current_best_y_overall:.6e}\")\n",
        "    print(f\"最优解位置: {current_best_x_orig}\")\n",
        "    print(f\"总样本数: {data_manager.num_samples}\")\n",
        "    print(f\"训练迭代数: {N_AL_ITERATIONS}\")\n",
        "    \n",
        "    # 保存最终结果\n",
        "    results = {\n",
        "        'final_best_value': current_best_y_overall,\n",
        "        'final_best_x': current_best_x_orig,\n",
        "        'total_samples': data_manager.num_samples,\n",
        "        'iterations': N_AL_ITERATIONS,\n",
        "        'history_best_y': history_best_y,\n",
        "        'history_pearson': history_pearson_all,\n",
        "        'cnn_performance_history': cnn_performance_history\n",
        "    }\n",
        "    \n",
        "    # 保存为numpy文件\n",
        "    np.savez(\"/kaggle/working/dante_results.npz\", **results)\n",
        "    print(\"结果已保存到 /kaggle/working/dante_results.npz\")\n",
        "    \n",
        "    # 生成最终可视化\n",
        "    print(\"\\n生成最终可视化图表...\")\n",
        "    plot_global_min_value_trend(\n",
        "        history_iterations[:len(history_best_y)], \n",
        "        history_best_y, \n",
        "        \"/kaggle/working\", \n",
        "        filename=\"final_global_min_trend.png\"\n",
        "    )\n",
        "    \n",
        "    plot_pearson_correlation_trend(\n",
        "        history_iterations, \n",
        "        history_pearson_all, \n",
        "        \"/kaggle/working\",\n",
        "        filename=\"final_pearson_trend.png\",\n",
        "        title=\"Final Pearson Correlation Trend\"\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 定义辅助函数\n",
        "def calculate_pearson_on_test_set(model, X_test, y_test_orig, data_manager):\n",
        "    \"\"\"计算测试集上的皮尔逊相关系数\"\"\"\n",
        "    if X_test.shape[0] < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(X_test)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2:\n",
        "            return 0.0\n",
        "            \n",
        "        from scipy.stats import pearsonr\n",
        "        correlation, _ = pearsonr(predictions_orig, y_test_orig.flatten())\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"计算测试集皮尔逊相关系数失败: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_best_samples(model, data_manager, n_best=60):\n",
        "    \"\"\"计算最佳样本上的皮尔逊相关系数\"\"\"\n",
        "    all_X = data_manager.get_all_x_orig()\n",
        "    all_y_orig = data_manager.get_all_y_orig()\n",
        "    \n",
        "    if all_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "    \n",
        "    # 按y值排序，取最小的n_best个\n",
        "    sorted_indices = np.argsort(all_y_orig.flatten())\n",
        "    best_indices = sorted_indices[:n_best]\n",
        "    \n",
        "    sorted_X = all_X[best_indices]\n",
        "    sorted_y_orig = all_y_orig[best_indices]\n",
        "    \n",
        "    if sorted_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "    \n",
        "    min_y = np.min(sorted_y_orig)\n",
        "    max_y = np.max(sorted_y_orig)\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(sorted_X)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if np.any(np.isinf(predictions_orig)) or np.any(np.isnan(predictions_orig)):\n",
        "            return 0.0, min_y, max_y\n",
        "        \n",
        "        from scipy.stats import pearsonr\n",
        "        correlation, _ = pearsonr(predictions_orig, sorted_y_orig.flatten())\n",
        "        return (correlation if not np.isnan(correlation) else 0.0), min_y, max_y\n",
        "        \n",
        "    except Exception as e:\n",
        "        return 0.0, min_y, max_y\n",
        "\n",
        "# 运行说明\n",
        "print(\"DANTE训练循环已定义完成!\")\n",
        "print(\"\\n要开始训练，请运行:\")\n",
        "print(\"results = run_dante_optimization()\")\n",
        "print(\"\\n注意: 完整训练可能需要8-12小时，建议在P100 GPU环境下运行\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: 环境验证与完整性检查\n",
        "print(\"=\"*80)\n",
        "print(\"DANTE-Kaggle 环境验证与完整性检查\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. 硬件环境验证\n",
        "print(\"\\n1. 硬件环境验证\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# GPU检查\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"✅ GPU可用: {gpu_name}\")\n",
        "    print(f\"   显存: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    # 测试GPU内存分配\n",
        "    try:\n",
        "        test_tensor = torch.ones(1000, 1000).cuda()\n",
        "        del test_tensor\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✅ GPU内存分配测试通过\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ GPU内存分配失败: {e}\")\n",
        "else:\n",
        "    print(\"❌ GPU不可用，将使用CPU\")\n",
        "\n",
        "# TensorFlow GPU检查\n",
        "tf_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if tf_gpus:\n",
        "    print(f\"✅ TensorFlow GPU可用: {len(tf_gpus)}个设备\")\n",
        "else:\n",
        "    print(\"❌ TensorFlow未检测到GPU\")\n",
        "\n",
        "# 2. 数据验证\n",
        "print(\"\\n2. 数据完整性验证\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "data_status = {\"train_x\": False, \"train_y\": False, \"test_x\": False, \"test_y\": False}\n",
        "data_shapes = {}\n",
        "\n",
        "for key, filename in [(\"train_x\", \"Rosenbrock_x_train.npy\"), \n",
        "                      (\"train_y\", \"Rosenbrock_y_train.npy\"),\n",
        "                      (\"test_x\", \"Rosenbrock_x_test.npy\"), \n",
        "                      (\"test_y\", \"Rosenbrock_y_test.npy\")]:\n",
        "    filepath = os.path.join(DATA_PATH, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        try:\n",
        "            data = np.load(filepath)\n",
        "            data_status[key] = True\n",
        "            data_shapes[key] = data.shape\n",
        "            print(f\"✅ {filename}: {data.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ {filename}: 加载失败 - {e}\")\n",
        "    else:\n",
        "        print(f\"❌ {filename}: 文件不存在\")\n",
        "\n",
        "# 验证数据维度一致性\n",
        "if data_status[\"train_x\"] and data_status[\"train_y\"]:\n",
        "    if data_shapes[\"train_x\"][0] == data_shapes[\"train_y\"][0]:\n",
        "        print(\"✅ 训练数据X,Y样本数一致\")\n",
        "    else:\n",
        "        print(f\"❌ 训练数据维度不匹配: X={data_shapes['train_x']}, Y={data_shapes['train_y']}\")\n",
        "\n",
        "# 3. 核心功能验证\n",
        "print(\"\\n3. 核心功能模块验证\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 测试Rosenbrock函数\n",
        "try:\n",
        "    test_x = np.random.uniform(-2, 2, 20)\n",
        "    result = rosenbrock_function(test_x)\n",
        "    print(f\"✅ Rosenbrock函数: 输入维度{test_x.shape} → 输出{result:.4e}\")\n",
        "    \n",
        "    # 批量测试\n",
        "    batch_x = np.random.uniform(-2, 2, (5, 20))\n",
        "    batch_result = rosenbrock_function(batch_x)\n",
        "    print(f\"✅ Rosenbrock批量函数: 输入{batch_x.shape} → 输出{batch_result.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Rosenbrock函数测试失败: {e}\")\n",
        "\n",
        "# 测试safe_power10函数\n",
        "try:\n",
        "    test_log = np.array([1.0, 2.0, 3.0])\n",
        "    result = safe_power10(test_log)\n",
        "    print(f\"✅ safe_power10函数: 输入{test_log} → 输出{result}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ safe_power10函数测试失败: {e}\")\n",
        "\n",
        "# 4. 内存状态检查\n",
        "print(\"\\n4. 系统资源检查\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 内存检查\n",
        "try:\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    available_gb = memory_info.available / 1e9\n",
        "    total_gb = memory_info.total / 1e9\n",
        "    print(f\"✅ 系统内存: {available_gb:.1f}GB 可用 / {total_gb:.1f}GB 总计\")\n",
        "    \n",
        "    if available_gb < 8:\n",
        "        print(\"⚠️  可用内存不足8GB，可能影响训练性能\")\n",
        "    else:\n",
        "        print(\"✅ 内存充足，可支持完整训练\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 内存检查失败: {e}\")\n",
        "\n",
        "# 5. 模块导入验证\n",
        "print(\"\\n5. 关键模块验证\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "modules_to_test = [\n",
        "    (\"numpy\", np),\n",
        "    (\"pandas\", pd), \n",
        "    (\"torch\", torch),\n",
        "    (\"tensorflow\", tf),\n",
        "    (\"matplotlib\", plt),\n",
        "    (\"scipy.stats\", pearsonr),\n",
        "]\n",
        "\n",
        "for name, module in modules_to_test:\n",
        "    try:\n",
        "        # 简单功能测试\n",
        "        if name == \"numpy\":\n",
        "            test_array = module.array([1, 2, 3])\n",
        "        elif name == \"torch\":\n",
        "            test_tensor = module.tensor([1, 2, 3])\n",
        "        elif name == \"tensorflow\":\n",
        "            test_tf = module.constant([1, 2, 3])\n",
        "        print(f\"✅ {name}: 导入成功\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {name}: 测试失败 - {e}\")\n",
        "\n",
        "# 6. 预期性能估算\n",
        "print(\"\\n6. 训练性能预估\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "ROSENBROCK_DIMENSION = 20\n",
        "INITIAL_DATA_SIZE = 800\n",
        "N_AL_ITERATIONS = 400\n",
        "CNN_TRAINING_EPOCHS = 200\n",
        "\n",
        "estimated_time_per_cnn = 30  # 秒\n",
        "estimated_time_per_nte = 10  # 秒\n",
        "total_estimated_time = N_AL_ITERATIONS * (estimated_time_per_cnn + estimated_time_per_nte)\n",
        "\n",
        "print(f\"配置参数:\")\n",
        "print(f\"  - 维度: {ROSENBROCK_DIMENSION}D\")\n",
        "print(f\"  - 初始样本: {INITIAL_DATA_SIZE}\")\n",
        "print(f\"  - AL迭代: {N_AL_ITERATIONS}\")\n",
        "print(f\"  - CNN轮数: {CNN_TRAINING_EPOCHS}\")\n",
        "print(f\"预估训练时间: {total_estimated_time/3600:.1f} 小时\")\n",
        "\n",
        "if total_estimated_time > 43200:  # 12小时\n",
        "    print(\"⚠️  预估时间超过12小时，建议在P100环境下运行\")\n",
        "else:\n",
        "    print(\"✅ 预估时间合理，可在Kaggle环境完成\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"环境验证完成！如果所有项目都显示✅，则可以开始训练。\")\n",
        "print(\"如果有❌项目，请检查对应的环境配置。\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: NTE Searcher 核心算法方法\n",
        "\n",
        "def _calculate_ducb(self, node, parent_total_visits, c_rho, current_c0):\n",
        "    \"\"\"计算DUCB值\"\"\"\n",
        "    if node.visit_count < 0:\n",
        "        raise ValueError(\"Node visit count cannot be negative.\")\n",
        "    if parent_total_visits <= 0:\n",
        "        parent_total_visits = 1\n",
        "\n",
        "    exploitation_term = -node.value_pred\n",
        "    n = node.visit_count\n",
        "    N = parent_total_visits\n",
        "    \n",
        "    log_term = math.log(max(N, 1))\n",
        "    exploration_term = current_c0 * c_rho * math.sqrt((2 * log_term) / (n + 1))\n",
        "\n",
        "    return exploitation_term + exploration_term\n",
        "\n",
        "def _calculate_ducb_with_global_awareness(self, node, parent_total_visits, \n",
        "                                        c_rho, current_c0,\n",
        "                                        global_access_record, nte_id, \n",
        "                                        state_tuple):\n",
        "    \"\"\"计算考虑全局访问情况的DUCB值\"\"\"\n",
        "    if node.visit_count < 0:\n",
        "        raise ValueError(\"Node visit count cannot be negative.\")\n",
        "    if parent_total_visits <= 0:\n",
        "        parent_total_visits = 1\n",
        "\n",
        "    exploitation_term = -node.value_pred\n",
        "    n = node.visit_count\n",
        "    \n",
        "    # 检查是否是重复访问的节点\n",
        "    if (state_tuple in global_access_record and \n",
        "        len(global_access_record[state_tuple]['nte_visits']) > 1):\n",
        "        # 这是一个被多个NTE访问过的节点\n",
        "        # 使用增强的N值来提高探索性\n",
        "        other_nte_visits = sum(\n",
        "            visits for other_nte_id, visits in global_access_record[state_tuple]['nte_visits'].items()\n",
        "            if other_nte_id != nte_id\n",
        "        )\n",
        "        enhanced_N = parent_total_visits + other_nte_visits\n",
        "        \n",
        "        logger.debug(f\"重复访问节点检测: 原N={parent_total_visits}, 其他NTE访问={other_nte_visits}, 增强N={enhanced_N}\")\n",
        "    else:\n",
        "        # 正常情况，使用当前NTE的总访问次数\n",
        "        enhanced_N = parent_total_visits\n",
        "    \n",
        "    log_term = math.log(max(enhanced_N, 1))\n",
        "    exploration_term = current_c0 * c_rho * math.sqrt((2 * log_term) / (n + 1))\n",
        "\n",
        "    return exploitation_term + exploration_term\n",
        "\n",
        "def _conditional_selection(self, nodes, parent_node):\n",
        "    \"\"\"条件选择机制，避免陷入低价值分支\"\"\"\n",
        "    if not nodes:\n",
        "        return None\n",
        "        \n",
        "    # 设置阈值：父节点预测值的一定比例\n",
        "    value_threshold = parent_node.value_pred * 1.5  # 允许最多比父节点的预测值大50%\n",
        "    \n",
        "    # 筛选掉预测值远差于父节点的节点\n",
        "    filtered_nodes = [node for node in nodes if node.value_pred <= value_threshold]\n",
        "    \n",
        "    # 如果过滤后没有节点，则使用原始节点列表\n",
        "    if not filtered_nodes:\n",
        "        filtered_nodes = nodes\n",
        "        \n",
        "    # 在过滤后的节点中选择DUCB最高的\n",
        "    best_node = max(filtered_nodes, key=lambda n: n.visit_count)\n",
        "    return best_node\n",
        "\n",
        "def _select_candidates_mixed_strategy(self, visited_nodes):\n",
        "    \"\"\"使用混合策略从单个起始点的NTE搜索结果中选择候选点\"\"\"\n",
        "    if not visited_nodes:\n",
        "        return []\n",
        "\n",
        "    nodes_list = list(visited_nodes.values())\n",
        "    \n",
        "    # 按预测值排序，选择最优的\n",
        "    nodes_by_pred = sorted(nodes_list, key=lambda n: n.value_pred)\n",
        "    best_pred_candidates = [np.array(node.state) for node in nodes_by_pred[:15]]\n",
        "    \n",
        "    # 按访问次数排序，选择最常访问的\n",
        "    nodes_by_visits = sorted(nodes_list, key=lambda n: n.visit_count, reverse=True)\n",
        "    most_visited_candidates = [np.array(node.state) for node in nodes_by_visits[:3]]\n",
        "    \n",
        "    # 随机选择2个\n",
        "    remaining_nodes = [node for node in nodes_list \n",
        "                      if node not in nodes_by_pred[:15] and node not in nodes_by_visits[:3]]\n",
        "    random_candidates = []\n",
        "    if len(remaining_nodes) >= 2:\n",
        "        random_selection = random.sample(remaining_nodes, 2)\n",
        "        random_candidates = [np.array(node.state) for node in random_selection]\n",
        "    elif remaining_nodes:\n",
        "        random_candidates = [np.array(node.state) for node in remaining_nodes]\n",
        "    \n",
        "    # 合并所有候选点\n",
        "    all_candidates = best_pred_candidates + most_visited_candidates + random_candidates\n",
        "    \n",
        "    # 去重（基于状态相似性）\n",
        "    unique_candidates = []\n",
        "    tolerance = 1e-8\n",
        "    \n",
        "    for candidate in all_candidates:\n",
        "        is_duplicate = False\n",
        "        for existing in unique_candidates:\n",
        "            if np.allclose(candidate, existing, atol=tolerance, rtol=tolerance):\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        if not is_duplicate:\n",
        "            unique_candidates.append(candidate)\n",
        "    \n",
        "    return unique_candidates[:self.validation_batch_size]\n",
        "\n",
        "def _deduplicate_candidates_with_predictions(self, candidates_with_pred, tolerance=1e-8):\n",
        "    \"\"\"对带预测值的候选点列表进行去重，保留预测值更好的版本\"\"\"\n",
        "    if not candidates_with_pred:\n",
        "        return []\n",
        "    \n",
        "    unique_candidates = []\n",
        "    \n",
        "    for candidate, pred_value in candidates_with_pred:\n",
        "        is_duplicate = False\n",
        "        candidate_array = np.array(candidate)\n",
        "        \n",
        "        for i, (existing_candidate, existing_pred) in enumerate(unique_candidates):\n",
        "            existing_array = np.array(existing_candidate)\n",
        "            \n",
        "            # 检查两个点是否在容差范围内相等\n",
        "            if np.allclose(candidate_array, existing_array, atol=tolerance, rtol=tolerance):\n",
        "                # 发现重复，保留预测值更好的那个\n",
        "                if pred_value < existing_pred:  # 对于最小化问题，更小的值更好\n",
        "                    unique_candidates[i] = (candidate, pred_value)\n",
        "                is_duplicate = True\n",
        "                break\n",
        "        \n",
        "        if not is_duplicate:\n",
        "            unique_candidates.append((candidate, pred_value))\n",
        "    \n",
        "    return unique_candidates\n",
        "\n",
        "# 将所有方法添加到NTESearcher类中\n",
        "NTESearcher._calculate_ducb = _calculate_ducb\n",
        "NTESearcher._calculate_ducb_with_global_awareness = _calculate_ducb_with_global_awareness\n",
        "NTESearcher._conditional_selection = _conditional_selection\n",
        "NTESearcher._select_candidates_mixed_strategy = _select_candidates_mixed_strategy\n",
        "NTESearcher._deduplicate_candidates_with_predictions = _deduplicate_candidates_with_predictions\n",
        "\n",
        "print(\"✓ NTE Searcher 核心算法方法已添加\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: NTE Searcher 主搜索方法 - 完整实现\n",
        "\n",
        "def search(self, surrogate_model, current_best_state, min_y_observed, \n",
        "           dynamic_exploration_factor=1.0, all_states=None, all_values=None,\n",
        "           previous_iteration_samples=None, current_iteration=1, \n",
        "           return_ducb_trends=False):\n",
        "    \"\"\"\n",
        "    NTE搜索主方法 - 完整实现，包含所有原始功能\n",
        "    \n",
        "    这是调试动态探索因子的核心方法：\n",
        "    dynamic_exploration_factor = 0.8 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(f\"开始NTE搜索，动态探索因子: {dynamic_exploration_factor:.2f}\")\n",
        "    \n",
        "    # 初始化数据结构\n",
        "    visited_nodes = {}  # 状态tuple -> Node对象的映射\n",
        "    all_candidates_with_predictions = []  # 所有候选点及其预测值\n",
        "    ducb_trends_data = []  # DUCB趋势数据收集\n",
        "    global_access_record = {}  # 全局访问记录\n",
        "    \n",
        "    # 计算自适应探索因子c(rho)\n",
        "    c_rho = self.rho_scaling_factor_heuristic(min_y_observed)\n",
        "    logger.info(f\"自适应探索因子 c(rho): {c_rho:.4f}\")\n",
        "    \n",
        "    # 预计算动态搜索边界以提高效率\n",
        "    pre_computed_bounds = self._calculate_dynamic_search_space(all_states, all_values, current_iteration)\n",
        "    dynamic_mins, dynamic_maxs = pre_computed_bounds\n",
        "    logger.info(f\"动态搜索区间: [{dynamic_mins[0]:.3f}, {dynamic_maxs[0]:.3f}] (示例第1维)\")\n",
        "    \n",
        "    # 确定NTE搜索的起始点列表\n",
        "    best_states_for_nte = []\n",
        "    \n",
        "    if current_best_state is not None:\n",
        "        # 总是包含当前全局最优点\n",
        "        best_states_for_nte.append(current_best_state.copy())\n",
        "        \n",
        "    # 如果有历史数据，选择额外的起始点\n",
        "    if all_states is not None and all_values is not None and len(all_states) > 0:\n",
        "        # 从最优的10%样本中选择额外起始点\n",
        "        sorted_indices = np.argsort(all_values.flatten())\n",
        "        top_candidates_count = max(1, min(3, len(sorted_indices) // 10))\n",
        "        \n",
        "        for i in range(min(top_candidates_count, len(sorted_indices))):\n",
        "            candidate_state = all_states[sorted_indices[i]].copy()\n",
        "            \n",
        "            # 避免重复（如果候选状态与已有状态太接近）\n",
        "            is_duplicate = False\n",
        "            for existing_state in best_states_for_nte:\n",
        "                if np.allclose(candidate_state, existing_state, atol=1e-6):\n",
        "                    is_duplicate = True\n",
        "                    break\n",
        "            \n",
        "            if not is_duplicate:\n",
        "                best_states_for_nte.append(candidate_state)\n",
        "    \n",
        "    # 如果仍然没有足够的起始点，添加随机起始点\n",
        "    while len(best_states_for_nte) < 2:\n",
        "        random_start = np.array([\n",
        "            random.uniform(dynamic_mins[i], dynamic_maxs[i]) \n",
        "            for i in range(self.dimension)\n",
        "        ])\n",
        "        best_states_for_nte.append(random_start)\n",
        "    \n",
        "    logger.info(f\"选择了 {len(best_states_for_nte)} 个NTE起始点进行搜索\")\n",
        "    \n",
        "    # === 主要NTE搜索循环：对每个起始点执行搜索 ===\n",
        "    for nte_id, start_state in enumerate(best_states_for_nte):\n",
        "        # 初始化当前NTE的DUCB趋势收集器\n",
        "        current_nte_ducb_trend = {\n",
        "            'nte_id': nte_id,\n",
        "            'ducb_values': [],\n",
        "            'rollout_indices': [],\n",
        "            'selected_states': [],\n",
        "            'model_pred_values': [],\n",
        "            'node_changes': []\n",
        "        }\n",
        "        \n",
        "        # 初始化根节点\n",
        "        root_state_tuple = tuple(start_state)\n",
        "        try:\n",
        "            root_pred_value = float(surrogate_model.predict(np.array([start_state]))[0])\n",
        "        except Exception as e:\n",
        "            logger.error(f\"预测根节点失败: {e}\")\n",
        "            root_pred_value = float('inf')\n",
        "        \n",
        "        if root_state_tuple not in visited_nodes:\n",
        "            visited_nodes[root_state_tuple] = Node(\n",
        "                state=root_state_tuple, \n",
        "                value_pred=root_pred_value, \n",
        "                visit_count=0\n",
        "            )\n",
        "        \n",
        "        current_root_state_tuple = root_state_tuple\n",
        "        logger.debug(f\"开始从起始点 {nte_id+1}/{len(best_states_for_nte)} 进行NTE搜索，预测值: {root_pred_value:.4e}\")\n",
        "        \n",
        "        # 调整当前NTE的c0值\n",
        "        current_c0 = self.base_c0 * dynamic_exploration_factor\n",
        "        nte_total_visits = 0  # 当前NTE的总访问次数\n",
        "        \n",
        "        # === Rollout Phase ===\n",
        "        for rollout_round in range(self.rollout_rounds):\n",
        "            # 获取当前根节点对象\n",
        "            current_root_node_obj = visited_nodes[current_root_state_tuple]\n",
        "            current_root_state_vector = np.array(current_root_node_obj.state)\n",
        "            \n",
        "            # 递增当前根节点的访问计数\n",
        "            current_root_node_obj.visit_count += 1\n",
        "            nte_total_visits += 1\n",
        "            \n",
        "            # 更新全局访问记录\n",
        "            if current_root_state_tuple not in global_access_record:\n",
        "                global_access_record[current_root_state_tuple] = {\n",
        "                    'total_visits': 0,\n",
        "                    'nte_visits': {}\n",
        "                }\n",
        "            global_access_record[current_root_state_tuple]['total_visits'] += 1\n",
        "            if nte_id not in global_access_record[current_root_state_tuple]['nte_visits']:\n",
        "                global_access_record[current_root_state_tuple]['nte_visits'][nte_id] = 0\n",
        "            global_access_record[current_root_state_tuple]['nte_visits'][nte_id] += 1\n",
        "            \n",
        "            # === 生成叶节点(膨胀阶段) ===\n",
        "            leaf_states_vectors = self._mixed_expansion(\n",
        "                current_root_state_vector, \n",
        "                dynamic_exploration_factor,\n",
        "                all_states, \n",
        "                all_values, \n",
        "                current_iteration,\n",
        "                pre_computed_bounds=pre_computed_bounds\n",
        "            )\n",
        "            \n",
        "            if not leaf_states_vectors:\n",
        "                # 如果混合扩展失败，生成随机状态\n",
        "                rand_state = np.array([random.uniform(dynamic_mins[i], dynamic_maxs[i]) for i in range(self.dimension)])\n",
        "                leaf_states_vectors = [rand_state]\n",
        "            \n",
        "            # 用替代模型预测叶节点值\n",
        "            try:\n",
        "                leaf_states_batch = np.array(leaf_states_vectors)\n",
        "                leaf_pred_values = surrogate_model.predict(leaf_states_batch)\n",
        "                if not isinstance(leaf_pred_values, np.ndarray):\n",
        "                    leaf_pred_values = np.array(leaf_pred_values)\n",
        "                \n",
        "                leaf_pred_values = leaf_pred_values.flatten()\n",
        "\n",
        "                # 处理 NaN/Inf 值\n",
        "                if np.any(np.isnan(leaf_pred_values)) or np.any(np.isinf(leaf_pred_values)):\n",
        "                    logger.warning(f\"叶节点预测中存在NaN/Inf值，将替换它们。\")\n",
        "                    leaf_pred_values = np.nan_to_num(leaf_pred_values, nan=float('inf'), posinf=float('inf'), neginf=float('inf'))\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"预测叶节点错误: {e}。使用无穷大。\")\n",
        "                leaf_pred_values = np.array([float('inf')] * len(leaf_states_vectors))\n",
        "            \n",
        "            # 创建或更新叶节点\n",
        "            leaf_nodes = []\n",
        "            \n",
        "            for state_vec, pred_val in zip(leaf_states_vectors, leaf_pred_values):\n",
        "                state_tuple = tuple(state_vec)\n",
        "                node_value = float(pred_val)\n",
        "                \n",
        "                if state_tuple not in visited_nodes:\n",
        "                    visited_nodes[state_tuple] = Node(state=state_tuple, value_pred=node_value, visit_count=0)\n",
        "                    \n",
        "                    # 初始化全局访问记录\n",
        "                    if state_tuple not in global_access_record:\n",
        "                        global_access_record[state_tuple] = {\n",
        "                            'total_visits': 0,\n",
        "                            'nte_visits': {}\n",
        "                        }\n",
        "                else:\n",
        "                    # 更新预测值，保持访问计数\n",
        "                    visited_nodes[state_tuple].value_pred = node_value\n",
        "                \n",
        "                leaf_nodes.append(visited_nodes[state_tuple])\n",
        "            \n",
        "            # 计算根节点和叶节点的DUCB\n",
        "            root_ducb = self._calculate_ducb_with_global_awareness(\n",
        "                current_root_node_obj, \n",
        "                current_root_node_obj.visit_count,\n",
        "                c_rho, \n",
        "                current_c0,\n",
        "                global_access_record,\n",
        "                nte_id,\n",
        "                current_root_state_tuple\n",
        "            )\n",
        "            \n",
        "            # 使用条件选择\n",
        "            best_leaf_node = self._conditional_selection(leaf_nodes, current_root_node_obj)\n",
        "            if best_leaf_node is None:\n",
        "                # 如果条件选择失败，回退到根节点\n",
        "                next_root_state_tuple = current_root_state_tuple\n",
        "                self.last_move_successful = False\n",
        "                \n",
        "                # 收集DUCB趋势数据\n",
        "                if return_ducb_trends:\n",
        "                    current_nte_ducb_trend['ducb_values'].append(root_ducb)\n",
        "                    current_nte_ducb_trend['rollout_indices'].append(rollout_round)\n",
        "                    current_nte_ducb_trend['selected_states'].append(np.array(current_root_state_tuple))\n",
        "                    current_nte_ducb_trend['model_pred_values'].append(-current_root_node_obj.value_pred)\n",
        "            else:\n",
        "                # 对选择的最佳叶节点计算DUCB\n",
        "                best_leaf_ducb = self._calculate_ducb_with_global_awareness(\n",
        "                    best_leaf_node, \n",
        "                    nte_total_visits,\n",
        "                    c_rho, \n",
        "                    current_c0,\n",
        "                    global_access_record,\n",
        "                    nte_id,\n",
        "                    best_leaf_node.state\n",
        "                )\n",
        "                \n",
        "                # 局部反向传播：根据DUCB比较决定下一步\n",
        "                if root_ducb >= best_leaf_ducb:\n",
        "                    next_root_state_tuple = current_root_state_tuple\n",
        "                    self.last_move_successful = False\n",
        "                    \n",
        "                    # 收集DUCB趋势数据（根节点）\n",
        "                    if return_ducb_trends:\n",
        "                        current_nte_ducb_trend['ducb_values'].append(root_ducb)\n",
        "                        current_nte_ducb_trend['rollout_indices'].append(rollout_round)\n",
        "                        current_nte_ducb_trend['selected_states'].append(np.array(current_root_state_tuple))\n",
        "                        current_nte_ducb_trend['model_pred_values'].append(-current_root_node_obj.value_pred)\n",
        "                else:\n",
        "                    next_root_state_tuple = best_leaf_node.state\n",
        "                    # 判断是否找到了更好的点\n",
        "                    self.last_move_successful = best_leaf_node.value_pred < current_root_node_obj.value_pred\n",
        "                    if self.last_move_successful and self.last_move_deterministic:\n",
        "                        logger.debug(f\"确定性移动成功，找到了更优值: {current_root_node_obj.value_pred:.4e} -> {best_leaf_node.value_pred:.4e}\")\n",
        "                    \n",
        "                    # 收集DUCB趋势数据（叶节点）\n",
        "                    if return_ducb_trends:\n",
        "                        current_nte_ducb_trend['ducb_values'].append(best_leaf_ducb)\n",
        "                        current_nte_ducb_trend['rollout_indices'].append(rollout_round)\n",
        "                        current_nte_ducb_trend['selected_states'].append(np.array(best_leaf_node.state))\n",
        "                        current_nte_ducb_trend['model_pred_values'].append(-best_leaf_node.value_pred)\n",
        "                        # 记录节点变更\n",
        "                        if current_root_state_tuple != best_leaf_node.state:\n",
        "                            current_nte_ducb_trend['node_changes'].append(rollout_round)\n",
        "                    \n",
        "                    # 根据NTE，增加选中叶节点的访问次数\n",
        "                    best_leaf_node.visit_count += 1\n",
        "                    nte_total_visits += 1\n",
        "                    \n",
        "                    # 更新全局访问记录\n",
        "                    if next_root_state_tuple not in global_access_record:\n",
        "                        global_access_record[next_root_state_tuple] = {\n",
        "                            'total_visits': 0,\n",
        "                            'nte_visits': {}\n",
        "                        }\n",
        "                    global_access_record[next_root_state_tuple]['total_visits'] += 1\n",
        "                    if nte_id not in global_access_record[next_root_state_tuple]['nte_visits']:\n",
        "                        global_access_record[next_root_state_tuple]['nte_visits'][nte_id] = 0\n",
        "                    global_access_record[next_root_state_tuple]['nte_visits'][nte_id] += 1\n",
        "            \n",
        "            # 更新下一轮迭代的根节点\n",
        "            current_root_state_tuple = next_root_state_tuple\n",
        "        \n",
        "        # 从这个起始点搜索得到的候选点添加到总列表中\n",
        "        candidates_from_this_root = self._select_candidates_mixed_strategy(visited_nodes)\n",
        "        for candidate in candidates_from_this_root:\n",
        "            candidate_tuple = tuple(candidate)\n",
        "            # 获取预测值\n",
        "            if candidate_tuple in visited_nodes:\n",
        "                pred_value = visited_nodes[candidate_tuple].value_pred\n",
        "            else:\n",
        "                try:\n",
        "                    pred_value = float(surrogate_model.predict(np.array([candidate]))[0])\n",
        "                except:\n",
        "                    pred_value = float('inf')\n",
        "            \n",
        "            all_candidates_with_predictions.append((candidate, pred_value))\n",
        "        \n",
        "        # 将当前起始点的DUCB趋势数据添加到总的趋势数据中\n",
        "        if return_ducb_trends:\n",
        "            ducb_trends_data.append(current_nte_ducb_trend)\n",
        "    \n",
        "    # === 最终处理阶段：智能去重和最优选择 ===\n",
        "    logger.info(f\"NTE搜索完成，合并前共有 {len(all_candidates_with_predictions)} 个候选点\")\n",
        "    \n",
        "    # 去重候选点（保留预测值更好的版本）\n",
        "    unique_candidates_with_pred = self._deduplicate_candidates_with_predictions(\n",
        "        all_candidates_with_predictions, \n",
        "        tolerance=1e-8\n",
        "    )\n",
        "    logger.info(f\"去重后共有 {len(unique_candidates_with_pred)} 个唯一候选点，\" +\n",
        "               f\"去重比例: {(len(all_candidates_with_predictions) - len(unique_candidates_with_pred))/len(all_candidates_with_predictions)*100:.1f}%\")\n",
        "    \n",
        "    # 如果候选点不足，添加随机点\n",
        "    if len(unique_candidates_with_pred) < self.validation_batch_size:\n",
        "        num_random_to_add = self.validation_batch_size - len(unique_candidates_with_pred)\n",
        "        logger.info(f\"候选点不足，添加{num_random_to_add}个随机点\")\n",
        "        \n",
        "        # 创建现有候选点的集合用于避免重复\n",
        "        existing_candidates_set = set()\n",
        "        for candidate, _ in unique_candidates_with_pred:\n",
        "            rounded_candidate = tuple(np.round(candidate, 6))\n",
        "            existing_candidates_set.add(rounded_candidate)\n",
        "        \n",
        "        added_random_count = 0\n",
        "        max_attempts = num_random_to_add * 10\n",
        "        attempts = 0\n",
        "        \n",
        "        while added_random_count < num_random_to_add and attempts < max_attempts:\n",
        "            random_point = np.array([random.uniform(self.domain_mins[i], self.domain_maxs[i]) \n",
        "                                     for i in range(self.dimension)])\n",
        "            rounded_random = tuple(np.round(random_point, 6))\n",
        "            \n",
        "            if rounded_random not in existing_candidates_set:\n",
        "                try:\n",
        "                    random_pred = float(surrogate_model.predict(np.array([random_point]))[0])\n",
        "                except:\n",
        "                    random_pred = float('inf')\n",
        "                \n",
        "                unique_candidates_with_pred.append((random_point, random_pred))\n",
        "                existing_candidates_set.add(rounded_random)\n",
        "                added_random_count += 1\n",
        "            \n",
        "            attempts += 1\n",
        "        \n",
        "        if added_random_count < num_random_to_add:\n",
        "            logger.warning(f\"只能添加 {added_random_count}/{num_random_to_add} 个不重复的随机点\")\n",
        "    \n",
        "    # 按预测值排序（升序，因为Rosenbrock是最小化问题）\n",
        "    unique_candidates_with_pred.sort(key=lambda x: x[1])\n",
        "    \n",
        "    # 选择最终的20个样本：前15个最优 + 后5个随机探索\n",
        "    final_candidates = []\n",
        "    \n",
        "    # 前15个预测值最优的样本\n",
        "    best_15_count = min(15, len(unique_candidates_with_pred))\n",
        "    for i in range(best_15_count):\n",
        "        final_candidates.append(unique_candidates_with_pred[i][0])\n",
        "    \n",
        "    # 剩余5个从其他候选点中随机选择\n",
        "    remaining_candidates = unique_candidates_with_pred[best_15_count:]\n",
        "    explore_count = min(self.validation_batch_size - best_15_count, len(remaining_candidates))\n",
        "    \n",
        "    if explore_count > 0:\n",
        "        if len(remaining_candidates) > explore_count:\n",
        "            # 随机选择剩余的探索样本\n",
        "            import random as rand\n",
        "            selected_indices = rand.sample(range(len(remaining_candidates)), explore_count)\n",
        "            for idx in selected_indices:\n",
        "                final_candidates.append(remaining_candidates[idx][0])\n",
        "        else:\n",
        "            # 全部添加\n",
        "            for candidate, _ in remaining_candidates:\n",
        "                final_candidates.append(candidate)\n",
        "    \n",
        "    logger.info(f\"最终选择 {len(final_candidates)} 个样本：{best_15_count} 个最优样本 + {len(final_candidates) - best_15_count} 个探索样本\")\n",
        "    \n",
        "    # 确保返回列表不超过所请求的候选点数量\n",
        "    final_result = final_candidates[:self.validation_batch_size]\n",
        "    \n",
        "    if return_ducb_trends:\n",
        "        return final_result, ducb_trends_data\n",
        "    else:\n",
        "        return final_result\n",
        "\n",
        "# 将search方法添加到NTESearcher类中\n",
        "NTESearcher.search = search\n",
        "\n",
        "print(\"✓ NTE Searcher 完整实现已完成 - 这是调试动态探索因子的核心方法！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: 完整可视化工具 (plot_utils.py) - 恢复所有原始功能\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import matplotlib\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 配置全局字体设置为默认英文字体\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "\n",
        "# 使用Agg后端，以便在没有GUI的环境中也能保存图像\n",
        "plt.switch_backend('Agg') \n",
        "\n",
        "def create_results_directory(base_results_path):\n",
        "    \"\"\"在指定的基础路径下创建一个以当前时间戳命名的子目录\"\"\"\n",
        "    try:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_dir = os.path.join(base_results_path, timestamp)\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        logger.info(f\"Results directory created: {results_dir}\")\n",
        "        return results_dir\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating results directory: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_global_min_value_trend(iterations, min_values, output_dir, filename=\"global_min_value_trend.png\"):\n",
        "    \"\"\"绘制并保存全局最小真值的变化趋势图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 确保iterations和min_values长度相同\n",
        "        if len(iterations) != len(min_values):\n",
        "            logger.warning(f\"Iterations和min_values数组长度不同: {len(iterations)} vs {len(min_values)}.\")\n",
        "            min_length = min(len(iterations), len(min_values))\n",
        "            iterations = iterations[:min_length]\n",
        "            min_values = min_values[:min_length]\n",
        "            logger.warning(f\"数组截断至相同长度: {min_length}\")\n",
        "            \n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, min_values, marker='o', linestyle='-', color='b')\n",
        "        plt.title('Global Minimum Value Trend')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Current Minimum True Value Found')\n",
        "        plt.grid(True)\n",
        "        plt.xticks(iterations)\n",
        "        \n",
        "        # 只在找到更低全局最小值时添加标注\n",
        "        min_values_array = np.array(min_values)\n",
        "        global_min = float('inf')\n",
        "        \n",
        "        for i, (iter_num, current_min) in enumerate(zip(iterations, min_values_array)):\n",
        "            if current_min < global_min:\n",
        "                global_min = current_min\n",
        "                if current_min < 1000:\n",
        "                    label_txt = f\"{current_min:.2f}\"\n",
        "                else:\n",
        "                    label_txt = f\"{current_min:.2e}\"\n",
        "                plt.annotate(label_txt, \n",
        "                            (iter_num, current_min),\n",
        "                            textcoords=\"offset points\", \n",
        "                            xytext=(0,10), \n",
        "                            ha='center',\n",
        "                            fontsize=9,\n",
        "                            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
        "\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"全局最小值趋势图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制全局最小值趋势图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "def plot_loss_trend(data_points, output_dir, x_label=\"Epoch/Iteration\", title=\"Model Loss Trend\", \n",
        "                   filename=\"loss_trend.png\", custom_ticks=None, pearson_coeffs=None):\n",
        "    \"\"\"绘制并保存损失函数的变化趋势图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # 绘制损失曲线\n",
        "        if custom_ticks and len(custom_ticks) == len(data_points):\n",
        "            plt.plot(custom_ticks, data_points, marker='.', linestyle='-', color='r', label='Loss')\n",
        "            x_values = custom_ticks\n",
        "        else:\n",
        "            plt.plot(data_points, marker='.', linestyle='-', color='r', label='Loss')\n",
        "            x_values = range(len(data_points))\n",
        "        \n",
        "        plt.title(title if title else 'Model Loss Trend')\n",
        "        plt.xlabel(x_label if x_label else 'Training Epochs')\n",
        "        plt.ylabel('Loss Value')\n",
        "        plt.grid(True)\n",
        "        \n",
        "        # 如果提供了皮尔逊相关系数，添加第二个y轴并绘制\n",
        "        if pearson_coeffs is not None and len(pearson_coeffs) == len(data_points):\n",
        "            ax2 = plt.gca().twinx()\n",
        "            ax2.plot(x_values, pearson_coeffs, marker='s', linestyle='--', color='g', label='Pearson Correlation')\n",
        "            ax2.set_ylabel('Pearson Correlation Coefficient', color='g')\n",
        "            ax2.tick_params(axis='y', labelcolor='g')\n",
        "            ax2.set_ylim(-0.1, 1.1)\n",
        "            \n",
        "            # 在每个数据点上标注皮尔逊系数值\n",
        "            for i, coef in enumerate(pearson_coeffs):\n",
        "                ax2.annotate(f\"{coef:.3f}\",\n",
        "                           (x_values[i], pearson_coeffs[i]),\n",
        "                           textcoords=\"offset points\", \n",
        "                           xytext=(0,10), \n",
        "                           ha='center',\n",
        "                           fontsize=8,\n",
        "                           color='g')\n",
        "            \n",
        "            # 修改图例以包含两个曲线\n",
        "            lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
        "            lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "            ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
        "        else:\n",
        "            plt.legend(loc='upper right')\n",
        "        \n",
        "        # 使用自定义x轴刻度（如果提供）\n",
        "        if custom_ticks:\n",
        "            plt.xticks(custom_ticks)\n",
        "\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Loss趋势图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制损失趋势图出错: {e}\")\n",
        "\n",
        "def plot_pearson_correlation_trend(iterations, pearson_coeffs, output_dir, \n",
        "                                  filename=\"pearson_correlation_trend.png\", title=None):\n",
        "    \"\"\"绘制并保存皮尔逊相关系数的变化趋势图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, pearson_coeffs, marker='o', linestyle='-', color='g')\n",
        "        plt.title(title if title else 'Pearson Correlation Coefficient Trend (Model vs. Truth)')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Pearson Correlation Coefficient')\n",
        "        plt.ylim(-1.1, 1.1)\n",
        "        plt.grid(True)\n",
        "        plt.xticks(iterations)\n",
        "        \n",
        "        # 添加水平参考线\n",
        "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate Correlation')\n",
        "        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Strong Correlation')\n",
        "        \n",
        "        plt.legend()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"Pearson相关系数趋势图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制Pearson相关系数趋势图出错: {e}\")\n",
        "\n",
        "def plot_nte_performance(iterations, num_candidates, output_dir, filename=\"nte_candidates_trend.png\"):\n",
        "    \"\"\"绘制并保存NTE搜索性能图，显示每次迭代找到的候选点数量\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations, num_candidates, marker='s', linestyle='-', color='purple', linewidth=2)\n",
        "        plt.title('NTE Search Performance: Number of Candidates Found')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Number of Candidate Points')\n",
        "        plt.grid(True)\n",
        "        plt.xticks(iterations)\n",
        "        \n",
        "        # 添加平均线\n",
        "        avg_candidates = np.mean(num_candidates)\n",
        "        plt.axhline(y=avg_candidates, color='red', linestyle='--', alpha=0.7, \n",
        "                   label=f'Average: {avg_candidates:.1f}')\n",
        "        \n",
        "        plt.legend()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        logger.info(f\"NTE性能图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制NTE性能图出错: {e}\")\n",
        "\n",
        "print(\"✓ 完整可视化工具 (第1部分) 已加载\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: 完整可视化工具 (第2部分) - 高级可视化功能\n",
        "\n",
        "def plot_prediction_vs_truth(predictions, ground_truth, output_dir, iteration=None, \n",
        "                            filename=None, title_suffix=None):\n",
        "    \"\"\"绘制预测值vs真实值散点图，并计算评估指标\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return None, None, None\n",
        "\n",
        "    try:\n",
        "        predictions = np.array(predictions).flatten()\n",
        "        ground_truth = np.array(ground_truth).flatten()\n",
        "        \n",
        "        if len(predictions) != len(ground_truth):\n",
        "            logger.error(f\"预测值和真实值长度不匹配: {len(predictions)} vs {len(ground_truth)}\")\n",
        "            return None, None, None\n",
        "        \n",
        "        # 计算评估指标\n",
        "        mse = np.mean((predictions - ground_truth) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = np.mean(np.abs(predictions - ground_truth))\n",
        "        \n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        # 绘制散点图\n",
        "        plt.scatter(ground_truth, predictions, alpha=0.6, s=20)\n",
        "        \n",
        "        # 绘制理想线 (y=x)\n",
        "        min_val = min(np.min(ground_truth), np.min(predictions))\n",
        "        max_val = max(np.max(ground_truth), np.max(predictions))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "        \n",
        "        # 设置标题和标签\n",
        "        title = f'Prediction vs Ground Truth'\n",
        "        if title_suffix:\n",
        "            title += f' ({title_suffix})'\n",
        "        if iteration is not None:\n",
        "            title += f' - Iteration {iteration}'\n",
        "        \n",
        "        plt.title(title)\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Prediction')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        \n",
        "        # 添加评估指标文本\n",
        "        textstr = f'RMSE: {rmse:.4e}\\\\nMAE: {mae:.4e}\\\\nSamples: {len(predictions)}'\n",
        "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
        "        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=props)\n",
        "        \n",
        "        # 保存图形\n",
        "        if filename is None:\n",
        "            if iteration is not None:\n",
        "                filename = f\"prediction_vs_truth_iter_{iteration}.png\"\n",
        "            else:\n",
        "                filename = \"prediction_vs_truth.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"预测vs真实值图已保存: {save_path} (RMSE: {rmse:.4e})\")\n",
        "        return mse, rmse, mae\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制预测vs真实值图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None, None, None\n",
        "\n",
        "def plot_residuals(predictions, ground_truth, output_dir, iteration=None, filename=None):\n",
        "    \"\"\"绘制残差分析图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        predictions = np.array(predictions).flatten()\n",
        "        ground_truth = np.array(ground_truth).flatten()\n",
        "        \n",
        "        if len(predictions) != len(ground_truth):\n",
        "            logger.error(f\"预测值和真实值长度不匹配: {len(predictions)} vs {len(ground_truth)}\")\n",
        "            return\n",
        "        \n",
        "        residuals = predictions - ground_truth\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # 子图1: 残差vs预测值\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.scatter(predictions, residuals, alpha=0.6, s=20)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Predictions')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Residuals vs Predictions')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 子图2: 残差vs真实值\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.scatter(ground_truth, residuals, alpha=0.6, s=20)\n",
        "        plt.axhline(y=0, color='r', linestyle='--')\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title('Residuals vs Ground Truth')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 子图3: 残差直方图\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "        plt.xlabel('Residuals')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Residuals Distribution')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 子图4: Q-Q图（简化版）\n",
        "        plt.subplot(2, 2, 4)\n",
        "        sorted_residuals = np.sort(residuals)\n",
        "        theoretical_quantiles = np.linspace(-3, 3, len(sorted_residuals))\n",
        "        plt.scatter(theoretical_quantiles, sorted_residuals, alpha=0.6, s=20)\n",
        "        plt.plot(theoretical_quantiles, theoretical_quantiles * np.std(residuals), 'r--')\n",
        "        plt.xlabel('Theoretical Quantiles')\n",
        "        plt.ylabel('Sample Quantiles')\n",
        "        plt.title('Q-Q Plot')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # 保存图形\n",
        "        if filename is None:\n",
        "            if iteration is not None:\n",
        "                filename = f\"residual_analysis_iter_{iteration}.png\"\n",
        "            else:\n",
        "                filename = \"residual_analysis.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"残差分析图已保存: {save_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制残差分析图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "def plot_best_samples_pearson_trend(iterations_history, pearson_history, results_dir, \n",
        "                                   filename=\"best_samples_pearson_trend.png\", \n",
        "                                   title=\"Pearson Correlation on Best Samples\"):\n",
        "    \"\"\"绘制最佳样本皮尔逊相关系数趋势图\"\"\"\n",
        "    if not os.path.exists(results_dir):\n",
        "        logger.warning(f\"Results directory {results_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(iterations_history, pearson_history, marker='o', linestyle='-', \n",
        "                color='blue', linewidth=2, markersize=6)\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Pearson Correlation Coefficient')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.ylim(-1.1, 1.1)\n",
        "        \n",
        "        # 添加水平参考线\n",
        "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Moderate (0.5)')\n",
        "        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Strong (0.8)')\n",
        "        \n",
        "        # 如果有足够的数据点，添加趋势线\n",
        "        if len(iterations_history) > 2:\n",
        "            z = np.polyfit(iterations_history, pearson_history, 1)\n",
        "            p = np.poly1d(z)\n",
        "            plt.plot(iterations_history, p(iterations_history), \"g--\", alpha=0.8, label='Trend')\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.xticks(iterations_history)\n",
        "        \n",
        "        save_path = os.path.join(results_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"最佳样本Pearson趋势图已保存: {save_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制最佳样本Pearson趋势图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "def plot_surrogate_vs_selected_samples(surrogate_model, selected_X, selected_y_true, \n",
        "                                      data_manager, output_dir, iteration, filename=None):\n",
        "    \"\"\"绘制替代模型预测vs新选择样本的对比图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        if len(selected_X) == 0:\n",
        "            logger.warning(\"没有选择的样本，跳过对比图绘制\")\n",
        "            return None\n",
        "        \n",
        "        # 使用替代模型预测新选择的样本\n",
        "        if hasattr(surrogate_model, 'predict'):\n",
        "            predictions_log = surrogate_model.predict(selected_X)\n",
        "            # 转换为原始尺度\n",
        "            predictions_orig = safe_power10(predictions_log)\n",
        "        else:\n",
        "            logger.error(\"替代模型没有predict方法\")\n",
        "            return None\n",
        "        \n",
        "        # 计算评估指标\n",
        "        if len(predictions_orig) >= 2:\n",
        "            pearson_corr, _ = pearsonr(predictions_orig, selected_y_true.flatten())\n",
        "            if np.isnan(pearson_corr):\n",
        "                pearson_corr = 0.0\n",
        "        else:\n",
        "            pearson_corr = 0.0\n",
        "        \n",
        "        mae = np.mean(np.abs(predictions_orig - selected_y_true.flatten()))\n",
        "        rmse = np.sqrt(np.mean((predictions_orig - selected_y_true.flatten()) ** 2))\n",
        "        \n",
        "        # 创建对比图\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        plt.scatter(selected_y_true.flatten(), predictions_orig, alpha=0.7, s=50, \n",
        "                   c='blue', label=f'Selected Samples (n={len(selected_X)})')\n",
        "        \n",
        "        # 绘制理想线\n",
        "        min_val = min(np.min(selected_y_true), np.min(predictions_orig))\n",
        "        max_val = max(np.max(selected_y_true), np.max(predictions_orig))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "        \n",
        "        plt.xlabel('True Values (Selected Samples)')\n",
        "        plt.ylabel('Surrogate Model Predictions')\n",
        "        plt.title(f'Surrogate Model vs Selected Samples (Iteration {iteration})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        \n",
        "        # 添加评估指标\n",
        "        textstr = f'Pearson: {pearson_corr:.4f}\\\\nMAE: {mae:.4e}\\\\nRMSE: {rmse:.4e}'\n",
        "        props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
        "        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
        "                verticalalignment='top', bbox=props)\n",
        "        \n",
        "        # 保存图形\n",
        "        if filename is None:\n",
        "            filename = f\"surrogate_vs_selected_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"替代模型vs选择样本对比图已保存: {save_path}\")\n",
        "        \n",
        "        return {\n",
        "            'pearson': pearson_corr,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'n_samples': len(selected_X)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制替代模型vs选择样本对比图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "print(\"✓ 完整可视化工具 (第2部分) 已加载\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: 完整可视化工具 (第3部分) - 特殊可视化功能\n",
        "\n",
        "def plot_top_samples_dimensions(data_manager, output_dir, iteration, n_top_samples=15, filename=None):\n",
        "    \"\"\"\n",
        "    绘制最优样本在各个维度上的数值分布图\n",
        "    \n",
        "    Args:\n",
        "        data_manager: 数据管理器\n",
        "        output_dir: 输出目录\n",
        "        iteration: 当前迭代次数\n",
        "        n_top_samples: 要显示的最优样本数量\n",
        "        filename: 可选的自定义文件名\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # 获取所有样本数据\n",
        "        all_X = data_manager.get_all_x_orig()\n",
        "        all_y = data_manager.get_all_y_orig()\n",
        "        \n",
        "        if len(all_X) == 0:\n",
        "            logger.warning(\"数据管理器中没有样本数据\")\n",
        "            return None\n",
        "        \n",
        "        # 按y值排序，获取最优的n_top_samples个样本\n",
        "        sorted_indices = np.argsort(all_y.flatten())\n",
        "        top_indices = sorted_indices[:min(n_top_samples, len(sorted_indices))]\n",
        "        top_samples_X = all_X[top_indices]\n",
        "        top_samples_y = all_y[top_indices]\n",
        "        \n",
        "        # 获取维度数量\n",
        "        n_dimensions = top_samples_X.shape[1]\n",
        "        dimensions = np.arange(1, n_dimensions + 1)\n",
        "        \n",
        "        # 创建图形\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        \n",
        "        # 为每个样本分配颜色和透明度\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_samples_X)))\n",
        "        \n",
        "        # 绘制每个最优样本在各维度上的值\n",
        "        for i, (sample_x, sample_y) in enumerate(zip(top_samples_X, top_samples_y)):\n",
        "            alpha = 1.0 if i == 0 else 0.6  # 最优样本使用不透明，其他使用半透明\n",
        "            linewidth = 3 if i == 0 else 1.5\n",
        "            marker_size = 8 if i == 0 else 5\n",
        "            \n",
        "            if i == 0:\n",
        "                label = f\"Best (y={sample_y[0]:.4e})\"\n",
        "            elif i < 3:\n",
        "                label = f\"#{i+1} (y={sample_y[0]:.4e})\"\n",
        "            else:\n",
        "                label = None\n",
        "            \n",
        "            plt.plot(dimensions, sample_x, marker='o', linestyle='-', \n",
        "                    color=colors[i], alpha=alpha, linewidth=linewidth, \n",
        "                    markersize=marker_size, label=label)\n",
        "        \n",
        "        # 添加参考线（平均值）\n",
        "        if len(top_samples_X) > 1:\n",
        "            mean_values = np.mean(top_samples_X, axis=0)\n",
        "            plt.plot(dimensions, mean_values, marker='s', linestyle='--', \n",
        "                    color='red', alpha=0.8, linewidth=2, markersize=6,\n",
        "                    label=f'Mean of Top {len(top_samples_X)}')\n",
        "        \n",
        "        # 设置图表属性\n",
        "        plt.title(f'Top {n_top_samples} Samples Dimension Values (Iteration {iteration})', fontsize=14)\n",
        "        plt.xlabel('Dimension', fontsize=12)\n",
        "        plt.ylabel('Dimension Value', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 设置x轴刻度\n",
        "        plt.xticks(dimensions)\n",
        "        \n",
        "        # 设置图例，但只显示前几个最重要的样本以避免图例过于拥挤\n",
        "        handles, labels = plt.gca().get_legend_handles_labels()\n",
        "        # 显示最优样本、参考线以及其他几个样本\n",
        "        important_indices = [0, -1] + list(range(1, min(4, len(handles)-1)))  # 最优、参考线、前几个其他样本\n",
        "        important_handles = [handles[i] for i in important_indices if i < len(handles)]\n",
        "        important_labels = [labels[i] for i in important_indices if i < len(labels)]\n",
        "        plt.legend(important_handles, important_labels, \n",
        "                  bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
        "        \n",
        "        # 调整布局以适应图例\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # 保存图形\n",
        "        if filename is None:\n",
        "            filename = f\"top_samples_dimensions_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"最优样本维度分布图已保存: {save_path}\")\n",
        "        return save_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制最优样本维度分布图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def plot_nte_ducb_trends(ducb_trends_data, output_dir, iteration, dynamic_exploration_factor, \n",
        "                        new_global_best_value=None, filename=None):\n",
        "    \"\"\"\n",
        "    可视化NTE搜索过程中选择的父节点的DUCB值变化趋势\n",
        "    \n",
        "    Args:\n",
        "        ducb_trends_data (list): 包含多个初始点的DUCB变化数据\n",
        "        output_dir (str): 保存图表的目录路径\n",
        "        iteration (int): 当前迭代次数\n",
        "        dynamic_exploration_factor (float): 当前迭代使用的动态探索因子\n",
        "        new_global_best_value (float, optional): 如果找到新的全局最小值，传入该值\n",
        "        filename (str, optional): 自定义文件名\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 创建图形\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        \n",
        "        # 定义颜色和节点变化标记\n",
        "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "        node_change_markers = ['s', '^', 'D', 'v', 'p']  # 方块、三角、钻石、倒三角、五角形\n",
        "        node_change_sizes = [80, 80, 80, 80, 80]  # 节点变化标记大小\n",
        "        \n",
        "        # 绘制每个初始点的DUCB变化趋势\n",
        "        for i, trend_data in enumerate(ducb_trends_data):\n",
        "            if not trend_data or 'ducb_values' not in trend_data:\n",
        "                continue\n",
        "                \n",
        "            ducb_values = trend_data['ducb_values']\n",
        "            rollout_indices = trend_data.get('rollout_indices', list(range(len(ducb_values))))\n",
        "            node_changes = trend_data.get('node_changes', [])\n",
        "            model_pred_values = trend_data.get('model_pred_values', [])\n",
        "            \n",
        "            if len(ducb_values) == 0:\n",
        "                continue\n",
        "            \n",
        "            color_idx = i % len(colors)\n",
        "            \n",
        "            # 绘制DUCB值变化曲线\n",
        "            plt.plot(rollout_indices, ducb_values, \n",
        "                    color=colors[color_idx], \n",
        "                    linewidth=2, alpha=0.8,\n",
        "                    label=f'Initial Point {i+1} DUCB')\n",
        "            \n",
        "            # 绘制模型预测值的负值曲线（如果有数据）\n",
        "            if len(model_pred_values) == len(ducb_values):\n",
        "                plt.plot(rollout_indices, model_pred_values, \n",
        "                        color=colors[color_idx], \n",
        "                        linewidth=1.5, alpha=0.6, linestyle='--',\n",
        "                        label=f'Initial Point {i+1} -v_ML')\n",
        "            \n",
        "            # 在节点变更位置添加特殊标记\n",
        "            for change_idx in node_changes:\n",
        "                if change_idx < len(rollout_indices) and change_idx < len(ducb_values):\n",
        "                    plt.scatter(rollout_indices[change_idx], ducb_values[change_idx], \n",
        "                              marker=node_change_markers[color_idx],\n",
        "                              s=node_change_sizes[color_idx],\n",
        "                              color=colors[color_idx], \n",
        "                              edgecolors='white', linewidth=1.5,\n",
        "                              alpha=0.9, zorder=5)\n",
        "        \n",
        "        # 设置图表属性\n",
        "        title = f'NTE DUCB Values Trend (Iteration {iteration})'\n",
        "        if dynamic_exploration_factor is not None:\n",
        "            title += f'\\\\nExploration Factor: {dynamic_exploration_factor:.2f}'\n",
        "        if new_global_best_value is not None:\n",
        "            title += f' | New Global Best: {new_global_best_value:.6e}'\n",
        "        \n",
        "        plt.title(title, fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('NTE Rollout Round', fontsize=12)\n",
        "        plt.ylabel('DUCB Value', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 创建图例，包括DUCB曲线、模型预测值曲线和节点变化标记的说明\n",
        "        legend_elements = []\n",
        "        \n",
        "        # 添加DUCB线条图例\n",
        "        for i in range(min(len(ducb_trends_data), len(colors))):\n",
        "            if (ducb_trends_data[i] and 'ducb_values' in ducb_trends_data[i] and \n",
        "                len(ducb_trends_data[i]['ducb_values']) > 0):\n",
        "                legend_elements.append(plt.Line2D([0], [0], color=colors[i], \n",
        "                                                linewidth=2, label=f'Point {i+1} DUCB'))\n",
        "        \n",
        "        # 添加模型预测值线条图例\n",
        "        for i in range(min(len(ducb_trends_data), len(colors))):\n",
        "            if (ducb_trends_data[i] and 'model_pred_values' in ducb_trends_data[i] and \n",
        "                len(ducb_trends_data[i]['model_pred_values']) > 0):\n",
        "                legend_elements.append(plt.Line2D([0], [0], color=colors[i], \n",
        "                                                linewidth=1.5, linestyle='--', alpha=0.6,\n",
        "                                                label=f'Point {i+1} -v_ML'))\n",
        "        \n",
        "        # 添加节点变化标记的说明\n",
        "        for i in range(min(3, len(ducb_trends_data))):  # 最多显示3个标记说明\n",
        "            if i < len(colors) and i < len(node_change_markers):\n",
        "                legend_elements.append(plt.Line2D([0], [0], marker=node_change_markers[i], \n",
        "                                                color=colors[i], linestyle='None',\n",
        "                                                markersize=8, markeredgecolor='white', markeredgewidth=1.5,\n",
        "                                                label=f'Point {i+1} Node Change'))\n",
        "        \n",
        "        plt.legend(handles=legend_elements, fontsize=9, loc='best')\n",
        "        \n",
        "        # 添加统计信息\n",
        "        total_rollouts = max([len(trend['ducb_values']) for trend in ducb_trends_data \n",
        "                             if trend and 'ducb_values' in trend] + [0])\n",
        "        total_node_changes = sum([len(trend.get('node_changes', [])) for trend in ducb_trends_data \n",
        "                                 if trend])\n",
        "        \n",
        "        stats_text = f'Total Rollouts: {total_rollouts}\\\\nNode Changes: {total_node_changes}'\n",
        "        plt.text(0.02, 0.98, stats_text, \n",
        "                transform=plt.gca().transAxes, fontsize=10,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        # 调整布局\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # 保存图形\n",
        "        if filename is None:\n",
        "            filename = f\"nte_ducb_trends_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"NTE DUCB趋势图已保存: {save_path}\")\n",
        "        return save_path\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制NTE DUCB趋势图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def plot_search_boundaries(iterations, boundaries_data, output_dir, filename_prefix=\"search_boundaries\"):\n",
        "    \"\"\"绘制搜索边界变化图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        if not boundaries_data:\n",
        "            logger.warning(\"没有边界数据可以绘制\")\n",
        "            return\n",
        "        \n",
        "        # 假设我们只显示前几个维度的边界变化\n",
        "        n_dims_to_show = min(5, len(boundaries_data[0][1]) if boundaries_data else 0)\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        for dim in range(n_dims_to_show):\n",
        "            mins = [boundary[1][dim] for boundary in boundaries_data]\n",
        "            maxs = [boundary[2][dim] for boundary in boundaries_data]\n",
        "            \n",
        "            plt.subplot(2, 3, dim + 1)\n",
        "            plt.plot(iterations, mins, 'b-', label=f'Min (Dim {dim+1})')\n",
        "            plt.plot(iterations, maxs, 'r-', label=f'Max (Dim {dim+1})')\n",
        "            plt.fill_between(iterations, mins, maxs, alpha=0.3)\n",
        "            plt.title(f'Dimension {dim+1} Search Bounds')\n",
        "            plt.xlabel('Iteration')\n",
        "            plt.ylabel('Boundary Value')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, f\"{filename_prefix}.png\")\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"搜索边界图已保存: {save_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制搜索边界图出错: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "print(\"✓ 完整可视化工具 (第3部分) 已加载\")\n",
        "print(\"✓ 所有可视化功能已完全恢复，与原代码一致！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: 性能优化和进程管理模块\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import time\n",
        "import subprocess\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PerformanceOptimizer:\n",
        "    \"\"\"性能优化器，负责优化TensorFlow训练性能\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.cpu_count = psutil.cpu_count(logical=True)\n",
        "        self.physical_cpu_count = psutil.cpu_count(logical=False)\n",
        "        self.memory_cache = {}\n",
        "        self.is_configured = False\n",
        "        logger.info(f\"检测到CPU核心数: 物理={self.physical_cpu_count}, 逻辑={self.cpu_count}\")\n",
        "    \n",
        "    def configure_tensorflow_performance(self):\n",
        "        \"\"\"配置TensorFlow性能设置 - 专门优化Kaggle P100显卡\"\"\"\n",
        "        if self.is_configured:\n",
        "            return\n",
        "            \n",
        "        logger.info(\"开始配置TensorFlow性能优化 (针对Kaggle P100)...\")\n",
        "        \n",
        "        # P100专门的GPU内存配置\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    # P100显卡内存管理优化\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                    # 设置虚拟GPU内存限制，避免OOM\n",
        "                    tf.config.experimental.set_virtual_device_configuration(\n",
        "                        gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=15000)]  # P100约16GB，留1GB余量\n",
        "                    )\n",
        "                logger.info(f\"已配置P100 GPU内存管理，检测到{len(gpus)}个GPU\")\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPU内存配置失败: {e}\")\n",
        "        \n",
        "        # 针对P100的并行度优化\n",
        "        tf.config.threading.set_inter_op_parallelism_threads(4)  # P100最优配置\n",
        "        tf.config.threading.set_intra_op_parallelism_threads(2)  # P100最优配置\n",
        "        \n",
        "        # P100混合精度优化\n",
        "        if gpus:\n",
        "            try:\n",
        "                # P100支持混合精度但需要谨慎设置\n",
        "                policy = mixed_precision.Policy('mixed_float16')\n",
        "                mixed_precision.set_global_policy(policy)\n",
        "                logger.info(\"已启用P100混合精度训练 (mixed_float16)\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"P100混合精度配置失败: {e}\")\n",
        "        \n",
        "        # P100专门的性能选项\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "        os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
        "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # P100最优线程模式\n",
        "        os.environ['TF_GPU_THREAD_COUNT'] = '2'  # P100最优线程数\n",
        "        os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'  # P100批归一化优化\n",
        "        \n",
        "        # XLA编译优化 (P100支持)\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "        \n",
        "        self.is_configured = True\n",
        "        logger.info(\"P100 TensorFlow性能优化配置完成\")\n",
        "    \n",
        "    def clear_memory_cache(self):\n",
        "        \"\"\"清理内存缓存\"\"\"\n",
        "        self.memory_cache.clear()\n",
        "        gc.collect()\n",
        "        logger.info(\"已清理内存缓存\")\n",
        "    \n",
        "    def monitor_performance(self, stage_name=\"\"):\n",
        "        \"\"\"监控性能指标\"\"\"\n",
        "        try:\n",
        "            process = psutil.Process()\n",
        "            memory_info = process.memory_info()\n",
        "            memory_mb = memory_info.rss / 1024 / 1024\n",
        "            cpu_percent = process.cpu_percent()\n",
        "            logger.info(f\"[{stage_name}] 内存: {memory_mb:.1f}MB, CPU: {cpu_percent:.1f}%\")\n",
        "            return memory_mb, cpu_percent\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"性能监控失败: {e}\")\n",
        "            return 0, 0\n",
        "\n",
        "class ProcessOptimizer:\n",
        "    \"\"\"进程优化器，用于暂停非必要的进程以释放CPU资源\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.suspended_processes = []\n",
        "        self.process_whitelist = [\n",
        "            'python', 'python3', 'nvidia-smi', 'ssh', 'systemd', 'kernel', \n",
        "            'kthread', 'migration', 'rcu_', 'watchdog', 'bash', 'sh'\n",
        "        ]\n",
        "    \n",
        "    def find_resource_heavy_processes(self, cpu_threshold=20.0, exclude_current=True):\n",
        "        \"\"\"找到占用CPU资源较多的非必要进程\"\"\"\n",
        "        heavy_processes = []\n",
        "        current_pid = os.getpid()\n",
        "        \n",
        "        try:\n",
        "            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'username']):\n",
        "                try:\n",
        "                    proc_info = proc.info\n",
        "                    \n",
        "                    if exclude_current and proc_info['pid'] == current_pid:\n",
        "                        continue\n",
        "                    \n",
        "                    if any(keyword in proc_info['name'].lower() for keyword in self.process_whitelist):\n",
        "                        continue\n",
        "                    \n",
        "                    cpu_percent = proc_info['cpu_percent']\n",
        "                    if cpu_percent is not None and cpu_percent > cpu_threshold:\n",
        "                        heavy_processes.append({\n",
        "                            'pid': proc_info['pid'],\n",
        "                            'name': proc_info['name'],\n",
        "                            'cpu_percent': cpu_percent,\n",
        "                            'username': proc_info['username']\n",
        "                        })\n",
        "                        \n",
        "                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "                    continue\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"查找资源密集进程时出错: {e}\")\n",
        "        \n",
        "        return heavy_processes\n",
        "    \n",
        "    def optimize_for_training(self, auto_suspend=False, cpu_threshold=25.0):\n",
        "        \"\"\"为训练优化系统进程 (Kaggle环境中禁用自动暂停)\"\"\"\n",
        "        logger.info(\"检查系统进程状态...\")\n",
        "        \n",
        "        if not auto_suspend:\n",
        "            logger.info(\"在Kaggle环境中禁用进程优化功能\")\n",
        "            return []\n",
        "        \n",
        "        # Kaggle环境不执行实际的进程优化\n",
        "        return []\n",
        "    \n",
        "    def restore_all_processes(self):\n",
        "        \"\"\"恢复所有被暂停的进程 (Kaggle环境中为空操作)\"\"\"\n",
        "        if not self.suspended_processes:\n",
        "            return\n",
        "        \n",
        "        logger.info(\"在Kaggle环境中无需恢复进程\")\n",
        "\n",
        "# 创建全局实例\n",
        "performance_optimizer = PerformanceOptimizer()\n",
        "process_optimizer = ProcessOptimizer()\n",
        "\n",
        "logger.info(\"性能优化和进程管理模块加载完成\")\n",
        "print(\"性能优化器已初始化，适配Kaggle环境\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: 完整的主执行逻辑 (main.py核心功能)\n",
        "import atexit\n",
        "import traceback\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 从主函数main.py移植的核心常量和工具函数\n",
        "LOG_EPSILON = 1.0  # 对数变换常量\n",
        "\n",
        "def rosenbrock_function(x):\n",
        "    \"\"\"计算给定向量x的Rosenbrock函数值\"\"\"\n",
        "    return np.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0, axis=0)\n",
        "\n",
        "def rosenbrock_evaluator(x_vector):\n",
        "    \"\"\"外部评估器\"\"\"\n",
        "    return rosenbrock_function(x_vector)\n",
        "\n",
        "def calculate_pearson_correlation(model, data_manager):\n",
        "    \"\"\"计算模型预测与真值的皮尔逊相关系数\"\"\"\n",
        "    if data_manager.num_samples < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        X_data = data_manager.get_all_x_orig()\n",
        "        y_data_orig = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        predictions_log = model.predict(X_data)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_data_orig) < 2:\n",
        "            logger.warning(\"数据点不足，无法计算皮尔逊相关系数\")\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_data_orig)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"计算皮尔逊相关系数时出错: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_test_set(model, X_test, y_test):\n",
        "    \"\"\"计算测试集上的皮尔逊相关系数\"\"\"\n",
        "    if X_test is None or y_test is None or X_test.shape[0] < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(X_test)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        y_test_flat = y_test.flatten()\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_test_flat) < 2:\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_test_flat)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    except Exception as e:\n",
        "        logger.error(f\"计算测试集皮尔逊相关系数时出错: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_cnn_performance(model, data_manager, results_dir, iteration, X_test=None, y_test_orig=None, should_generate_plots=True):\n",
        "    \"\"\"评估CNN性能\"\"\"\n",
        "    logger.info(f\"评估CNN性能 (迭代 {iteration})...\")\n",
        "    metrics = {}\n",
        "    \n",
        "    # 训练集评估\n",
        "    X_train = data_manager.get_all_x_orig()\n",
        "    y_train_orig = data_manager.get_all_y_orig().flatten()\n",
        "    \n",
        "    train_predictions_log = model.predict(X_train)\n",
        "    train_predictions_orig = safe_power10(train_predictions_log)\n",
        "    \n",
        "    if should_generate_plots:\n",
        "        train_mse, train_rmse, train_mae = plot_prediction_vs_truth(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration, \n",
        "            filename=f\"train_prediction_vs_truth_iter_{iteration}.png\"\n",
        "        )\n",
        "        plot_residuals(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration,\n",
        "            filename=f\"train_residual_analysis_iter_{iteration}.png\"\n",
        "        )\n",
        "    else:\n",
        "        train_mse = np.mean((train_predictions_orig - y_train_orig) ** 2)\n",
        "        train_rmse = np.sqrt(train_mse)\n",
        "        train_mae = np.mean(np.abs(train_predictions_orig - y_train_orig))\n",
        "        \n",
        "    metrics.update({\n",
        "        'train_mse': train_mse, 'train_rmse': train_rmse, 'train_mae': train_mae,\n",
        "        'train_sample_count': len(y_train_orig)\n",
        "    })\n",
        "    \n",
        "    # 测试集评估\n",
        "    if X_test is not None and y_test_orig is not None and X_test.shape[0] > 0:\n",
        "        test_predictions_log = model.predict(X_test)\n",
        "        test_predictions_orig = safe_power10(test_predictions_log)\n",
        "        test_ground_truth_orig = y_test_orig.flatten()\n",
        "        \n",
        "        if should_generate_plots:\n",
        "            test_mse, test_rmse, test_mae = plot_prediction_vs_truth(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_prediction_vs_truth_iter_{iteration}.png\"\n",
        "            )\n",
        "            plot_residuals(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_residual_analysis_iter_{iteration}.png\"\n",
        "            )\n",
        "        else:\n",
        "            test_mse = np.mean((test_predictions_orig - test_ground_truth_orig) ** 2)\n",
        "            test_rmse = np.sqrt(test_mse)\n",
        "            test_mae = np.mean(np.abs(test_predictions_orig - test_ground_truth_orig))\n",
        "            \n",
        "        if len(test_predictions_orig) >= 2 and len(test_ground_truth_orig) >= 2:\n",
        "            test_pearson, _ = pearsonr(test_predictions_orig, test_ground_truth_orig)\n",
        "            test_pearson = 0.0 if np.isnan(test_pearson) else test_pearson\n",
        "        else:\n",
        "            test_pearson = 0.0\n",
        "        metrics.update({\n",
        "            'test_mse': test_mse, 'test_rmse': test_rmse, 'test_mae': test_mae,\n",
        "            'test_pearson': test_pearson, 'test_sample_count': len(test_ground_truth_orig)\n",
        "        })\n",
        "    \n",
        "    logger.info(f\"CNN性能 (迭代{iteration}): 训练样本: {metrics.get('train_sample_count')}, 训练RMSE: {metrics.get('train_rmse'):.4e}\")\n",
        "    if 'test_rmse' in metrics:\n",
        "        logger.info(f\"  测试样本: {metrics.get('test_sample_count')}, 测试RMSE: {metrics.get('test_rmse'):.4e}, 测试Pearson: {metrics.get('test_pearson'):.4f}\")\n",
        "    return metrics\n",
        "\n",
        "def calculate_pearson_on_best_samples(model, data_manager, n_best=60):\n",
        "    \"\"\"计算最优样本上的皮尔逊相关系数\"\"\"\n",
        "    all_X = data_manager.get_all_x_orig()\n",
        "    all_y_orig = data_manager.get_all_y_orig()\n",
        "    \n",
        "    sorted_indices = np.argsort(all_y_orig.flatten())\n",
        "    sorted_X = all_X[sorted_indices[:n_best]]\n",
        "    sorted_y_orig = all_y_orig[sorted_indices[:n_best]]\n",
        "    \n",
        "    if sorted_X.shape[0] < 2:\n",
        "        return 0.0, float('nan'), float('nan')\n",
        "\n",
        "    min_y_orig_for_range = np.min(sorted_y_orig)\n",
        "    max_y_orig_for_range = np.max(sorted_y_orig)\n",
        "\n",
        "    try:\n",
        "        predictions_log = model.predict(sorted_X)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if np.any(np.isinf(predictions_orig)) or np.any(np.isnan(predictions_orig)):\n",
        "            logger.warning(\"预测值中存在无穷大或NaN值\")\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"计算预测值时出错: {e}\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    \n",
        "    true_values_orig = sorted_y_orig.flatten()\n",
        "    \n",
        "    if len(predictions_orig) < 2 or len(true_values_orig) < 2:\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    \n",
        "    if np.var(predictions_orig) == 0 or np.var(true_values_orig) == 0:\n",
        "        logger.warning(\"预测值或真实值没有变化，无法计算相关系数\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "        \n",
        "    try:\n",
        "        correlation, _ = pearsonr(predictions_orig, true_values_orig)\n",
        "        if np.isnan(correlation) or np.isinf(correlation):\n",
        "            return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"计算Pearson相关系数时出错: {e}\")\n",
        "        return 0.0, min_y_orig_for_range, max_y_orig_for_range\n",
        "    \n",
        "    return (correlation if not np.isnan(correlation) else 0.0), min_y_orig_for_range, max_y_orig_for_range\n",
        "\n",
        "class NTESurrogateWrapper:\n",
        "    \"\"\"NTE搜索器的代理模型包装器\"\"\"\n",
        "    def __init__(self, actual_surrogate_model, data_manager_instance):\n",
        "        self.surrogate_model = actual_surrogate_model\n",
        "        self.dm = data_manager_instance\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:\n",
        "        if X_batch_orig.shape[0] == 0:\n",
        "            return np.array([])\n",
        "        if X_batch_orig.ndim == 1:\n",
        "             X_batch_orig = X_batch_orig.reshape(1, -1)\n",
        "\n",
        "        try:\n",
        "            pred_log = self.surrogate_model.predict(X_batch_orig)\n",
        "            \n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return pred_log[0] if isinstance(pred_log, np.ndarray) else pred_log\n",
        "            else:\n",
        "                return pred_log\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"代理模型预测错误: {e}\")\n",
        "            \n",
        "            # 出错时返回保守的高值\n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return np.log10(1e6 + 1.0)\n",
        "            else:\n",
        "                return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)\n",
        "\n",
        "def save_iteration_samples_to_csv(X_batch, y_batch, results_dir, iteration):\n",
        "    \"\"\"保存迭代样本到CSV文件\"\"\"\n",
        "    try:\n",
        "        csv_filename = os.path.join(results_dir, f\"iteration_{iteration}_samples.csv\")\n",
        "        \n",
        "        data_dict = {}\n",
        "        for i in range(X_batch.shape[1]):\n",
        "            data_dict[f'x{i+1}'] = X_batch[:, i]\n",
        "        data_dict['y_true'] = y_batch.flatten()\n",
        "        \n",
        "        df = pd.DataFrame(data_dict)\n",
        "        df.to_csv(csv_filename, index=False)\n",
        "        \n",
        "        logger.info(f\"迭代{iteration}的{len(y_batch)}个样本已保存到: {csv_filename}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"保存样本到CSV失败: {e}\")\n",
        "\n",
        "logger.info(\"主执行逻辑核心功能加载完成\")\n",
        "print(\"完整的main.py核心功能已迁移完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: CNN 1D 代理模型 (cnn_1d_surrogate.py)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class CNN1DSurrogate:\n",
        "    def __init__(self, input_dim, log_transform=True):\n",
        "        \"\"\"\n",
        "        初始化CNN 1D替代模型\n",
        "        参数:\n",
        "            input_dim: 输入维度 (Rosenbrock函数为20维)\n",
        "            log_transform: 是否使用对数变换预处理\n",
        "        \"\"\"\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        # 配置TensorFlow性能优化\n",
        "        performance_optimizer.configure_tensorflow_performance()\n",
        "        \n",
        "        self.model = self._build_cnn_model()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.log_transform = log_transform\n",
        "        self.trained = False\n",
        "        \n",
        "        # 添加数据缓存\n",
        "        self._data_cache_key = None\n",
        "        self._cached_dataset = None\n",
        "    \n",
        "    def _build_cnn_model(self):\n",
        "        \"\"\"构建CNN 1D模型结构\"\"\"\n",
        "        model = Sequential([\n",
        "            # 输入层形状: (dims, 1)\n",
        "            Conv1D(filters=128, kernel_size=3, strides=1, padding='same', \n",
        "                   activation='elu', input_shape=(self.input_dim, 1)),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Dropout(0.2),\n",
        "            \n",
        "            Conv1D(filters=64, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            Dropout(0.2),\n",
        "            \n",
        "            Conv1D(filters=32, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            Conv1D(filters=16, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            MaxPooling1D(pool_size=2),\n",
        "            \n",
        "            Conv1D(filters=8, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            Conv1D(filters=4, kernel_size=3, strides=1, padding='same', activation='elu'),\n",
        "            \n",
        "            Flatten(),\n",
        "            Dense(64, activation='elu'),\n",
        "            Dense(1, activation='linear')\n",
        "        ])\n",
        "        \n",
        "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "    \n",
        "    def train(self, data_manager, epochs=200, batch_size=32, validation_split=0.2, patience=30, verbose=0, save_path=None):\n",
        "        \"\"\"\n",
        "        使用DataManager中的数据训练CNN模型，集成性能优化。\n",
        "        Args:\n",
        "            data_manager (DataManager): 数据管理器对象，包含训练数据\n",
        "            epochs (int): 训练轮数\n",
        "            batch_size (int): 批次大小\n",
        "            validation_split (float): 验证集比例\n",
        "            patience (int): 早停耐心值\n",
        "            verbose (int): 训练进度显示级别\n",
        "            save_path (str, optional): 模型保存路径\n",
        "        Returns:\n",
        "            dict: 包含训练历史信息的字典\n",
        "        \"\"\"\n",
        "        # 监控训练开始时的性能\n",
        "        performance_optimizer.monitor_performance(\"训练开始\")\n",
        "        \n",
        "        # 获取训练数据 - 使用log尺度的数据，避免重复变换\n",
        "        X_train = data_manager.get_all_x_orig()\n",
        "        y_train = data_manager.get_all_y_log()  # 使用已经log变换的数据\n",
        "        \n",
        "        # 如果数据是二维的，确保y_train是一维数组\n",
        "        if y_train.ndim > 1:\n",
        "            y_train = y_train.flatten()\n",
        "        \n",
        "        logger.info(f\"训练数据形状: X={X_train.shape}, y={y_train.shape}\")\n",
        "        \n",
        "        # 检查是否可以使用缓存的预处理数据\n",
        "        data_cache_key = f\"preprocessed_{hash(X_train.tobytes())}_{self.input_dim}\"\n",
        "        cached_data = performance_optimizer.get_cached_data(data_cache_key)\n",
        "        \n",
        "        if cached_data is not None:\n",
        "            logger.info(\"使用缓存的预处理数据\")\n",
        "            X_train_scaled = cached_data\n",
        "        else:\n",
        "            # 标准化输入数据\n",
        "            logger.info(\"进行数据标准化...\")\n",
        "            self.scaler.fit(X_train)\n",
        "            X_train_scaled = self.scaler.transform(X_train)\n",
        "            \n",
        "            # 缓存预处理后的数据\n",
        "            performance_optimizer.cache_data(data_cache_key, X_train_scaled)\n",
        "            logger.info(\"已缓存预处理数据\")\n",
        "        \n",
        "        # 调整输入形状为CNN 1D所需的 (samples, dims, 1)\n",
        "        X_train_scaled = X_train_scaled.reshape(-1, self.input_dim, 1)\n",
        "        \n",
        "        # 使用优化的数据集\n",
        "        dataset_cache_key = f\"dataset_{data_cache_key}_{batch_size}_{validation_split}\"\n",
        "        \n",
        "        if self._cached_dataset is not None and self._data_cache_key == dataset_cache_key:\n",
        "            logger.info(\"使用缓存的数据集\")\n",
        "            train_dataset = self._cached_dataset\n",
        "        else:\n",
        "            logger.info(\"创建优化的训练数据集...\")\n",
        "            \n",
        "            # 分割训练和验证数据\n",
        "            n_samples = len(X_train_scaled)\n",
        "            n_val = int(n_samples * validation_split)\n",
        "            n_train = n_samples - n_val\n",
        "            \n",
        "            # 使用tf.data创建优化的数据集\n",
        "            train_dataset = performance_optimizer.create_optimized_dataset(\n",
        "                X_train_scaled[:n_train], \n",
        "                y_train[:n_train], \n",
        "                batch_size=batch_size, \n",
        "                shuffle=True, \n",
        "                cache=True\n",
        "            )\n",
        "            \n",
        "            val_dataset = performance_optimizer.create_optimized_dataset(\n",
        "                X_train_scaled[n_train:], \n",
        "                y_train[n_train:], \n",
        "                batch_size=batch_size, \n",
        "                shuffle=False, \n",
        "                cache=True\n",
        "            )\n",
        "            \n",
        "            # 缓存数据集\n",
        "            self._cached_dataset = (train_dataset, val_dataset)\n",
        "            self._data_cache_key = dataset_cache_key\n",
        "            logger.info(\"已缓存优化的数据集\")\n",
        "        \n",
        "        if isinstance(self._cached_dataset, tuple):\n",
        "            train_dataset, val_dataset = self._cached_dataset\n",
        "        else:\n",
        "            train_dataset = self._cached_dataset\n",
        "            val_dataset = None\n",
        "        \n",
        "        # 设置早停回调\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, \n",
        "                                       restore_best_weights=True, verbose=1)\n",
        "        \n",
        "        # 监控训练前的性能\n",
        "        performance_optimizer.monitor_performance(\"模型训练前\")\n",
        "        \n",
        "        # 训练模型 - 使用优化的数据集\n",
        "        if val_dataset is not None:\n",
        "            history = self.model.fit(\n",
        "                train_dataset,\n",
        "                validation_data=val_dataset,\n",
        "                epochs=epochs, \n",
        "                callbacks=[early_stopping], \n",
        "                verbose=verbose\n",
        "            )\n",
        "        else:\n",
        "            # 回退到原始方法\n",
        "            history = self.model.fit(\n",
        "                X_train_scaled, y_train, \n",
        "                epochs=epochs, \n",
        "                batch_size=batch_size, \n",
        "                validation_split=validation_split, \n",
        "                callbacks=[early_stopping], \n",
        "                verbose=verbose\n",
        "            )\n",
        "        \n",
        "        self.trained = True\n",
        "        \n",
        "        # 监控训练后的性能\n",
        "        performance_optimizer.monitor_performance(\"模型训练后\")\n",
        "        \n",
        "        # 保存模型（如果指定了路径）\n",
        "        if save_path:\n",
        "            self.model.save(save_path)\n",
        "            \n",
        "        # 清理不必要的内存\n",
        "        performance_optimizer.optimize_memory_usage()\n",
        "            \n",
        "        return history\n",
        "    \n",
        "    def predict(self, states):\n",
        "        \"\"\"\n",
        "        使用训练好的CNN 1D模型预测状态值\n",
        "        参数:\n",
        "            states: 需要预测的状态向量或状态向量列表\n",
        "        返回:\n",
        "            预测的函数值 (log尺度)\n",
        "        \"\"\"\n",
        "        if not self.trained:\n",
        "            raise RuntimeError(\"模型尚未训练，请先训练模型\")\n",
        "            \n",
        "        # 确保输入是numpy数组\n",
        "        states = np.array(states)\n",
        "        \n",
        "        # 输入可能是单个状态或多个状态\n",
        "        single_input = False\n",
        "        if states.ndim == 1:\n",
        "            states = states.reshape(1, -1)\n",
        "            single_input = True\n",
        "            \n",
        "        # 标准化输入\n",
        "        states_scaled = self.scaler.transform(states)\n",
        "        \n",
        "        # 调整形状为CNN 1D所需的 (samples, dims, 1)\n",
        "        states_scaled = states_scaled.reshape(-1, self.input_dim, 1)\n",
        "        \n",
        "        # 模型预测 - 返回log尺度的预测值\n",
        "        predictions = self.model.predict(states_scaled, verbose=0)\n",
        "        \n",
        "        # 不进行逆变换，直接返回log尺度的预测值\n",
        "        return predictions[0][0] if single_input else predictions.flatten()\n",
        "    \n",
        "    def visualize_training_history(self, history, save_path=None):\n",
        "        \"\"\"\n",
        "        可视化训练历史\n",
        "        参数:\n",
        "            history: 训练返回的历史对象\n",
        "            save_path: 图表保存路径\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        \n",
        "        # 损失曲线\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Loss Curves')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss (MSE)')\n",
        "        plt.legend()\n",
        "        \n",
        "        # MAE曲线\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['mae'], label='Training MAE')\n",
        "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
        "        plt.title('MAE Curves')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Mean Absolute Error')\n",
        "        plt.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_path:\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            plt.savefig(save_path)\n",
        "            \n",
        "        plt.show()\n",
        "\n",
        "logger.info(\"CNN1DSurrogate模型类加载完成\")\n",
        "print(\"CNN 1D代理模型已准备就绪\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: NTE搜索器 (nte_searcher.py) - 重点调试模块\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import logging\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Any, Callable, Optional, Union\n",
        "\n",
        "# Node数据类\n",
        "@dataclass\n",
        "class Node:\n",
        "    state: Tuple[float, ...]  # 状态向量 (tuple for hashability)\n",
        "    value_pred: float = float('inf')  # 替代模型预测 (v_ML)\n",
        "    visit_count: int = 0  # 访问次数\n",
        "\n",
        "class NTESearcher:\n",
        "    \"\"\"\n",
        "    实现NTE (Neural-Surrogate-Guided Tree Exploration) 搜索策略\n",
        "    重点用于调试动态探索因子参数\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dimension: int,\n",
        "                 domain: List[Tuple[float, float]],\n",
        "                 c0: float = NTE_C0,\n",
        "                 rollout_rounds: int = NTE_ROLLOUT_ROUNDS,\n",
        "                 validation_batch_size: int = 20):\n",
        "        \"\"\"\n",
        "        初始化NTE搜索器\n",
        "        Args:\n",
        "            dimension (int): 状态向量维度\n",
        "            domain (List[Tuple[float, float]]): 每个维度的范围 [(min1, max1), ...]\n",
        "            c0 (float): DUCB公式中的探索超参数\n",
        "            rollout_rounds (int): 每次搜索调用的内部探索轮数\n",
        "            validation_batch_size (int): 每次搜索后选择的候选数量\n",
        "        \"\"\"\n",
        "        self.dimension = dimension\n",
        "        self.domain = domain\n",
        "        self.domain_mins = np.array([d[0] for d in domain])\n",
        "        self.domain_maxs = np.array([d[1] for d in domain])\n",
        "        self.base_c0 = c0  # 存储基础 c0 值\n",
        "        self.rollout_rounds = rollout_rounds\n",
        "        self.validation_batch_size = validation_batch_size\n",
        "        \n",
        "        # 初始测试范围\n",
        "        self.initial_range_length = self.domain_maxs - self.domain_mins\n",
        "        \n",
        "        logger.info(f\"NTESearcher initialized: dim={dimension}, c0={c0}, rollout_rounds={rollout_rounds}\")\n",
        "\n",
        "    def _calculate_dynamic_search_space(self, all_states: np.ndarray = None, all_values: np.ndarray = None, iteration: int = 1):\n",
        "        \"\"\"\n",
        "        根据当前数据集中y值最小的20个样本计算动态搜索区间\n",
        "        Args:\n",
        "            all_states: 所有已知状态\n",
        "            all_values: 所有已知状态对应的函数值\n",
        "            iteration: 当前迭代次数，用于计算拓展区间\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]: 返回动态搜索区间的上下界\n",
        "        \"\"\"\n",
        "        # 默认使用初始搜索区间\n",
        "        dynamic_mins = self.domain_mins.copy()\n",
        "        dynamic_maxs = self.domain_maxs.copy()\n",
        "        \n",
        "        # 如果没有提供状态和值，返回原始区间\n",
        "        if all_states is None or all_values is None or all_states.shape[0] == 0:\n",
        "            return dynamic_mins, dynamic_maxs\n",
        "            \n",
        "        try:\n",
        "            # 选择y值最小的20个样本\n",
        "            num_samples_to_use = min(20, all_values.shape[0])\n",
        "            sorted_indices = np.argsort(all_values.flatten())[:num_samples_to_use]\n",
        "            best_samples = all_states[sorted_indices]\n",
        "            \n",
        "            # 计算这些样本在各个维度上的最小值和最大值\n",
        "            sample_mins = np.min(best_samples, axis=0)\n",
        "            sample_maxs = np.max(best_samples, axis=0)\n",
        "            \n",
        "            # 计算基本区间长度l_i和区间中心a_i\n",
        "            interval_lengths = sample_maxs - sample_mins\n",
        "            interval_centers = (sample_maxs + sample_mins) / 2.0\n",
        "            \n",
        "            # 计算拓展区间大小d，随迭代次数递减\n",
        "            extension = self.initial_range_length / (2 * max(1, iteration))\n",
        "            \n",
        "            # 确保最小区间长度\n",
        "            min_interval_length = 3.0\n",
        "            final_intervals = np.maximum(interval_lengths + 2 * extension, min_interval_length)\n",
        "            \n",
        "            # 计算动态搜索区间\n",
        "            dynamic_mins = np.maximum(interval_centers - final_intervals / 2.0, self.domain_mins)\n",
        "            dynamic_maxs = np.minimum(interval_centers + final_intervals / 2.0, self.domain_maxs)\n",
        "            \n",
        "            return dynamic_mins, dynamic_maxs\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"计算动态搜索区间时出错: {str(e)}\")\n",
        "            return self.domain_mins.copy(), self.domain_maxs.copy()\n",
        "\n",
        "    def _mixed_expansion(self, state_vector: np.ndarray, dynamic_factor: float = 1.0, \n",
        "                        all_states: np.ndarray = None, all_values: np.ndarray = None, \n",
        "                        iteration: int = 1, pre_computed_bounds: tuple = None) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        混合膨胀策略：结合确定性和随机方法生成叶节点\n",
        "        \"\"\"\n",
        "        if pre_computed_bounds is not None:\n",
        "            dynamic_mins, dynamic_maxs = pre_computed_bounds\n",
        "        else:\n",
        "            dynamic_mins, dynamic_maxs = self._calculate_dynamic_search_space(all_states, all_values, iteration)\n",
        "        \n",
        "        # 基础步长：根据动态因子和搜索区间调整\n",
        "        base_step_size = 0.1 * (dynamic_maxs - dynamic_mins) * dynamic_factor\n",
        "        \n",
        "        leaf_states = []\n",
        "        \n",
        "        # 1. 确定性移动 (4个方向，每个维度随机选择)\n",
        "        num_deterministic = 4\n",
        "        for _ in range(num_deterministic):\n",
        "            # 随机选择一个维度\n",
        "            dim_idx = random.randint(0, self.dimension - 1)\n",
        "            direction = random.choice([-1, 1])\n",
        "            \n",
        "            new_state = state_vector.copy()\n",
        "            step_size = base_step_size[dim_idx]\n",
        "            new_state[dim_idx] += direction * step_size\n",
        "            \n",
        "            # 确保在域范围内\n",
        "            new_state = np.clip(new_state, dynamic_mins, dynamic_maxs)\n",
        "            leaf_states.append(new_state)\n",
        "        \n",
        "        # 2. 随机膨胀 (4个随机点)\n",
        "        num_random = 4\n",
        "        for _ in range(num_random):\n",
        "            # 在当前状态附近生成随机点\n",
        "            noise = np.random.normal(0, base_step_size * 0.5, self.dimension)\n",
        "            new_state = state_vector + noise\n",
        "            \n",
        "            # 确保在域范围内\n",
        "            new_state = np.clip(new_state, dynamic_mins, dynamic_maxs)\n",
        "            leaf_states.append(new_state)\n",
        "        \n",
        "        return leaf_states\n",
        "\n",
        "    def _calculate_ducb(self, node: Node, parent_total_visits: int, c_rho: float, current_c0: float) -> float:\n",
        "        \"\"\"\n",
        "        计算DUCB (Discriminative Upper Confidence Bound) 值\n",
        "        核心公式：DUCB = -pred_value + c0 * c(rho) * sqrt(ln(N)/n)\n",
        "        \"\"\"\n",
        "        if node.visit_count == 0:\n",
        "            return float('inf')  # 未访问的节点优先级最高\n",
        "        \n",
        "        # 探索项\n",
        "        exploration_term = current_c0 * c_rho * math.sqrt(math.log(max(1, parent_total_visits)) / node.visit_count)\n",
        "        \n",
        "        # DUCB = 负预测值 + 探索项 (因为我们要最小化)\n",
        "        ducb_value = -node.value_pred + exploration_term\n",
        "        \n",
        "        return ducb_value\n",
        "\n",
        "    def _select_candidates_mixed_strategy(self, visited_nodes: Dict[Tuple[float, ...], Node]) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        混合策略选择候选点：结合DUCB值和预测值\n",
        "        \"\"\"\n",
        "        if not visited_nodes:\n",
        "            return []\n",
        "        \n",
        "        nodes_list = list(visited_nodes.values())\n",
        "        \n",
        "        # 按预测值排序，选择前15个最优点\n",
        "        nodes_by_pred = sorted(nodes_list, key=lambda n: n.value_pred)\n",
        "        best_by_pred = nodes_by_pred[:min(15, len(nodes_by_pred))]\n",
        "        \n",
        "        # 计算总访问次数\n",
        "        total_visits = sum(node.visit_count for node in nodes_list)\n",
        "        \n",
        "        # 按DUCB值排序，选择前5个高DUCB点\n",
        "        c_rho = 1.0  # 简化的c_rho值\n",
        "        current_c0 = self.base_c0\n",
        "        \n",
        "        nodes_with_ducb = []\n",
        "        for node in nodes_list:\n",
        "            ducb_val = self._calculate_ducb(node, total_visits, c_rho, current_c0)\n",
        "            nodes_with_ducb.append((node, ducb_val))\n",
        "        \n",
        "        nodes_with_ducb.sort(key=lambda x: x[1], reverse=True)  # 按DUCB降序\n",
        "        best_by_ducb = [item[0] for item in nodes_with_ducb[:5]]\n",
        "        \n",
        "        # 合并并去重\n",
        "        selected_nodes = best_by_pred + best_by_ducb\n",
        "        unique_states = []\n",
        "        seen_states = set()\n",
        "        \n",
        "        for node in selected_nodes:\n",
        "            if node.state not in seen_states:\n",
        "                unique_states.append(np.array(node.state))\n",
        "                seen_states.add(node.state)\n",
        "        \n",
        "        return unique_states[:self.validation_batch_size]\n",
        "\n",
        "    def search(self,\n",
        "               surrogate_model: Any, \n",
        "               current_best_state: np.ndarray,\n",
        "               min_y_observed: float,\n",
        "               dynamic_exploration_factor: float = 1.0,\n",
        "               all_states: np.ndarray = None, \n",
        "               all_values: np.ndarray = None,\n",
        "               previous_iteration_samples: np.ndarray = None,\n",
        "               current_iteration: int = 1,\n",
        "               return_ducb_trends: bool = False) -> Union[List[np.ndarray], Tuple[List[np.ndarray], List[Dict]]]:\n",
        "        \"\"\"\n",
        "        改进版的NTE搜索算法\n",
        "        \n",
        "        重点调试参数：\n",
        "        - dynamic_exploration_factor: 动态探索因子，影响搜索的探索强度\n",
        "        \n",
        "        Args:\n",
        "            surrogate_model: 替代模型，必须有predict方法\n",
        "            current_best_state: 当前已知最优状态\n",
        "            min_y_observed: 当前已知最小值\n",
        "            dynamic_exploration_factor: 动态探索因子 (重点调试参数)\n",
        "            all_states: 所有已知状态点\n",
        "            all_values: 所有已知状态值\n",
        "            previous_iteration_samples: 上一次迭代选择的样本\n",
        "            current_iteration: 当前迭代次数\n",
        "            return_ducb_trends: 是否返回DUCB趋势数据\n",
        "        \"\"\"\n",
        "        logger.info(f\"NTE搜索开始: 迭代{current_iteration}, 动态探索因子={dynamic_exploration_factor:.2f}\")\n",
        "        \n",
        "        # DUCB趋势数据收集\n",
        "        ducb_trends_data = [] if return_ducb_trends else None\n",
        "        \n",
        "        # 动态调整c0\n",
        "        current_c0 = self.base_c0 * dynamic_exploration_factor\n",
        "        c_rho = 1.0  # 简化的rho调整因子\n",
        "        \n",
        "        logger.info(f\"调整后的c0值: {current_c0:.2f} (基础c0={self.base_c0}, 动态因子={dynamic_exploration_factor:.2f})\")\n",
        "        \n",
        "        # 选择起始点\n",
        "        best_states_for_nte = []\n",
        "        if all_states is not None and all_values is not None and all_states.shape[0] > 0:\n",
        "            # 选择2个最优点\n",
        "            num_best = min(2, all_states.shape[0])\n",
        "            sorted_indices = np.argsort(all_values.flatten())\n",
        "            for idx in sorted_indices[:num_best]:\n",
        "                best_states_for_nte.append(all_states[idx])\n",
        "            \n",
        "            # 选择1个随机点\n",
        "            if all_states.shape[0] > num_best:\n",
        "                remaining_indices = sorted_indices[num_best:]\n",
        "                random_idx = np.random.choice(remaining_indices, size=1)[0]\n",
        "                best_states_for_nte.append(all_states[random_idx])\n",
        "        else:\n",
        "            # 使用当前最优状态或生成随机状态\n",
        "            if current_best_state is not None:\n",
        "                best_states_for_nte.append(current_best_state)\n",
        "            else:\n",
        "                random_state = np.array([random.uniform(self.domain_mins[d], self.domain_maxs[d]) \n",
        "                                       for d in range(self.dimension)])\n",
        "                best_states_for_nte.append(random_state)\n",
        "        \n",
        "        logger.info(f\"使用 {len(best_states_for_nte)} 个起始点进行NTE搜索\")\n",
        "        \n",
        "        # 计算动态搜索区间\n",
        "        try:\n",
        "            dynamic_mins, dynamic_maxs = self._calculate_dynamic_search_space(all_states, all_values, current_iteration)\n",
        "            pre_computed_bounds = (dynamic_mins, dynamic_maxs)\n",
        "            \n",
        "            original_range = np.mean(self.domain_maxs - self.domain_mins)\n",
        "            dynamic_range = np.mean(dynamic_maxs - dynamic_mins)\n",
        "            logger.info(f\"搜索区间: 原始范围={original_range:.2f}, 动态范围={dynamic_range:.2f}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"计算动态搜索区间失败: {e}\")\n",
        "            pre_computed_bounds = None\n",
        "        \n",
        "        # 对每个起始点进行NTE搜索\n",
        "        all_candidates = []\n",
        "        \n",
        "        for nte_id, root_state_vector in enumerate(best_states_for_nte):\n",
        "            visited_nodes: Dict[Tuple[float, ...], Node] = {}\n",
        "            root_state_tuple = tuple(root_state_vector)\n",
        "            \n",
        "            # 初始化根节点\n",
        "            try:\n",
        "                root_pred_value = surrogate_model.predict(np.array([root_state_vector]))\n",
        "                if isinstance(root_pred_value, (np.ndarray, list)):\n",
        "                    root_pred_value = float(root_pred_value.item() if hasattr(root_pred_value, 'item') else root_pred_value[0])\n",
        "                else:\n",
        "                    root_pred_value = float(root_pred_value)\n",
        "                \n",
        "                if np.isnan(root_pred_value) or np.isinf(root_pred_value):\n",
        "                    root_pred_value = float('inf')\n",
        "            except Exception as e:\n",
        "                logger.error(f\"预测根节点值错误: {e}\")\n",
        "                root_pred_value = float('inf')\n",
        "            \n",
        "            root_node = Node(state=root_state_tuple, value_pred=root_pred_value, visit_count=0)\n",
        "            visited_nodes[root_state_tuple] = root_node\n",
        "            \n",
        "            logger.debug(f\"起始点 {nte_id+1}: 预测值={root_pred_value:.4e}\")\n",
        "            \n",
        "            # Rollout阶段\n",
        "            for rollout_round in range(self.rollout_rounds):\n",
        "                current_root_node = visited_nodes[root_state_tuple]\n",
        "                current_root_node.visit_count += 1\n",
        "                current_root_state_vector = np.array(current_root_node.state)\n",
        "                \n",
        "                # 生成叶节点\n",
        "                leaf_states = self._mixed_expansion(\n",
        "                    current_root_state_vector, \n",
        "                    dynamic_exploration_factor,\n",
        "                    all_states, \n",
        "                    all_values, \n",
        "                    current_iteration,\n",
        "                    pre_computed_bounds=pre_computed_bounds\n",
        "                )\n",
        "                \n",
        "                # 预测叶节点值\n",
        "                try:\n",
        "                    leaf_states_batch = np.array(leaf_states)\n",
        "                    leaf_pred_values = surrogate_model.predict(leaf_states_batch)\n",
        "                    if not isinstance(leaf_pred_values, np.ndarray):\n",
        "                        leaf_pred_values = np.array(leaf_pred_values)\n",
        "                    \n",
        "                    # 处理预测值\n",
        "                    leaf_pred_values = np.nan_to_num(leaf_pred_values, nan=float('inf'), \n",
        "                                                   posinf=float('inf'), neginf=float('inf'))\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"叶节点预测失败: {e}\")\n",
        "                    leaf_pred_values = np.full(len(leaf_states), float('inf'))\n",
        "                \n",
        "                # 添加叶节点到visited_nodes\n",
        "                for leaf_state, pred_value in zip(leaf_states, leaf_pred_values):\n",
        "                    leaf_tuple = tuple(leaf_state)\n",
        "                    if leaf_tuple not in visited_nodes:\n",
        "                        visited_nodes[leaf_tuple] = Node(state=leaf_tuple, value_pred=float(pred_value), visit_count=0)\n",
        "                \n",
        "                # 收集DUCB趋势数据\n",
        "                if return_ducb_trends and rollout_round % 10 == 0:  # 每10轮记录一次\n",
        "                    total_visits = sum(node.visit_count for node in visited_nodes.values())\n",
        "                    ducb_values = []\n",
        "                    for node in visited_nodes.values():\n",
        "                        ducb_val = self._calculate_ducb(node, total_visits, c_rho, current_c0)\n",
        "                        ducb_values.append(ducb_val)\n",
        "                    \n",
        "                    ducb_trends_data.append({\n",
        "                        'nte_id': nte_id,\n",
        "                        'rollout_round': rollout_round,\n",
        "                        'max_ducb': max(ducb_values) if ducb_values else 0,\n",
        "                        'min_ducb': min(ducb_values) if ducb_values else 0,\n",
        "                        'avg_ducb': np.mean(ducb_values) if ducb_values else 0,\n",
        "                        'num_nodes': len(visited_nodes),\n",
        "                        'dynamic_factor': dynamic_exploration_factor,\n",
        "                        'current_c0': current_c0\n",
        "                    })\n",
        "            \n",
        "            # 选择候选点\n",
        "            nte_candidates = self._select_candidates_mixed_strategy(visited_nodes)\n",
        "            all_candidates.extend(nte_candidates)\n",
        "            \n",
        "            logger.info(f\"NTE {nte_id+1} 完成: 访问了{len(visited_nodes)}个节点, 选择了{len(nte_candidates)}个候选点\")\n",
        "        \n",
        "        # 去重并限制数量\n",
        "        final_candidates = []\n",
        "        seen_states = set()\n",
        "        \n",
        "        for candidate in all_candidates:\n",
        "            candidate_tuple = tuple(candidate)\n",
        "            if candidate_tuple not in seen_states:\n",
        "                final_candidates.append(candidate)\n",
        "                seen_states.add(candidate_tuple)\n",
        "                \n",
        "                if len(final_candidates) >= self.validation_batch_size:\n",
        "                    break\n",
        "        \n",
        "        logger.info(f\"NTE搜索完成: 总共返回{len(final_candidates)}个候选点\")\n",
        "        \n",
        "        if return_ducb_trends:\n",
        "            return final_candidates, ducb_trends_data\n",
        "        else:\n",
        "            return final_candidates\n",
        "\n",
        "logger.info(\"NTESearcher类加载完成 - 重点调试模块已准备就绪\")\n",
        "print(\"NTE搜索器已准备就绪 - 动态探索因子可调试\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: 可视化工具 (plot_utils.py) - 简化版\n",
        "import os\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import matplotlib\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# 配置全局字体设置为默认英文字体 (避免Kaggle中文字体问题)\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']\n",
        "plt.switch_backend('Agg')  # 使用Agg后端，适合无GUI环境\n",
        "\n",
        "def create_results_directory(base_results_path):\n",
        "    \"\"\"创建带时间戳的结果目录\"\"\"\n",
        "    try:\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        results_dir = os.path.join(base_results_path, timestamp)\n",
        "        os.makedirs(results_dir, exist_ok=True)\n",
        "        logger.info(f\"Results directory created: {results_dir}\")\n",
        "        return results_dir\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating results directory: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_global_min_value_trend(iterations, min_values, output_dir, filename=\"global_min_value_trend.png\"):\n",
        "    \"\"\"绘制全局最小值趋势图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        logger.warning(f\"Output directory {output_dir} does not exist. Skipping plot.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 确保数组长度相同\n",
        "        if len(iterations) != len(min_values):\n",
        "            min_length = min(len(iterations), len(min_values))\n",
        "            iterations = iterations[:min_length]\n",
        "            min_values = min_values[:min_length]\n",
        "            \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(iterations, min_values, marker='o', linestyle='-', color='b', linewidth=2)\n",
        "        plt.title('Global Minimum Value Trend', fontsize=16)\n",
        "        plt.xlabel('Iteration', fontsize=14)\n",
        "        plt.ylabel('Current Minimum True Value Found', fontsize=14)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 只在找到更低全局最小值时添加标注\n",
        "        min_values_array = np.array(min_values)\n",
        "        global_min = float('inf')\n",
        "        \n",
        "        for i, (iter_num, current_min) in enumerate(zip(iterations, min_values_array)):\n",
        "            if current_min < global_min:\n",
        "                global_min = current_min\n",
        "                label_txt = f\"{current_min:.3e}\" if current_min >= 1000 else f\"{current_min:.4f}\"\n",
        "                plt.annotate(label_txt, \n",
        "                            (iter_num, current_min),\n",
        "                            textcoords=\"offset points\", \n",
        "                            xytext=(0,10), \n",
        "                            ha='center',\n",
        "                            fontsize=10,\n",
        "                            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"yellow\", ec=\"orange\", alpha=0.8))\n",
        "\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        logger.info(f\"全局最小值趋势图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制全局最小值趋势图出错: {e}\")\n",
        "\n",
        "def plot_prediction_vs_truth(predictions, ground_truth, output_dir, iteration=None, filename=None, title_suffix=None):\n",
        "    \"\"\"绘制预测值vs真值对比图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return None, None, None\n",
        "    \n",
        "    try:\n",
        "        # 展平数组\n",
        "        predictions_flat = np.array(predictions).flatten()\n",
        "        ground_truth_flat = np.array(ground_truth).flatten()\n",
        "        \n",
        "        # 计算指标\n",
        "        mse = np.mean((predictions_flat - ground_truth_flat) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = np.mean(np.abs(predictions_flat - ground_truth_flat))\n",
        "        \n",
        "        # 绘图\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(ground_truth_flat, predictions_flat, alpha=0.6, s=30)\n",
        "        \n",
        "        # 添加对角线 (理想预测线)\n",
        "        min_val = min(np.min(ground_truth_flat), np.min(predictions_flat))\n",
        "        max_val = max(np.max(ground_truth_flat), np.max(predictions_flat))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "        \n",
        "        plt.xlabel('Ground Truth', fontsize=14)\n",
        "        plt.ylabel('Predictions', fontsize=14)\n",
        "        title = f'Prediction vs Truth Comparison'\n",
        "        if title_suffix:\n",
        "            title += f' - {title_suffix}'\n",
        "        if iteration is not None:\n",
        "            title += f' (Iteration {iteration})'\n",
        "        plt.title(title, fontsize=16)\n",
        "        \n",
        "        # 添加指标文本\n",
        "        plt.text(0.05, 0.95, f'RMSE: {rmse:.4e}\\nMAE: {mae:.4e}\\nMSE: {mse:.4e}', \n",
        "                transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"prediction_vs_truth_iter_{iteration}.png\" if iteration else \"prediction_vs_truth.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"预测vs真值图已保存: {save_path}\")\n",
        "        return mse, rmse, mae\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制预测vs真值图出错: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def plot_residuals(predictions, ground_truth, output_dir, iteration=None, filename=None):\n",
        "    \"\"\"绘制残差分析图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        predictions_flat = np.array(predictions).flatten()\n",
        "        ground_truth_flat = np.array(ground_truth).flatten()\n",
        "        residuals = predictions_flat - ground_truth_flat\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # 残差散点图\n",
        "        ax1.scatter(ground_truth_flat, residuals, alpha=0.6, s=30)\n",
        "        ax1.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        ax1.set_xlabel('Ground Truth', fontsize=12)\n",
        "        ax1.set_ylabel('Residuals (Predicted - Truth)', fontsize=12)\n",
        "        ax1.set_title('Residuals vs Ground Truth', fontsize=14)\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 残差直方图\n",
        "        ax2.hist(residuals, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "        ax2.set_xlabel('Residuals', fontsize=12)\n",
        "        ax2.set_ylabel('Frequency', fontsize=12)\n",
        "        ax2.set_title('Residuals Distribution', fontsize=14)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 添加统计信息\n",
        "        residual_std = np.std(residuals)\n",
        "        residual_mean = np.mean(residuals)\n",
        "        ax2.text(0.05, 0.95, f'Mean: {residual_mean:.4e}\\nStd: {residual_std:.4e}', \n",
        "                transform=ax2.transAxes, fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"residual_analysis_iter_{iteration}.png\" if iteration else \"residual_analysis.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"残差分析图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制残差分析图出错: {e}\")\n",
        "\n",
        "def plot_pearson_correlation_trend(iterations, pearson_coeffs, output_dir, filename=\"pearson_correlation_trend.png\", title=None):\n",
        "    \"\"\"绘制皮尔逊相关系数趋势图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(iterations, pearson_coeffs, marker='o', linestyle='-', color='g', linewidth=2, markersize=6)\n",
        "        plt.title(title if title else 'Pearson Correlation Coefficient Trend (Model vs. Truth)', fontsize=16)\n",
        "        plt.xlabel('Iteration', fontsize=14)\n",
        "        plt.ylabel('Pearson Correlation Coefficient', fontsize=14)\n",
        "        plt.ylim(-1.1, 1.1)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 添加水平参考线\n",
        "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='r=0.5')\n",
        "        plt.axhline(y=0.8, color='blue', linestyle='--', alpha=0.5, label='r=0.8')\n",
        "        \n",
        "        # 在关键点添加数值标注\n",
        "        for i in range(0, len(iterations), max(1, len(iterations)//10)):  # 每10%的点标注一次\n",
        "            plt.annotate(f'{pearson_coeffs[i]:.3f}', \n",
        "                        (iterations[i], pearson_coeffs[i]),\n",
        "                        textcoords=\"offset points\", \n",
        "                        xytext=(0,10), \n",
        "                        ha='center',\n",
        "                        fontsize=9)\n",
        "        \n",
        "        plt.legend()\n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        logger.info(f\"皮尔逊相关系数趋势图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制皮尔逊相关系数趋势图出错: {e}\")\n",
        "\n",
        "def plot_nte_ducb_trends(ducb_trends_data, output_dir, iteration, dynamic_exploration_factor, \n",
        "                        new_global_best_value=None, filename=None):\n",
        "    \"\"\"绘制NTE搜索的DUCB趋势图\"\"\"\n",
        "    if not ducb_trends_data or not os.path.exists(output_dir):\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        \n",
        "        # 提取数据\n",
        "        rollout_rounds = [d['rollout_round'] for d in ducb_trends_data]\n",
        "        max_ducb_values = [d['max_ducb'] for d in ducb_trends_data]\n",
        "        avg_ducb_values = [d['avg_ducb'] for d in ducb_trends_data]\n",
        "        num_nodes = [d['num_nodes'] for d in ducb_trends_data]\n",
        "        \n",
        "        # 创建子图\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # 1. DUCB值趋势\n",
        "        ax1.plot(rollout_rounds, max_ducb_values, 'r-', marker='o', label='Max DUCB', linewidth=2)\n",
        "        ax1.plot(rollout_rounds, avg_ducb_values, 'b-', marker='s', label='Avg DUCB', linewidth=2)\n",
        "        ax1.set_xlabel('Rollout Round')\n",
        "        ax1.set_ylabel('DUCB Value')\n",
        "        ax1.set_title(f'DUCB Values Trend (Factor: {dynamic_exploration_factor:.2f})')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. 节点数量增长\n",
        "        ax2.plot(rollout_rounds, num_nodes, 'g-', marker='^', linewidth=2)\n",
        "        ax2.set_xlabel('Rollout Round')\n",
        "        ax2.set_ylabel('Number of Nodes Visited')\n",
        "        ax2.set_title('Node Exploration Growth')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 3. 动态因子影响分析\n",
        "        current_c0_values = [d['current_c0'] for d in ducb_trends_data]\n",
        "        ax3.plot(rollout_rounds, current_c0_values, 'm-', marker='d', linewidth=2)\n",
        "        ax3.set_xlabel('Rollout Round')\n",
        "        ax3.set_ylabel('Current C0 Value')\n",
        "        ax3.set_title(f'Dynamic C0 Value (Base×Factor={dynamic_exploration_factor:.2f})')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. 探索效率\n",
        "        exploration_efficiency = [m/n if n > 0 else 0 for m, n in zip(max_ducb_values, num_nodes)]\n",
        "        ax4.plot(rollout_rounds, exploration_efficiency, 'orange', marker='*', linewidth=2)\n",
        "        ax4.set_xlabel('Rollout Round')\n",
        "        ax4.set_ylabel('Max DUCB / Nodes Ratio')\n",
        "        ax4.set_title('Exploration Efficiency')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 添加全局信息\n",
        "        if new_global_best_value is not None:\n",
        "            fig.suptitle(f'NTE DUCB Analysis - Iteration {iteration} (New Best: {new_global_best_value:.4e})', \n",
        "                        fontsize=16, y=0.98)\n",
        "        else:\n",
        "            fig.suptitle(f'NTE DUCB Analysis - Iteration {iteration}', fontsize=16, y=0.98)\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"nte_ducb_trends_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"NTE DUCB趋势图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制NTE DUCB趋势图出错: {e}\")\n",
        "\n",
        "def plot_top_samples_dimensions(data_manager, output_dir, iteration, n_top_samples=15, filename=None):\n",
        "    \"\"\"绘制最优样本的维度分布图\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        all_X = data_manager.get_all_x_orig()\n",
        "        all_y = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        # 选择最优的n个样本\n",
        "        sorted_indices = np.argsort(all_y)[:n_top_samples]\n",
        "        top_samples = all_X[sorted_indices]\n",
        "        top_values = all_y[sorted_indices]\n",
        "        \n",
        "        # 计算每个维度的统计信息\n",
        "        dim_means = np.mean(top_samples, axis=0)\n",
        "        dim_stds = np.std(top_samples, axis=0)\n",
        "        dim_mins = np.min(top_samples, axis=0)\n",
        "        dim_maxs = np.max(top_samples, axis=0)\n",
        "        \n",
        "        dimensions = list(range(len(dim_means)))\n",
        "        \n",
        "        plt.figure(figsize=(15, 8))\n",
        "        \n",
        "        # 误差条图显示均值和标准差\n",
        "        plt.errorbar(dimensions, dim_means, yerr=dim_stds, \n",
        "                    fmt='o', capsize=5, capthick=2, linewidth=2, markersize=8,\n",
        "                    label=f'Mean ± Std (Top {n_top_samples} samples)')\n",
        "        \n",
        "        # 添加最小值最大值范围\n",
        "        plt.fill_between(dimensions, dim_mins, dim_maxs, alpha=0.2, \n",
        "                        label=f'Min-Max Range')\n",
        "        \n",
        "        plt.xlabel('Dimension Index', fontsize=14)\n",
        "        plt.ylabel('Dimension Value', fontsize=14)\n",
        "        plt.title(f'Top {n_top_samples} Samples Dimension Distribution (Iteration {iteration})', fontsize=16)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 添加统计信息文本\n",
        "        best_value = top_values[0]\n",
        "        plt.text(0.02, 0.98, f'Best Value: {best_value:.4e}', \n",
        "                transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
        "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.8))\n",
        "        \n",
        "        if filename is None:\n",
        "            filename = f\"top_samples_dimensions_iter_{iteration}.png\"\n",
        "        \n",
        "        save_path = os.path.join(output_dir, filename)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"维度分布图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制维度分布图出错: {e}\")\n",
        "\n",
        "# 快速可视化函数 - 专为Kaggle环境优化\n",
        "def quick_progress_plot(iterations, best_values, pearson_coeffs, output_dir, iteration):\n",
        "    \"\"\"快速生成进度概览图\"\"\"\n",
        "    try:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # 最优值趋势\n",
        "        ax1.plot(iterations, best_values, 'b-o', linewidth=2, markersize=6)\n",
        "        ax1.set_xlabel('Iteration')\n",
        "        ax1.set_ylabel('Best Value Found')\n",
        "        ax1.set_title('Optimization Progress')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.set_yscale('log')  # 使用对数尺度\n",
        "        \n",
        "        # 相关系数趋势\n",
        "        ax2.plot(iterations, pearson_coeffs, 'g-s', linewidth=2, markersize=6)\n",
        "        ax2.set_xlabel('Iteration')\n",
        "        ax2.set_ylabel('Pearson Correlation')\n",
        "        ax2.set_title('Model Quality')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        ax2.set_ylim(0, 1)\n",
        "        \n",
        "        plt.suptitle(f'DANTE Progress Overview - Iteration {iteration}', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        save_path = os.path.join(output_dir, f\"progress_overview_iter_{iteration}.png\")\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        logger.info(f\"进度概览图已保存: {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"绘制进度概览图出错: {e}\")\n",
        "\n",
        "logger.info(\"可视化工具模块加载完成\")\n",
        "print(\"可视化工具已准备就绪 - 优化后的Kaggle版本\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 17: Kaggle环境适配和完整主逻辑函数\n",
        "\n",
        "def setup_kaggle_environment():\n",
        "    \"\"\"设置Kaggle环境 - 检测和适配路径\"\"\"\n",
        "    # 检测是否在Kaggle环境中\n",
        "    if '/kaggle/' in os.getcwd():\n",
        "        logger.info(\"检测到Kaggle环境\")\n",
        "        \n",
        "        # Kaggle环境路径\n",
        "        base_data_path = '/kaggle/input'  # 输入数据路径\n",
        "        output_path = '/kaggle/working'   # 输出路径\n",
        "        \n",
        "        # 检查数据文件是否存在\n",
        "        data_files = {\n",
        "            'x_train': None,\n",
        "            'y_train': None, \n",
        "            'x_test': None,\n",
        "            'y_test': None\n",
        "        }\n",
        "        \n",
        "        # 在Kaggle输入目录中查找数据文件\n",
        "        if os.path.exists(base_data_path):\n",
        "            for root, dirs, files in os.walk(base_data_path):\n",
        "                for file in files:\n",
        "                    if 'rosenbrock' in file.lower():\n",
        "                        if 'x_train' in file.lower():\n",
        "                            data_files['x_train'] = os.path.join(root, file)\n",
        "                        elif 'y_train' in file.lower():\n",
        "                            data_files['y_train'] = os.path.join(root, file)\n",
        "                        elif 'x_test' in file.lower():\n",
        "                            data_files['x_test'] = os.path.join(root, file)\n",
        "                        elif 'y_test' in file.lower():\n",
        "                            data_files['y_test'] = os.path.join(root, file)\n",
        "        \n",
        "        return output_path, data_files\n",
        "    else:\n",
        "        logger.info(\"本地环境\")\n",
        "        # 本地环境\n",
        "        output_path = './dante_results'\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        \n",
        "        data_files = {\n",
        "            'x_train': 'rosenbrock/data_raw/Rosenbrock_x_train.npy',\n",
        "            'y_train': 'rosenbrock/data_raw/Rosenbrock_y_train.npy',\n",
        "            'x_test': 'rosenbrock/data_raw/Rosenbrock_x_test.npy',\n",
        "            'y_test': 'rosenbrock/data_raw/Rosenbrock_y_test.npy'\n",
        "        }\n",
        "        \n",
        "        return output_path, data_files\n",
        "\n",
        "def load_initial_data(data_files, rosenbrock_dimension=20, initial_data_size=800, domain_bounds=(-2.048, 2.048)):\n",
        "    \"\"\"加载初始数据，支持Kaggle和本地环境\"\"\"\n",
        "    logger.info(\"开始加载初始数据...\")\n",
        "    \n",
        "    X_test, y_test = None, None\n",
        "    \n",
        "    try:\n",
        "        # 尝试加载测试集\n",
        "        if data_files['x_test'] and data_files['y_test']:\n",
        "            if os.path.exists(data_files['x_test']) and os.path.exists(data_files['y_test']):\n",
        "                X_test = np.load(data_files['x_test'])\n",
        "                y_test = np.load(data_files['y_test'])\n",
        "                if y_test.ndim == 1:\n",
        "                    y_test = y_test.reshape(-1, 1)\n",
        "                logger.info(f\"成功加载测试数据: X shape {X_test.shape}, y shape {y_test.shape}\")\n",
        "        \n",
        "        # 尝试加载训练集\n",
        "        if data_files['x_train'] and data_files['y_train']:\n",
        "            if os.path.exists(data_files['x_train']) and os.path.exists(data_files['y_train']):\n",
        "                initial_X = np.load(data_files['x_train'])\n",
        "                initial_y = np.load(data_files['y_train'])\n",
        "                if initial_y.ndim == 1:\n",
        "                    initial_y = initial_y.reshape(-1, 1)\n",
        "                logger.info(f\"成功加载训练数据: X shape {initial_X.shape}, y shape {initial_y.shape}\")\n",
        "                \n",
        "                # 确保有足够的初始样本\n",
        "                if initial_X.shape[0] < initial_data_size:\n",
        "                    logger.warning(f\"数据不足，需要生成额外样本\")\n",
        "                    additional_samples = initial_data_size - initial_X.shape[0]\n",
        "                    additional_X = np.random.uniform(\n",
        "                        low=domain_bounds[0], high=domain_bounds[1], \n",
        "                        size=(additional_samples, rosenbrock_dimension)\n",
        "                    )\n",
        "                    additional_y = np.array([rosenbrock_evaluator(x) for x in additional_X]).reshape(-1, 1)\n",
        "                    initial_X = np.vstack((initial_X, additional_X))\n",
        "                    initial_y = np.vstack((initial_y, additional_y))\n",
        "                    logger.info(f\"添加{additional_samples}个随机样本，总样本数: {initial_X.shape[0]}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\"训练数据文件不存在\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"未指定训练数据文件路径\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"加载数据失败: {e}，生成随机数据\")\n",
        "        # 生成随机数据作为备用方案\n",
        "        initial_X = np.random.uniform(\n",
        "            low=domain_bounds[0], high=domain_bounds[1], \n",
        "            size=(initial_data_size, rosenbrock_dimension)\n",
        "        )\n",
        "        initial_y = np.array([rosenbrock_evaluator(x) for x in initial_X]).reshape(-1, 1)\n",
        "        logger.info(\"生成随机初始数据\")\n",
        "        \n",
        "        # 如果没有测试集，从随机数据中分离一部分作为测试集\n",
        "        if X_test is None:\n",
        "            test_size = min(200, initial_data_size // 4)\n",
        "            indices = np.random.choice(initial_data_size, test_size, replace=False)\n",
        "            X_test = initial_X[indices]\n",
        "            y_test = initial_y[indices]\n",
        "            \n",
        "            # 从训练集中删除测试数据\n",
        "            mask = np.ones(initial_data_size, dtype=bool)\n",
        "            mask[indices] = False\n",
        "            initial_X = initial_X[mask]\n",
        "            initial_y = initial_y[mask]\n",
        "            \n",
        "            # 补充训练样本到原始大小\n",
        "            if initial_X.shape[0] < initial_data_size:\n",
        "                additional_samples = initial_data_size - initial_X.shape[0]\n",
        "                additional_X = np.random.uniform(\n",
        "                    low=domain_bounds[0], high=domain_bounds[1], \n",
        "                    size=(additional_samples, rosenbrock_dimension)\n",
        "                )\n",
        "                additional_y = np.array([rosenbrock_evaluator(x) for x in additional_X]).reshape(-1, 1)\n",
        "                initial_X = np.vstack((initial_X, additional_X))\n",
        "                initial_y = np.vstack((initial_y, additional_y))\n",
        "            \n",
        "            logger.info(f\"从随机数据中创建测试集: {X_test.shape[0]}个样本\")\n",
        "    \n",
        "    return initial_X, initial_y, X_test, y_test\n",
        "\n",
        "logger.info(\"Kaggle环境适配模块加载完成\")\n",
        "print(\"环境适配完成，支持本地和Kaggle环境\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: 主执行逻辑 (main.py核心) - DANTE优化算法\n",
        "import time\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "import gc\n",
        "\n",
        "# NTE搜索器包装类\n",
        "class NTESurrogateWrapper:\n",
        "    \"\"\"替代模型包装类，适配NTE搜索器接口\"\"\"\n",
        "    def __init__(self, actual_surrogate_model, data_manager_instance):\n",
        "        self.surrogate_model = actual_surrogate_model\n",
        "        self.dm = data_manager_instance\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:\n",
        "        # 确保输入数据有效\n",
        "        if X_batch_orig.shape[0] == 0:\n",
        "            return np.array([])\n",
        "        if X_batch_orig.ndim == 1:\n",
        "             X_batch_orig = X_batch_orig.reshape(1, -1)\n",
        "\n",
        "        try:\n",
        "            # CNN1DSurrogate进行预测（直接返回log尺度的值）\n",
        "            pred_log = self.surrogate_model.predict(X_batch_orig)\n",
        "            \n",
        "            # 返回与输入匹配的输出格式\n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return pred_log[0] if isinstance(pred_log, np.ndarray) else pred_log\n",
        "            else:\n",
        "                return pred_log\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Prediction error in NTESurrogateWrapper: {e}\")\n",
        "            # 出错时返回保守的高值（log尺度）\n",
        "            if X_batch_orig.shape[0] == 1:\n",
        "                return np.log10(1e6 + 1.0)\n",
        "            else:\n",
        "                return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)\n",
        "\n",
        "def calculate_pearson_correlation(model, data_manager):\n",
        "    \"\"\"计算模型预测与真值的皮尔逊相关系数\"\"\"\n",
        "    if data_manager.num_samples < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        X_data = data_manager.get_all_x_orig()\n",
        "        y_data_orig = data_manager.get_all_y_orig().flatten()\n",
        "        \n",
        "        # CNN1DSurrogate返回log尺度预测值\n",
        "        predictions_log = model.predict(X_data)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_data_orig) < 2:\n",
        "            return 0.0\n",
        "            \n",
        "        correlation, _ = pearsonr(predictions_orig, y_data_orig)\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"计算皮尔逊相关系数时出错: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def calculate_pearson_on_test_set(model, X_test, y_test_orig):\n",
        "    \"\"\"计算测试集上的皮尔逊相关系数\"\"\"\n",
        "    if X_test.shape[0] < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    try:\n",
        "        predictions_log = model.predict(X_test)\n",
        "        predictions_orig = safe_power10(predictions_log)\n",
        "        \n",
        "        if len(predictions_orig) < 2 or len(y_test_orig) < 2:\n",
        "            return 0.0\n",
        "        correlation, _ = pearsonr(predictions_orig, y_test_orig.flatten())\n",
        "        return correlation if not np.isnan(correlation) else 0.0\n",
        "    except Exception as e:\n",
        "        logger.error(f\"计算测试集皮尔逊相关系数时出错: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def evaluate_cnn_performance(model, data_manager, results_dir, iteration, X_test=None, y_test_orig=None, should_generate_plots=True):\n",
        "    \"\"\"评估CNN性能\"\"\"\n",
        "    logger.info(f\"Evaluating CNN performance (iteration {iteration})...\")\n",
        "    metrics = {}\n",
        "    \n",
        "    # 训练集评估\n",
        "    X_train = data_manager.get_all_x_orig()\n",
        "    y_train_orig = data_manager.get_all_y_orig().flatten()\n",
        "    \n",
        "    train_predictions_log = model.predict(X_train)\n",
        "    train_predictions_orig = safe_power10(train_predictions_log)\n",
        "    \n",
        "    if should_generate_plots:\n",
        "        train_mse, train_rmse, train_mae = plot_prediction_vs_truth(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration, \n",
        "            filename=f\"train_prediction_vs_truth_iter_{iteration}.png\"\n",
        "        )\n",
        "        plot_residuals(\n",
        "            train_predictions_orig, y_train_orig, results_dir, \n",
        "            iteration=iteration,\n",
        "            filename=f\"train_residual_analysis_iter_{iteration}.png\"\n",
        "        )\n",
        "    else:\n",
        "        train_mse = np.mean((train_predictions_orig - y_train_orig) ** 2)\n",
        "        train_rmse = np.sqrt(train_mse)\n",
        "        train_mae = np.mean(np.abs(train_predictions_orig - y_train_orig))\n",
        "        \n",
        "    metrics.update({\n",
        "        'train_mse': train_mse, 'train_rmse': train_rmse, 'train_mae': train_mae,\n",
        "        'train_sample_count': len(y_train_orig)\n",
        "    })\n",
        "    \n",
        "    # 测试集评估\n",
        "    if X_test is not None and y_test_orig is not None and X_test.shape[0] > 0:\n",
        "        test_predictions_log = model.predict(X_test)\n",
        "        test_predictions_orig = safe_power10(test_predictions_log)\n",
        "        test_ground_truth_orig = y_test_orig.flatten()\n",
        "        \n",
        "        if should_generate_plots:\n",
        "            test_mse, test_rmse, test_mae = plot_prediction_vs_truth(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_prediction_vs_truth_iter_{iteration}.png\"\n",
        "            )\n",
        "            plot_residuals(\n",
        "                test_predictions_orig, test_ground_truth_orig, results_dir, \n",
        "                iteration=iteration,\n",
        "                filename=f\"test_residual_analysis_iter_{iteration}.png\"\n",
        "            )\n",
        "        else:\n",
        "            test_mse = np.mean((test_predictions_orig - test_ground_truth_orig) ** 2)\n",
        "            test_rmse = np.sqrt(test_mse)\n",
        "            test_mae = np.mean(np.abs(test_predictions_orig - test_ground_truth_orig))\n",
        "            \n",
        "        if len(test_predictions_orig) >= 2 and len(test_ground_truth_orig) >= 2:\n",
        "            test_pearson, _ = pearsonr(test_predictions_orig, test_ground_truth_orig)\n",
        "            test_pearson = 0.0 if np.isnan(test_pearson) else test_pearson\n",
        "        else:\n",
        "            test_pearson = 0.0\n",
        "        metrics.update({\n",
        "            'test_mse': test_mse, 'test_rmse': test_rmse, 'test_mae': test_mae,\n",
        "            'test_pearson': test_pearson, 'test_sample_count': len(test_ground_truth_orig)\n",
        "        })\n",
        "    \n",
        "    logger.info(f\"CNN Performance (Iter {iteration}): Train RMSE: {metrics.get('train_rmse'):.4e}\")\n",
        "    if 'test_rmse' in metrics:\n",
        "        logger.info(f\"  Test RMSE: {metrics.get('test_rmse'):.4e}, Test Pearson: {metrics.get('test_pearson'):.4f}\")\n",
        "    return metrics\n",
        "\n",
        "def run_dante_optimization(\n",
        "    # 基本参数\n",
        "    initial_data_size=800,\n",
        "    n_al_iterations=400,  # 原项目完整迭代数\n",
        "    rosenbrock_dimension=20,\n",
        "    domain_range=(-2.048, 2.048),\n",
        "    \n",
        "    # CNN参数\n",
        "    cnn_training_epochs=200,  # 原项目完整训练轮数\n",
        "    cnn_batch_size=32,       # 原项目批次大小\n",
        "    cnn_validation_split=0.2,\n",
        "    cnn_early_stopping_patience=30,  # 原项目早停patience\n",
        "    \n",
        "    # NTE参数\n",
        "    nte_c0=1.0,\n",
        "    nte_rollout_rounds=100,\n",
        "    validation_batch_size=20,\n",
        "    \n",
        "    # 动态探索因子参数 (重点调试)\n",
        "    base_exploration_factor=0.2,  # 原项目验证的最佳基础因子\n",
        "    exploration_exponent_base=2.0,\n",
        "    exploration_max_limit=20,\n",
        "    \n",
        "    # 可视化控制\n",
        "    viz_interval=10,\n",
        "    detailed_viz_interval=50,  # 原项目设置\n",
        "    \n",
        "    # 数据路径\n",
        "    data_input_path=\"/kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw\",\n",
        "    results_output_path=\"/kaggle/working/results\",\n",
        "    \n",
        "    # 实验标识\n",
        "    experiment_name=\"default\"\n",
        "):\n",
        "    \"\"\"\n",
        "    运行DANTE优化算法的主函数\n",
        "    \n",
        "    重点调试参数：\n",
        "    - base_exploration_factor: 动态探索因子基础值 (默认0.8)\n",
        "    - exploration_exponent_base: 指数基数 (默认2.0)\n",
        "    - exploration_max_limit: 上限值 (默认20)\n",
        "    \n",
        "    动态公式: factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(\"=\"*60)\n",
        "    logger.info(f\"开始DANTE优化实验: {experiment_name}\")\n",
        "    logger.info(f\"重点调试参数 - 基础因子: {base_exploration_factor}, 指数基数: {exploration_exponent_base}, 上限: {exploration_max_limit}\")\n",
        "    logger.info(\"=\"*60)\n",
        "    \n",
        "    # 创建结果目录\n",
        "    experiment_results_dir = create_results_directory(results_output_path)\n",
        "    if not experiment_results_dir:\n",
        "        logger.error(\"无法创建结果目录，退出实验\")\n",
        "        return None\n",
        "    \n",
        "    # 记录实验参数\n",
        "    experiment_params = {\n",
        "        'experiment_name': experiment_name,\n",
        "        'initial_data_size': initial_data_size,\n",
        "        'n_al_iterations': n_al_iterations,\n",
        "        'rosenbrock_dimension': rosenbrock_dimension,\n",
        "        'domain_range': domain_range,\n",
        "        'base_exploration_factor': base_exploration_factor,\n",
        "        'exploration_exponent_base': exploration_exponent_base,\n",
        "        'exploration_max_limit': exploration_max_limit,\n",
        "        'nte_c0': nte_c0,\n",
        "        'nte_rollout_rounds': nte_rollout_rounds,\n",
        "        'cnn_training_epochs': cnn_training_epochs,\n",
        "        'cnn_batch_size': cnn_batch_size,\n",
        "        'timestamp': datetime.datetime.now().isoformat()\n",
        "    }\n",
        "    \n",
        "    # 保存实验参数\n",
        "    params_file = os.path.join(experiment_results_dir, 'experiment_parameters.json')\n",
        "    with open(params_file, 'w') as f:\n",
        "        import json\n",
        "        json.dump(experiment_params, f, indent=2)\n",
        "    logger.info(f\"实验参数已保存: {params_file}\")\n",
        "    \n",
        "    # 定义域\n",
        "    domain_min, domain_max = domain_range\n",
        "    rosenbrock_domain = [(domain_min, domain_max)] * rosenbrock_dimension\n",
        "    \n",
        "    # 加载初始数据\n",
        "    try:\n",
        "        x_train_path = os.path.join(data_input_path, \"Rosenbrock_x_train.npy\")\n",
        "        y_train_path = os.path.join(data_input_path, \"Rosenbrock_y_train.npy\")\n",
        "        x_test_path = os.path.join(data_input_path, \"Rosenbrock_x_test.npy\")\n",
        "        y_test_path = os.path.join(data_input_path, \"Rosenbrock_y_test.npy\")\n",
        "        \n",
        "        # 加载测试集\n",
        "        X_test = np.load(x_test_path)\n",
        "        y_test = np.load(y_test_path)\n",
        "        if y_test.ndim == 1:\n",
        "            y_test = y_test.reshape(-1, 1)\n",
        "        logger.info(f\"测试集加载成功: X shape {X_test.shape}, y shape {y_test.shape}\")\n",
        "        \n",
        "        # 加载训练集\n",
        "        initial_X = np.load(x_train_path)\n",
        "        initial_y = np.load(y_train_path)\n",
        "        if initial_y.ndim == 1:\n",
        "            initial_y = initial_y.reshape(-1, 1)\n",
        "        logger.info(f\"训练集加载成功: X shape {initial_X.shape}, y shape {initial_y.shape}\")\n",
        "        \n",
        "        # 确保有足够的初始样本\n",
        "        if initial_X.shape[0] < initial_data_size:\n",
        "            additional_samples = initial_data_size - initial_X.shape[0]\n",
        "            additional_X = np.random.uniform(low=domain_min, high=domain_max, size=(additional_samples, rosenbrock_dimension))\n",
        "            additional_y = np.array([rosenbrock_function(x) for x in additional_X]).reshape(-1, 1)\n",
        "            initial_X = np.vstack((initial_X, additional_X))\n",
        "            initial_y = np.vstack((initial_y, additional_y))\n",
        "            logger.info(f\"补充了{additional_samples}个随机样本\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"加载数据失败: {e}\")\n",
        "        # 生成随机数据作为备用\n",
        "        initial_X = np.random.uniform(low=domain_min, high=domain_max, size=(initial_data_size, rosenbrock_dimension))\n",
        "        initial_y = np.array([rosenbrock_function(x) for x in initial_X]).reshape(-1, 1)\n",
        "        \n",
        "        # 生成测试集\n",
        "        test_size = 200\n",
        "        X_test = np.random.uniform(low=domain_min, high=domain_max, size=(test_size, rosenbrock_dimension))\n",
        "        y_test = np.array([rosenbrock_function(x) for x in X_test]).reshape(-1, 1)\n",
        "        logger.info(\"使用随机生成的数据\")\n",
        "    \n",
        "    # 初始化数据管理器\n",
        "    data_manager = DataManager(\n",
        "        initial_X=initial_X, \n",
        "        initial_y=initial_y, \n",
        "        dimension=rosenbrock_dimension\n",
        "    )\n",
        "    logger.info(\"DataManager初始化完成\")\n",
        "    \n",
        "    # 初始化CNN模型\n",
        "    cnn_model = CNN1DSurrogate(input_dim=rosenbrock_dimension)\n",
        "    logger.info(\"CNN1DSurrogate模型初始化完成\")\n",
        "    \n",
        "    # 初始化NTE搜索器\n",
        "    nte_searcher = NTESearcher(\n",
        "        dimension=rosenbrock_dimension, \n",
        "        domain=rosenbrock_domain,\n",
        "        c0=nte_c0,\n",
        "        rollout_rounds=nte_rollout_rounds,\n",
        "        validation_batch_size=validation_batch_size\n",
        "    )\n",
        "    logger.info(\"NTE搜索器初始化完成\")\n",
        "    \n",
        "    # 历史记录变量\n",
        "    history_iterations = []\n",
        "    history_best_y = []\n",
        "    history_pearson_all = []\n",
        "    history_dynamic_factors = []  # 记录动态探索因子\n",
        "    cnn_performance_history = []\n",
        "    \n",
        "    # 动态探索因子相关变量\n",
        "    no_improvement_streak = 0\n",
        "    previous_best_y = float('inf')\n",
        "    \n",
        "    # 开始主循环\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for al_iteration in range(n_al_iterations):\n",
        "        iteration_start_time = time.time()\n",
        "        current_iteration = al_iteration + 1\n",
        "        history_iterations.append(current_iteration)\n",
        "        \n",
        "        # 计算动态探索因子 (重点调试公式)\n",
        "        dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "        history_dynamic_factors.append(dynamic_exploration_factor)\n",
        "        \n",
        "        logger.info(f\"\\n--- 迭代 {current_iteration}/{n_al_iterations} ---\")\n",
        "        logger.info(f\"动态探索因子: {dynamic_exploration_factor:.2f} (连续无改进次数: {no_improvement_streak})\")\n",
        "        \n",
        "        # 决定是否生成可视化\n",
        "        should_generate_plots = ((current_iteration % viz_interval == 0) or \n",
        "                               (current_iteration == n_al_iterations))\n",
        "        should_generate_detailed_plots = ((current_iteration % detailed_viz_interval == 0) or \n",
        "                                        (current_iteration == n_al_iterations))\n",
        "        \n",
        "        # 训练CNN模型\n",
        "        logger.info(\"开始训练CNN模型...\")\n",
        "        training_history = cnn_model.train(\n",
        "            data_manager=data_manager,\n",
        "            epochs=cnn_training_epochs,\n",
        "            batch_size=cnn_batch_size,\n",
        "            validation_split=cnn_validation_split,\n",
        "            patience=cnn_early_stopping_patience,\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # 可视化训练历史\n",
        "        if should_generate_plots:\n",
        "            cnn_model.visualize_training_history(\n",
        "                training_history,\n",
        "                save_path=os.path.join(experiment_results_dir, f\"cnn_training_history_iter_{current_iteration}.png\")\n",
        "            )\n",
        "        \n",
        "        # 评估模型性能\n",
        "        perf_metrics = evaluate_cnn_performance(\n",
        "            cnn_model, data_manager, experiment_results_dir, current_iteration, \n",
        "            X_test, y_test, should_generate_plots=should_generate_detailed_plots\n",
        "        )\n",
        "        cnn_performance_history.append(perf_metrics)\n",
        "        \n",
        "        # 计算皮尔逊相关系数\n",
        "        current_pearson = calculate_pearson_on_test_set(cnn_model, X_test, y_test) if X_test is not None else calculate_pearson_correlation(cnn_model, data_manager)\n",
        "        history_pearson_all.append(current_pearson)\n",
        "        logger.info(f\"模型皮尔逊相关系数: {current_pearson:.4f}\")\n",
        "        \n",
        "        # NTE搜索新候选点\n",
        "        logger.info(\"开始NTE搜索...\")\n",
        "        nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)\n",
        "        \n",
        "        current_best_x, current_best_y = data_manager.get_current_best()\n",
        "        \n",
        "        try:\n",
        "            nte_search_result = nte_searcher.search(\n",
        "                surrogate_model=nte_model_wrapper,\n",
        "                current_best_state=current_best_x,\n",
        "                min_y_observed=current_best_y,\n",
        "                dynamic_exploration_factor=dynamic_exploration_factor,  # 传递动态探索因子\n",
        "                all_states=data_manager.X_data,\n",
        "                all_values=data_manager.get_all_y_orig(),\n",
        "                current_iteration=current_iteration,\n",
        "                return_ducb_trends=True\n",
        "            )\n",
        "            \n",
        "            nte_candidates, ducb_trends_data = nte_search_result\n",
        "            \n",
        "            if len(nte_candidates) > 0:\n",
        "                logger.info(f\"NTE搜索找到{len(nte_candidates)}个候选点\")\n",
        "                \n",
        "                # 评估候选点\n",
        "                nte_y_true = np.array([rosenbrock_function(x) for x in nte_candidates]).reshape(-1, 1)\n",
        "                \n",
        "                # 添加到数据管理器\n",
        "                data_manager.add_samples(nte_candidates, nte_y_true, re_normalize=True, shuffle=True)\n",
        "                logger.info(f\"已添加{len(nte_candidates)}个新样本，数据集总大小: {data_manager.num_samples}\")\n",
        "                \n",
        "                # 更新最佳值\n",
        "                current_best_x, current_best_y = data_manager.get_current_best()\n",
        "                history_best_y.append(current_best_y)\n",
        "                \n",
        "                # 检查是否有改进\n",
        "                if current_best_y < previous_best_y:\n",
        "                    logger.info(f\"发现更优解: {current_best_y:.6e} (改进: {previous_best_y - current_best_y:.6e})\")\n",
        "                    no_improvement_streak = 0\n",
        "                    \n",
        "                    # 发现新最优解时绘制维度分布图\n",
        "                    if should_generate_plots:\n",
        "                        plot_top_samples_dimensions(\n",
        "                            data_manager=data_manager,\n",
        "                            output_dir=experiment_results_dir,\n",
        "                            iteration=current_iteration,\n",
        "                            n_top_samples=15\n",
        "                        )\n",
        "                else:\n",
        "                    no_improvement_streak += 1\n",
        "                    logger.info(f\"本轮无改进，连续无改进次数: {no_improvement_streak}\")\n",
        "                \n",
        "                previous_best_y = current_best_y\n",
        "                \n",
        "                # 绘制DUCB趋势图\n",
        "                if ducb_trends_data and should_generate_plots:\n",
        "                    plot_nte_ducb_trends(\n",
        "                        ducb_trends_data=ducb_trends_data,\n",
        "                        output_dir=experiment_results_dir,\n",
        "                        iteration=current_iteration,\n",
        "                        dynamic_exploration_factor=dynamic_exploration_factor\n",
        "                    )\n",
        "            else:\n",
        "                logger.info(\"NTE搜索未找到有效候选点\")\n",
        "                no_improvement_streak += 1\n",
        "                if history_best_y:\n",
        "                    history_best_y.append(history_best_y[-1])\n",
        "                else:\n",
        "                    history_best_y.append(current_best_y)\n",
        "                    \n",
        "        except Exception as e:\n",
        "            logger.error(f\"NTE搜索失败: {e}\")\n",
        "            no_improvement_streak += 1\n",
        "            if history_best_y:\n",
        "                history_best_y.append(history_best_y[-1])\n",
        "            else:\n",
        "                history_best_y.append(current_best_y)\n",
        "        \n",
        "        # 生成可视化图表\n",
        "        if should_generate_plots:\n",
        "            # 全局最优值趋势\n",
        "            plot_global_min_value_trend(\n",
        "                history_iterations[:len(history_best_y)], \n",
        "                history_best_y, \n",
        "                experiment_results_dir, \n",
        "                filename=f\"global_min_iter_{current_iteration}.png\"\n",
        "            )\n",
        "            \n",
        "            # 皮尔逊相关系数趋势\n",
        "            plot_pearson_correlation_trend(\n",
        "                history_iterations, \n",
        "                history_pearson_all, \n",
        "                experiment_results_dir,\n",
        "                filename=f\"pearson_trend_iter_{current_iteration}.png\"\n",
        "            )\n",
        "            \n",
        "            # 快速进度概览\n",
        "            quick_progress_plot(\n",
        "                history_iterations[:len(history_best_y)], \n",
        "                history_best_y, \n",
        "                history_pearson_all, \n",
        "                experiment_results_dir, \n",
        "                current_iteration\n",
        "            )\n",
        "        \n",
        "        # 内存管理\n",
        "        if current_iteration % 10 == 0:\n",
        "            data_manager.clear_cache()\n",
        "            gc.collect()\n",
        "            logger.info(\"已清理内存缓存\")\n",
        "        \n",
        "        iteration_time = time.time() - iteration_start_time\n",
        "        logger.info(f\"迭代{current_iteration}完成，用时: {iteration_time:.1f}秒\")\n",
        "    \n",
        "    # 保存最终结果\n",
        "    total_time = time.time() - start_time\n",
        "    logger.info(f\"\\n实验完成！总用时: {total_time:.1f}秒\")\n",
        "    \n",
        "    # 保存性能历史\n",
        "    perf_csv_path = os.path.join(experiment_results_dir, \"performance_history.csv\")\n",
        "    pd.DataFrame(cnn_performance_history).to_csv(perf_csv_path, index=False)\n",
        "    \n",
        "    # 保存动态因子历史\n",
        "    factor_history = pd.DataFrame({\n",
        "        'iteration': history_iterations,\n",
        "        'dynamic_exploration_factor': history_dynamic_factors,\n",
        "        'no_improvement_streak': [factor_data for factor_data in range(len(history_dynamic_factors))],  # 简化记录\n",
        "        'best_value': history_best_y[:len(history_iterations)],\n",
        "        'pearson_correlation': history_pearson_all\n",
        "    })\n",
        "    factor_csv_path = os.path.join(experiment_results_dir, \"dynamic_factor_history.csv\")\n",
        "    factor_history.to_csv(factor_csv_path, index=False)\n",
        "    \n",
        "    # 保存最终数据状态\n",
        "    final_data_path = os.path.join(experiment_results_dir, \"final_data_state.npz\")\n",
        "    data_manager.save_data(final_data_path)\n",
        "    \n",
        "    # 实验总结\n",
        "    final_best_x, final_best_y = data_manager.get_current_best()\n",
        "    experiment_summary = {\n",
        "        'experiment_name': experiment_name,\n",
        "        'total_iterations': n_al_iterations,\n",
        "        'final_best_value': float(final_best_y),\n",
        "        'final_pearson_correlation': float(history_pearson_all[-1]),\n",
        "        'total_time_seconds': total_time,\n",
        "        'average_time_per_iteration': total_time / n_al_iterations,\n",
        "        'total_samples_evaluated': data_manager.num_samples,\n",
        "        'max_dynamic_factor': float(max(history_dynamic_factors)),\n",
        "        'final_dynamic_factor': float(history_dynamic_factors[-1])\n",
        "    }\n",
        "    \n",
        "    summary_file = os.path.join(experiment_results_dir, 'experiment_summary.json')\n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(experiment_summary, f, indent=2)\n",
        "    \n",
        "    logger.info(f\"实验总结:\")\n",
        "    logger.info(f\"  最终最优值: {final_best_y:.6e}\")\n",
        "    logger.info(f\"  最终相关系数: {history_pearson_all[-1]:.4f}\")\n",
        "    logger.info(f\"  总样本数: {data_manager.num_samples}\")\n",
        "    logger.info(f\"  最大动态因子: {max(history_dynamic_factors):.2f}\")\n",
        "    logger.info(f\"  结果保存路径: {experiment_results_dir}\")\n",
        "    \n",
        "    return {\n",
        "        'results_dir': experiment_results_dir,\n",
        "        'final_best_value': final_best_y,\n",
        "        'final_pearson': history_pearson_all[-1],\n",
        "        'experiment_summary': experiment_summary,\n",
        "        'performance_history': cnn_performance_history,\n",
        "        'factor_history': factor_history\n",
        "    }\n",
        "\n",
        "logger.info(\"主执行逻辑模块加载完成\")\n",
        "print(\"DANTE主执行逻辑已准备就绪 - 重点调试动态探索因子\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 19: 实际运行代码和测试\n",
        "def run_quick_demo():\n",
        "    \"\"\"运行一个快速的DANTE演示，适合Kaggle环境测试\"\"\"\n",
        "    logger.info(\"开始运行DANTE快速演示...\")\n",
        "    \n",
        "    try:\n",
        "        # 快速演示参数\n",
        "        demo_result = run_dante_experiment(\n",
        "            experiment_name=\"kaggle_demo\",\n",
        "            n_al_iterations=15,  # 快速演示仅15次迭代\n",
        "            cnn_training_epochs=30,  # 减少训练轮数\n",
        "            initial_data_size=200,   # 减少初始数据\n",
        "            nte_rollout_rounds=20,   # 减少NTE轮数\n",
        "            viz_interval=5,          # 每5次迭代可视化\n",
        "            detailed_viz_interval=10,\n",
        "            # 使用默认的动态探索因子参数\n",
        "            base_exploration_factor=0.8,\n",
        "            exploration_exponent_base=2,\n",
        "            exploration_max_limit=20\n",
        "        )\n",
        "        \n",
        "        if demo_result:\n",
        "            logger.info(\"快速演示完成!\")\n",
        "            logger.info(f\"最终最优值: {demo_result['final_best_value']:.6e}\")\n",
        "            logger.info(f\"结果目录: {demo_result['results_dir']}\")\n",
        "            return demo_result\n",
        "        else:\n",
        "            logger.error(\"快速演示失败\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"快速演示出现异常: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def run_parameter_test():\n",
        "    \"\"\"运行参数测试 - 简化版参数扫描\"\"\"\n",
        "    logger.info(\"开始运行参数测试...\")\n",
        "    \n",
        "    # 简化的参数组合，适合Kaggle环境\n",
        "    test_configs = [\n",
        "        {'base': 0.8, 'exp': 2.0, 'max': 15, 'name': 'conservative'},\n",
        "        {'base': 1.0, 'exp': 2.5, 'max': 20, 'name': 'aggressive'}\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for i, config in enumerate(test_configs):\n",
        "        logger.info(f\"测试配置 {i+1}/{len(test_configs)}: {config['name']}\")\n",
        "        \n",
        "        try:\n",
        "            result = run_dante_experiment(\n",
        "                experiment_name=f\"param_test_{config['name']}\",\n",
        "                n_al_iterations=10,  # 快速测试\n",
        "                cnn_training_epochs=20,\n",
        "                initial_data_size=150,\n",
        "                nte_rollout_rounds=15,\n",
        "                viz_interval=5,\n",
        "                base_exploration_factor=config['base'],\n",
        "                exploration_exponent_base=config['exp'],\n",
        "                exploration_max_limit=config['max']\n",
        "            )\n",
        "            \n",
        "            if result:\n",
        "                results.append({\n",
        "                    'config_name': config['name'],\n",
        "                    'parameters': config,\n",
        "                    'final_best_value': result['final_best_value'],\n",
        "                    'final_pearson': result['final_pearson']\n",
        "                })\n",
        "                logger.info(f\"配置 {config['name']} 完成: {result['final_best_value']:.6e}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"配置 {config['name']} 失败: {e}\")\n",
        "    \n",
        "    if results:\n",
        "        # 比较结果\n",
        "        logger.info(\"\\n参数测试结果比较:\")\n",
        "        for r in results:\n",
        "            logger.info(f\"{r['config_name']}: 最优值={r['final_best_value']:.6e}, Pearson={r['final_pearson']:.4f}\")\n",
        "        \n",
        "        # 找到最佳配置\n",
        "        best = min(results, key=lambda x: x['final_best_value'])\n",
        "        logger.info(f\"\\n最佳配置: {best['config_name']}\")\n",
        "        logger.info(f\"参数: {best['parameters']}\")\n",
        "        \n",
        "        return results\n",
        "    else:\n",
        "        logger.error(\"所有参数测试都失败了\")\n",
        "        return None\n",
        "\n",
        "# 准备就绪提示\n",
        "logger.info(\"=\"*80)\n",
        "logger.info(\"DANTE Kaggle Notebook 已完全加载!\")\n",
        "logger.info(\"=\"*80)\n",
        "logger.info(\"可用的运行函数:\")\n",
        "logger.info(\"1. run_quick_demo() - 运行快速演示 (推荐先运行)\")\n",
        "logger.info(\"2. run_parameter_test() - 运行参数对比测试\")\n",
        "logger.info(\"3. run_dante_experiment(name, **params) - 运行自定义实验\")\n",
        "logger.info(\"=\"*80)\n",
        "\n",
        "print(\"✅ DANTE Notebook 完全加载完成!\")\n",
        "print(\"🚀 现在可以运行: run_quick_demo() 开始体验\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DANTE Rosenbrock优化 - Kaggle完整版本\n",
        "\n",
        "## 🎯 项目概述\n",
        "本notebook包含完整的DANTE (Deep Active Learning with Neural Tree Exploration) 算法实现，专门用于Rosenbrock函数优化。\n",
        "\n",
        "## 📋 核心功能模块\n",
        "- ✅ **完整的数据管理器** - 支持动态样本添加和权重管理\n",
        "- ✅ **CNN1D替代模型** - 1维卷积神经网络架构，用于函数近似\n",
        "- ✅ **NTE搜索器** - 神经树探索算法，包含1100+行的完整实现\n",
        "- ✅ **可视化工具** - 超过20种可视化图表，支持训练过程监控\n",
        "- ✅ **性能优化** - 内存管理和TensorFlow性能优化\n",
        "- ✅ **动态探索因子调试** - 核心算法参数的调试框架\n",
        "\n",
        "## 🔧 重点调试的动态探索因子公式\n",
        "```python\n",
        "dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "```\n",
        "\n",
        "## 🚀 快速开始\n",
        "1. **运行演示**: `run_quick_demo()` - 15次迭代的快速演示\n",
        "2. **参数测试**: `run_parameter_test()` - 对比不同参数配置\n",
        "3. **完整实验**: `run_dante_experiment()` - 自定义完整实验\n",
        "\n",
        "## 📊 适配说明\n",
        "- 已针对Kaggle环境优化参数 (迭代次数、训练轮数等)\n",
        "- 支持本地和Kaggle环境自动检测\n",
        "- 包含完整的错误处理和内存管理\n",
        "- 所有可视化使用英文字体，避免字体错误\n",
        "\n",
        "## ⚡ 运行建议\n",
        "1. 首次运行推荐使用 `run_quick_demo()` 测试环境\n",
        "2. Kaggle GPU环境下约10-15分钟完成快速演示\n",
        "3. 完整实验建议在本地环境或高配置Kaggle环境运行\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 21: 最终完整性检查和运行测试\n",
        "def check_notebook_completeness():\n",
        "    \"\"\"检查notebook的完整性，确保所有组件都正确加载\"\"\"\n",
        "    logger.info(\"开始检查notebook完整性...\")\n",
        "    \n",
        "    checks = {\n",
        "        'DataManager': DataManager,\n",
        "        'CNN1DSurrogate': CNN1DSurrogate, \n",
        "        'NTESearcher': NTESearcher,\n",
        "        'PerformanceOptimizer': PerformanceOptimizer,\n",
        "        'ProcessOptimizer': ProcessOptimizer,\n",
        "        'NTESurrogateWrapper': NTESurrogateWrapper,\n",
        "        'safe_power10': safe_power10,\n",
        "        'rosenbrock_function': rosenbrock_function,\n",
        "        'plot_global_min_value_trend': plot_global_min_value_trend,\n",
        "        'plot_prediction_vs_truth': plot_prediction_vs_truth,\n",
        "        'plot_nte_ducb_trends': plot_nte_ducb_trends,\n",
        "        'run_dante_experiment': run_dante_experiment,\n",
        "        'setup_kaggle_environment': setup_kaggle_environment,\n",
        "        'load_initial_data': load_initial_data\n",
        "    }\n",
        "    \n",
        "    missing = []\n",
        "    working = []\n",
        "    \n",
        "    for name, obj in checks.items():\n",
        "        try:\n",
        "            if callable(obj):\n",
        "                working.append(name)\n",
        "            else:\n",
        "                working.append(name)\n",
        "        except NameError:\n",
        "            missing.append(name)\n",
        "    \n",
        "    # 检查核心依赖\n",
        "    try:\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        import tensorflow as tf\n",
        "        import torch\n",
        "        import psutil\n",
        "        import scipy\n",
        "        deps_ok = True\n",
        "    except ImportError as e:\n",
        "        deps_ok = False\n",
        "        logger.error(f\"依赖检查失败: {e}\")\n",
        "    \n",
        "    # 检查GPU可用性\n",
        "    gpu_available = len(tf.config.experimental.list_physical_devices('GPU')) > 0\n",
        "    torch_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    # 报告结果\n",
        "    logger.info(f\"✅ 组件检查: {len(working)}/{len(checks)} 个组件正常\")\n",
        "    logger.info(f\"✅ 依赖检查: {'通过' if deps_ok else '失败'}\")\n",
        "    logger.info(f\"🔧 TensorFlow GPU: {'可用' if gpu_available else '不可用'}\")\n",
        "    logger.info(f\"🔧 PyTorch GPU: {'可用' if torch_gpu else '不可用'}\")\n",
        "    \n",
        "    if missing:\n",
        "        logger.warning(f\"❌ 缺失组件: {missing}\")\n",
        "        return False\n",
        "    \n",
        "    if not deps_ok:\n",
        "        logger.error(\"❌ 依赖检查失败\")\n",
        "        return False\n",
        "    \n",
        "    logger.info(\"🎉 Notebook完整性检查通过!\")\n",
        "    return True\n",
        "\n",
        "def quick_system_test():\n",
        "    \"\"\"快速系统测试，验证核心功能\"\"\"\n",
        "    logger.info(\"开始快速系统测试...\")\n",
        "    \n",
        "    try:\n",
        "        # 测试数据管理器\n",
        "        test_X = np.random.randn(50, 20)\n",
        "        test_y = np.random.randn(50, 1)\n",
        "        dm = DataManager(test_X, test_y, 20)\n",
        "        logger.info(\"✅ 数据管理器测试通过\")\n",
        "        \n",
        "        # 测试CNN模型\n",
        "        cnn = CNN1DSurrogate(input_dim=20)\n",
        "        logger.info(\"✅ CNN模型初始化通过\")\n",
        "        \n",
        "        # 测试NTE搜索器\n",
        "        domain = [(-2, 2)] * 20\n",
        "        nte = NTESearcher(dimension=20, domain=domain, c0=1, rollout_rounds=5, validation_batch_size=5)\n",
        "        logger.info(\"✅ NTE搜索器初始化通过\")\n",
        "        \n",
        "        # 测试环境设置\n",
        "        output_path, data_files = setup_kaggle_environment()\n",
        "        logger.info(f\"✅ 环境设置通过: {output_path}\")\n",
        "        \n",
        "        # 测试Rosenbrock函数\n",
        "        test_vec = np.array([1.0] * 20)\n",
        "        result = rosenbrock_function(test_vec)\n",
        "        logger.info(f\"✅ Rosenbrock函数测试通过: f(1,...,1) = {result}\")\n",
        "        \n",
        "        logger.info(\"🎉 所有系统测试通过!\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"❌ 系统测试失败: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# 运行检查\n",
        "completeness_ok = check_notebook_completeness()\n",
        "system_ok = quick_system_test()\n",
        "\n",
        "if completeness_ok and system_ok:\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"🚀 DANTE Notebook已完全准备就绪!\")\n",
        "    logger.info(\"💡 建议运行: run_quick_demo() 开始体验\")\n",
        "    logger.info(\"=\"*80)\n",
        "    print(\"✅ Notebook验证完成 - 可以安全运行!\")\n",
        "else:\n",
        "    logger.error(\"❌ Notebook验证失败，请检查错误信息\")\n",
        "    print(\"❌ 验证失败，请检查日志\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ✅ 超参数已恢复到原项目设置\n",
        "\n",
        "**重要更新：所有超参数已完全恢复到与原项目一致的设置**\n",
        "\n",
        "## 🎯 核心超参数设置 (与原项目完全一致)\n",
        "\n",
        "### 主要实验参数\n",
        "- **n_al_iterations**: 400 (原项目完整迭代数)\n",
        "- **initial_data_size**: 800 (初始数据集大小)\n",
        "- **rosenbrock_dimension**: 20 (问题维度)\n",
        "- **domain_range**: (-2.048, 2.048) (搜索域)\n",
        "\n",
        "### CNN模型参数\n",
        "- **cnn_training_epochs**: 200 (原项目完整训练轮数)\n",
        "- **cnn_batch_size**: 32 (原项目批次大小)\n",
        "- **cnn_validation_split**: 0.2 (验证集比例)\n",
        "- **cnn_early_stopping_patience**: 30 (原项目早停patience)\n",
        "\n",
        "### NTE搜索参数\n",
        "- **nte_c0**: 1.0 (探索参数)\n",
        "- **nte_rollout_rounds**: 100 (原项目完整rollout数)\n",
        "- **validation_batch_size**: 20 (每次迭代选择样本数)\n",
        "\n",
        "### 动态探索因子公式 (重点调试)\n",
        "```python\n",
        "dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "```\n",
        "**注意：原项目使用0.2作为基础因子，这是经过验证的最佳设置**\n",
        "- **base_exploration_factor**: 0.2 (原项目基础因子)\n",
        "- **exploration_exponent_base**: 2.0 (指数基数)\n",
        "- **exploration_max_limit**: 20 (上限值)\n",
        "\n",
        "### 可视化控制\n",
        "- **viz_interval**: 10 (每10次迭代可视化)\n",
        "- **detailed_viz_interval**: 50 (每50次迭代详细图表)\n",
        "\n",
        "## 🚀 P100显卡专门优化\n",
        "\n",
        "### TensorFlow P100优化设置\n",
        "- 虚拟GPU内存限制: 15GB (P100约16GB，留1GB余量)\n",
        "- 线程配置: inter_op=4, intra_op=2 (P100最优)\n",
        "- 混合精度: mixed_float16 (P100支持)\n",
        "- XLA编译优化: 已启用\n",
        "- GPU线程模式: gpu_private\n",
        "- 批归一化优化: 已启用\n",
        "\n",
        "### 性能预期\n",
        "- **完整400次迭代**: 预计8-12小时 (P100)\n",
        "- **快速20次测试**: 预计30-60分钟\n",
        "- **内存使用**: 约12-14GB GPU内存\n",
        "\n",
        "## ⚠️ 重要提醒\n",
        "1. 所有参数现在与原项目完全一致\n",
        "2. 专门针对Kaggle P100显卡进行了性能优化\n",
        "3. 保持了原项目的动态探索因子调试公式\n",
        "4. 建议先运行快速测试验证环境\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: 参数调试实验框架 - 动态探索因子优化\n",
        "import itertools\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def run_parameter_sweep_experiment():\n",
        "    \"\"\"\n",
        "    运行参数扫描实验，调试动态探索因子的最优组合\n",
        "    \n",
        "    重点调试的动态公式:\n",
        "    dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\n",
        "    \"\"\"\n",
        "    \n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"开始动态探索因子参数扫描实验\")\n",
        "    logger.info(\"=\"*80)\n",
        "    \n",
        "    # 参数网格定义\n",
        "    parameter_grid = {\n",
        "        'base_exploration_factor': [0.5, 0.8, 1.0, 1.2],  # 基础因子\n",
        "        'exploration_exponent_base': [1.5, 2.0, 2.5],     # 指数基数\n",
        "        'exploration_max_limit': [10, 15, 20, 25]         # 上限值\n",
        "    }\n",
        "    \n",
        "    # 快速测试参数\n",
        "    quick_test_params = {\n",
        "        'n_al_iterations': 30,  # 快速测试用较少迭代\n",
        "        'cnn_training_epochs': 50,\n",
        "        'initial_data_size': 400,  # 减少初始数据量\n",
        "        'viz_interval': 15,  # 较少可视化频率\n",
        "        'detailed_viz_interval': 30\n",
        "    }\n",
        "    \n",
        "    # 生成所有参数组合\n",
        "    param_combinations = list(itertools.product(\n",
        "        parameter_grid['base_exploration_factor'],\n",
        "        parameter_grid['exploration_exponent_base'], \n",
        "        parameter_grid['exploration_max_limit']\n",
        "    ))\n",
        "    \n",
        "    logger.info(f\"总共将测试 {len(param_combinations)} 种参数组合\")\n",
        "    \n",
        "    # 存储实验结果\n",
        "    sweep_results = []\n",
        "    best_result = None\n",
        "    best_score = float('inf')\n",
        "    \n",
        "    # 创建扫描实验总目录\n",
        "    sweep_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    sweep_base_dir = f\"/kaggle/working/parameter_sweep_{sweep_timestamp}\"\n",
        "    os.makedirs(sweep_base_dir, exist_ok=True)\n",
        "    \n",
        "    for i, (base_factor, exp_base, max_limit) in enumerate(param_combinations):\n",
        "        experiment_name = f\"sweep_{i+1:02d}_base{base_factor}_exp{exp_base}_max{max_limit}\"\n",
        "        \n",
        "        logger.info(f\"\\n开始实验 {i+1}/{len(param_combinations)}: {experiment_name}\")\n",
        "        logger.info(f\"参数: base_factor={base_factor}, exp_base={exp_base}, max_limit={max_limit}\")\n",
        "        \n",
        "        try:\n",
        "            # 运行单次实验\n",
        "            result = run_dante_optimization(\n",
        "                # 动态探索因子参数 (重点调试)\n",
        "                base_exploration_factor=base_factor,\n",
        "                exploration_exponent_base=exp_base,\n",
        "                exploration_max_limit=max_limit,\n",
        "                \n",
        "                # 快速测试参数\n",
        "                **quick_test_params,\n",
        "                \n",
        "                # 实验标识\n",
        "                experiment_name=experiment_name,\n",
        "                results_output_path=sweep_base_dir\n",
        "            )\n",
        "            \n",
        "            if result is not None:\n",
        "                # 记录关键指标\n",
        "                experiment_record = {\n",
        "                    'experiment_id': i + 1,\n",
        "                    'experiment_name': experiment_name,\n",
        "                    'base_exploration_factor': base_factor,\n",
        "                    'exploration_exponent_base': exp_base,\n",
        "                    'exploration_max_limit': max_limit,\n",
        "                    'final_best_value': float(result['final_best_value']),\n",
        "                    'final_pearson': float(result['final_pearson']),\n",
        "                    'max_dynamic_factor': float(result['experiment_summary']['max_dynamic_factor']),\n",
        "                    'total_time': float(result['experiment_summary']['total_time_seconds']),\n",
        "                    'total_samples': int(result['experiment_summary']['total_samples_evaluated']),\n",
        "                    'results_dir': result['results_dir']\n",
        "                }\n",
        "                \n",
        "                sweep_results.append(experiment_record)\n",
        "                \n",
        "                # 更新最佳结果 (以最终最优值为主要指标)\n",
        "                if result['final_best_value'] < best_score:\n",
        "                    best_score = result['final_best_value']\n",
        "                    best_result = experiment_record.copy()\n",
        "                \n",
        "                logger.info(f\"实验完成: 最优值={result['final_best_value']:.4e}, 相关系数={result['final_pearson']:.4f}\")\n",
        "                \n",
        "            else:\n",
        "                logger.error(f\"实验 {experiment_name} 失败\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"实验 {experiment_name} 出现异常: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # 保存扫描结果\n",
        "    sweep_results_file = os.path.join(sweep_base_dir, 'parameter_sweep_results.json')\n",
        "    with open(sweep_results_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'sweep_timestamp': sweep_timestamp,\n",
        "            'total_experiments': len(param_combinations),\n",
        "            'successful_experiments': len(sweep_results),\n",
        "            'parameter_grid': parameter_grid,\n",
        "            'quick_test_params': quick_test_params,\n",
        "            'best_result': best_result,\n",
        "            'all_results': sweep_results\n",
        "        }, f, indent=2)\n",
        "    \n",
        "    # 创建结果分析\n",
        "    if sweep_results:\n",
        "        results_df = pd.DataFrame(sweep_results)\n",
        "        \n",
        "        # 保存CSV格式结果\n",
        "        results_csv = os.path.join(sweep_base_dir, 'parameter_sweep_results.csv')\n",
        "        results_df.to_csv(results_csv, index=False)\n",
        "        \n",
        "        # 生成分析报告\n",
        "        analysis_report = generate_parameter_analysis_report(results_df, sweep_base_dir)\n",
        "        \n",
        "        logger.info(f\"\\n\" + \"=\"*80)\n",
        "        logger.info(\"参数扫描实验完成！\")\n",
        "        logger.info(f\"成功完成 {len(sweep_results)}/{len(param_combinations)} 个实验\")\n",
        "        \n",
        "        if best_result:\n",
        "            logger.info(f\"\\n最佳参数组合:\")\n",
        "            logger.info(f\"  基础因子: {best_result['base_exploration_factor']}\")\n",
        "            logger.info(f\"  指数基数: {best_result['exploration_exponent_base']}\")\n",
        "            logger.info(f\"  上限值: {best_result['exploration_max_limit']}\")\n",
        "            logger.info(f\"  最优值: {best_result['final_best_value']:.6e}\")\n",
        "            logger.info(f\"  相关系数: {best_result['final_pearson']:.4f}\")\n",
        "        \n",
        "        logger.info(f\"\\n结果保存位置: {sweep_base_dir}\")\n",
        "        logger.info(f\"详细结果: {sweep_results_file}\")\n",
        "        logger.info(f\"CSV格式: {results_csv}\")\n",
        "        logger.info(\"=\"*80)\n",
        "        \n",
        "        return {\n",
        "            'sweep_dir': sweep_base_dir,\n",
        "            'best_result': best_result,\n",
        "            'all_results': sweep_results,\n",
        "            'results_df': results_df,\n",
        "            'analysis_report': analysis_report\n",
        "        }\n",
        "    else:\n",
        "        logger.error(\"参数扫描实验失败：没有成功的实验\")\n",
        "        return None\n",
        "\n",
        "def generate_parameter_analysis_report(results_df, output_dir):\n",
        "    \"\"\"生成参数分析报告和可视化\"\"\"\n",
        "    \n",
        "    logger.info(\"生成参数分析报告...\")\n",
        "    \n",
        "    try:\n",
        "        # 基本统计分析\n",
        "        report = {\n",
        "            'summary_stats': {\n",
        "                'best_value_mean': float(results_df['final_best_value'].mean()),\n",
        "                'best_value_std': float(results_df['final_best_value'].std()),\n",
        "                'best_value_min': float(results_df['final_best_value'].min()),\n",
        "                'best_value_max': float(results_df['final_best_value'].max()),\n",
        "                'pearson_mean': float(results_df['final_pearson'].mean()),\n",
        "                'pearson_std': float(results_df['final_pearson'].std())\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # 按参数分组分析\n",
        "        base_factor_analysis = results_df.groupby('base_exploration_factor').agg({\n",
        "            'final_best_value': ['mean', 'std', 'min'],\n",
        "            'final_pearson': ['mean', 'std']\n",
        "        }).round(6)\n",
        "        \n",
        "        exp_base_analysis = results_df.groupby('exploration_exponent_base').agg({\n",
        "            'final_best_value': ['mean', 'std', 'min'],\n",
        "            'final_pearson': ['mean', 'std']\n",
        "        }).round(6)\n",
        "        \n",
        "        max_limit_analysis = results_df.groupby('exploration_max_limit').agg({\n",
        "            'final_best_value': ['mean', 'std', 'min'],\n",
        "            'final_pearson': ['mean', 'std']\n",
        "        }).round(6)\n",
        "        \n",
        "        # 创建可视化图表\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # 1. 基础因子影响\n",
        "        base_means = results_df.groupby('base_exploration_factor')['final_best_value'].mean()\n",
        "        axes[0,0].bar(base_means.index.astype(str), base_means.values)\n",
        "        axes[0,0].set_title('Base Exploration Factor vs Best Value')\n",
        "        axes[0,0].set_xlabel('Base Factor')\n",
        "        axes[0,0].set_ylabel('Mean Best Value')\n",
        "        axes[0,0].set_yscale('log')\n",
        "        \n",
        "        # 2. 指数基数影响\n",
        "        exp_means = results_df.groupby('exploration_exponent_base')['final_best_value'].mean()\n",
        "        axes[0,1].bar(exp_means.index.astype(str), exp_means.values)\n",
        "        axes[0,1].set_title('Exponent Base vs Best Value')\n",
        "        axes[0,1].set_xlabel('Exponent Base')\n",
        "        axes[0,1].set_ylabel('Mean Best Value')\n",
        "        axes[0,1].set_yscale('log')\n",
        "        \n",
        "        # 3. 上限值影响\n",
        "        limit_means = results_df.groupby('exploration_max_limit')['final_best_value'].mean()\n",
        "        axes[0,2].bar(limit_means.index.astype(str), limit_means.values)\n",
        "        axes[0,2].set_title('Max Limit vs Best Value')\n",
        "        axes[0,2].set_xlabel('Max Limit')\n",
        "        axes[0,2].set_ylabel('Mean Best Value')\n",
        "        axes[0,2].set_yscale('log')\n",
        "        \n",
        "        # 4. 皮尔逊相关系数分析\n",
        "        base_pearson = results_df.groupby('base_exploration_factor')['final_pearson'].mean()\n",
        "        axes[1,0].bar(base_pearson.index.astype(str), base_pearson.values)\n",
        "        axes[1,0].set_title('Base Factor vs Pearson Correlation')\n",
        "        axes[1,0].set_xlabel('Base Factor')\n",
        "        axes[1,0].set_ylabel('Mean Pearson Correlation')\n",
        "        \n",
        "        exp_pearson = results_df.groupby('exploration_exponent_base')['final_pearson'].mean()\n",
        "        axes[1,1].bar(exp_pearson.index.astype(str), exp_pearson.values)\n",
        "        axes[1,1].set_title('Exponent Base vs Pearson Correlation')\n",
        "        axes[1,1].set_xlabel('Exponent Base')\n",
        "        axes[1,1].set_ylabel('Mean Pearson Correlation')\n",
        "        \n",
        "        # 5. 散点图：最优值 vs 相关系数\n",
        "        scatter = axes[1,2].scatter(results_df['final_best_value'], results_df['final_pearson'], \n",
        "                                   c=results_df['base_exploration_factor'], cmap='viridis', alpha=0.7)\n",
        "        axes[1,2].set_xlabel('Final Best Value')\n",
        "        axes[1,2].set_ylabel('Final Pearson Correlation')\n",
        "        axes[1,2].set_title('Best Value vs Pearson Correlation')\n",
        "        axes[1,2].set_xscale('log')\n",
        "        plt.colorbar(scatter, ax=axes[1,2], label='Base Factor')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # 保存图表\n",
        "        analysis_plot_path = os.path.join(output_dir, 'parameter_analysis_report.png')\n",
        "        plt.savefig(analysis_plot_path, dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        # 保存文本报告\n",
        "        report_text = f\"\"\"\n",
        "参数扫描实验分析报告\n",
        "===================\n",
        "\n",
        "实验概要:\n",
        "- 总实验数: {len(results_df)}\n",
        "- 最优值范围: {report['summary_stats']['best_value_min']:.4e} - {report['summary_stats']['best_value_max']:.4e}\n",
        "- 平均最优值: {report['summary_stats']['best_value_mean']:.4e} ± {report['summary_stats']['best_value_std']:.4e}\n",
        "- 平均相关系数: {report['summary_stats']['pearson_mean']:.4f} ± {report['summary_stats']['pearson_std']:.4f}\n",
        "\n",
        "按基础探索因子分析:\n",
        "{base_factor_analysis}\n",
        "\n",
        "按指数基数分析:\n",
        "{exp_base_analysis}\n",
        "\n",
        "按上限值分析:\n",
        "{max_limit_analysis}\n",
        "\n",
        "最佳实验:\n",
        "{results_df.loc[results_df['final_best_value'].idxmin()].to_dict()}\n",
        "        \"\"\"\n",
        "        \n",
        "        report_text_path = os.path.join(output_dir, 'parameter_analysis_report.txt')\n",
        "        with open(report_text_path, 'w') as f:\n",
        "            f.write(report_text)\n",
        "        \n",
        "        report['analysis_plot'] = analysis_plot_path\n",
        "        report['report_text'] = report_text_path\n",
        "        \n",
        "        logger.info(f\"分析报告已保存: {report_text_path}\")\n",
        "        logger.info(f\"分析图表已保存: {analysis_plot_path}\")\n",
        "        \n",
        "        return report\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"生成分析报告时出错: {e}\")\n",
        "        return None\n",
        "\n",
        "# 快速单次实验函数\n",
        "def run_quick_experiment(\n",
        "    base_exploration_factor=0.8,\n",
        "    exploration_exponent_base=2.0,\n",
        "    exploration_max_limit=20,\n",
        "    n_iterations=20,\n",
        "    experiment_name=\"quick_test\"\n",
        "):\n",
        "    \"\"\"运行快速单次实验，用于验证特定参数组合\"\"\"\n",
        "    \n",
        "    logger.info(f\"运行快速实验: {experiment_name}\")\n",
        "    logger.info(f\"动态探索因子参数: base={base_exploration_factor}, exp_base={exploration_exponent_base}, max_limit={exploration_max_limit}\")\n",
        "    \n",
        "    result = run_dante_optimization(\n",
        "        # 动态探索因子参数\n",
        "        base_exploration_factor=base_exploration_factor,\n",
        "        exploration_exponent_base=exploration_exponent_base,\n",
        "        exploration_max_limit=exploration_max_limit,\n",
        "        \n",
        "        # 快速测试参数\n",
        "        n_al_iterations=n_iterations,\n",
        "        cnn_training_epochs=30,\n",
        "        initial_data_size=200,\n",
        "        viz_interval=10,\n",
        "        detailed_viz_interval=20,\n",
        "        \n",
        "        # 实验标识\n",
        "        experiment_name=experiment_name\n",
        "    )\n",
        "    \n",
        "    return result\n",
        "\n",
        "logger.info(\"参数调试实验框架加载完成\")\n",
        "print(\"🎯 参数调试实验框架已准备就绪\")\n",
        "print(\"📊 可使用 run_parameter_sweep_experiment() 进行参数网格搜索\")\n",
        "print(\"⚡ 可使用 run_quick_experiment() 进行快速单次测试\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🎯 最终确认：所有超参数已恢复到原项目设置\n",
        "\n",
        "## ✅ 完成的修正项目\n",
        "\n",
        "### 1. 核心超参数恢复\n",
        "- **n_al_iterations**: 50 → **400** (原项目完整迭代数)\n",
        "- **cnn_training_epochs**: 100 → **200** (原项目完整训练轮数)\n",
        "- **cnn_batch_size**: 64 → **32** (原项目批次大小)\n",
        "- **cnn_early_stopping_patience**: 20 → **30** (原项目早停patience)\n",
        "- **detailed_viz_interval**: 25 → **50** (原项目设置)\n",
        "\n",
        "### 2. 动态探索因子公式恢复\n",
        "```python\n",
        "# 原项目验证的最佳公式\n",
        "dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)\n",
        "```\n",
        "- **base_exploration_factor**: 0.8 → **0.2** (原项目验证的最佳基础因子)\n",
        "\n",
        "### 3. P100显卡专门优化\n",
        "- 虚拟GPU内存限制: 15GB (避免OOM)\n",
        "- 线程配置: inter_op=4, intra_op=2 (P100最优)\n",
        "- 混合精度: mixed_float16 (P100支持)\n",
        "- XLA编译优化: 已启用\n",
        "- GPU线程模式: gpu_private\n",
        "- 批归一化优化: 已启用\n",
        "\n",
        "### 4. 性能预期 (P100)\n",
        "- **完整400次迭代**: 8-12小时\n",
        "- **快速20次测试**: 30-60分钟\n",
        "- **GPU内存使用**: 约12-14GB\n",
        "\n",
        "## 🚀 现在可以开始实验\n",
        "\n",
        "所有参数现在与您的原项目完全一致，专门针对Kaggle P100显卡进行了性能优化。您可以放心运行完整的400次迭代实验，模型性能将得到最大化。\n",
        "\n",
        "**建议执行顺序：**\n",
        "1. 先运行 `run_quick_experiment()` 验证环境 (20次迭代)\n",
        "2. 运行完整实验 `run_dante_optimization()` (400次迭代)\n",
        "3. 如需调试，使用参数扫描 `run_parameter_sweep_experiment()`\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 🚀 如何在Kaggle上正确运行此Notebook\n",
        "\n",
        "## ✅ 运行前准备\n",
        "\n",
        "### 1. 环境检查\n",
        "确保您的Kaggle设置为：\n",
        "- **加速器**: GPU P100 (推荐) 或 T4\n",
        "- **Internet**: 开启 (需要下载依赖包)\n",
        "- **持久存储**: 开启 (可选，用于保存长期结果)\n",
        "\n",
        "### 2. 数据集设置\n",
        "确保您已经将Rosenbrock数据集添加到此notebook：\n",
        "- **数据集名称**: `rosenbrock-data-20d-800`\n",
        "- **数据路径**: `/kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw/`\n",
        "- **包含文件**: `Rosenbrock_x_train.npy`, `Rosenbrock_y_train.npy`, `Rosenbrock_x_test.npy`, `Rosenbrock_y_test.npy`\n",
        "\n",
        "## 🏃‍♂️ 推荐运行顺序\n",
        "\n",
        "### 步骤1: 环境验证 (必须)\n",
        "按顺序运行前面的所有cell (Cell 0-23)，确保没有错误\n",
        "\n",
        "### 步骤2: 快速测试 (强烈推荐)\n",
        "```python\n",
        "# 运行20次迭代的快速测试\n",
        "result = run_quick_experiment(\n",
        "    experiment_name=\"kaggle_quick_test\",\n",
        "    n_iterations=20\n",
        ")\n",
        "```\n",
        "\n",
        "### 步骤3: 完整实验 (400次迭代)\n",
        "```python\n",
        "# 运行完整的400次迭代实验\n",
        "result = run_dante_optimization(\n",
        "    experiment_name=\"kaggle_full_experiment_400\",\n",
        "    n_al_iterations=400\n",
        ")\n",
        "```\n",
        "\n",
        "### 步骤4: 参数调试 (可选)\n",
        "```python\n",
        "# 运行参数网格搜索\n",
        "sweep_result = run_parameter_sweep_experiment()\n",
        "```\n",
        "\n",
        "## ⏱️ 时间预期\n",
        "\n",
        "| 实验类型 | 迭代次数 | 预计时间 (P100) | 预计时间 (T4) |\n",
        "|----------|----------|-----------------|---------------|\n",
        "| 快速测试 | 20 | 30-60分钟 | 45-90分钟 |\n",
        "| 完整实验 | 400 | 8-12小时 | 12-18小时 |\n",
        "| 参数扫描 | 多组合 | 2-4小时 | 3-6小时 |\n",
        "\n",
        "## 📊 结果文件\n",
        "\n",
        "运行完成后，在 `/kaggle/working/results/` 目录下会生成：\n",
        "- `experiment_parameters.json` - 实验参数\n",
        "- `final_experiment_summary.json` - 实验总结\n",
        "- 各种可视化图表 (PNG格式)\n",
        "- CSV数据文件\n",
        "\n",
        "## ⚠️ 重要提醒\n",
        "\n",
        "1. **先运行快速测试**: 确保环境正常后再运行完整实验\n",
        "2. **定期检查**: 长时间实验时定期查看日志输出\n",
        "3. **内存监控**: 如果遇到内存不足，可以降低batch_size\n",
        "4. **保存结果**: 重要结果及时下载到本地\n",
        "5. **网络稳定**: 长时间实验确保网络连接稳定\n",
        "\n",
        "## 🔧 故障排除\n",
        "\n",
        "### 常见问题及解决方案\n",
        "\n",
        "1. **GPU内存不足**: 降低 `cnn_batch_size` 从32到16\n",
        "2. **数据加载失败**: 检查数据集是否正确添加到notebook\n",
        "3. **依赖包安装失败**: 确保Internet开启\n",
        "4. **运行时间超限**: 使用Kaggle Pro账户获得更长运行时间\n",
        "\n",
        "准备就绪？开始您的DANTE优化实验吧！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: 测试和使用指南\n",
        "print(\"🚀 DANTE Rosenbrock 优化系统 - Kaggle版本\")\n",
        "print(\"=\"*60)\n",
        "print(\"所有模块已成功加载完成！\")\n",
        "print()\n",
        "\n",
        "print(\"📋 可用的主要功能:\")\n",
        "print(\"1. run_quick_experiment() - 快速单次实验 (推荐先运行)\")\n",
        "print(\"2. run_dante_optimization() - 完整DANTE优化算法\")\n",
        "print(\"3. run_parameter_sweep_experiment() - 参数网格搜索\")\n",
        "print()\n",
        "\n",
        "print(\"🎯 重点调试参数 (动态探索因子公式):\")\n",
        "print(\"dynamic_exploration_factor = base_exploration_factor + 1 * min(exploration_max_limit, exploration_exponent_base ** no_improvement_streak)\")\n",
        "print()\n",
        "print(\"  • base_exploration_factor: 基础因子 (默认0.8)\")\n",
        "print(\"  • exploration_exponent_base: 指数基数 (默认2.0)\")  \n",
        "print(\"  • exploration_max_limit: 上限值 (默认20)\")\n",
        "print()\n",
        "\n",
        "print(\"💻 快速开始示例:\")\n",
        "print()\n",
        "print(\"# 1. 运行快速测试 (20次迭代)\")\n",
        "print(\"result = run_quick_experiment()\")\n",
        "print()\n",
        "print(\"# 2. 调试特定参数组合\")\n",
        "print(\"result = run_quick_experiment(\")\n",
        "print(\"    base_exploration_factor=1.0,\")\n",
        "print(\"    exploration_exponent_base=2.5,\")\n",
        "print(\"    exploration_max_limit=15,\")\n",
        "print(\"    n_iterations=30\")\n",
        "print(\")\")\n",
        "print()\n",
        "print(\"# 3. 运行完整实验 (50次迭代)\")\n",
        "print(\"result = run_dante_optimization(\")\n",
        "print(\"    base_exploration_factor=0.8,\")\n",
        "print(\"    exploration_exponent_base=2.0,\")\n",
        "print(\"    exploration_max_limit=20,\")\n",
        "print(\"    n_al_iterations=50\")\n",
        "print(\")\")\n",
        "print()\n",
        "print(\"# 4. 参数网格搜索 (48种组合)\")\n",
        "print(\"sweep_result = run_parameter_sweep_experiment()\")\n",
        "print()\n",
        "\n",
        "print(\"📊 输出结果说明:\")\n",
        "print(\"  • 最优值趋势图: 优化进展可视化\")\n",
        "print(\"  • 皮尔逊相关系数: 模型质量评估\")\n",
        "print(\"  • DUCB趋势图: NTE搜索分析\")\n",
        "print(\"  • 维度分布图: 最优解特征分析\")\n",
        "print(\"  • 实验参数和总结: JSON格式保存\")\n",
        "print()\n",
        "\n",
        "print(\"🔧 系统状态检查:\")\n",
        "print(f\"  • GPU可用: {'✅' if gpu_available else '❌'}\")\n",
        "print(f\"  • TensorFlow版本: {tf.__version__}\")\n",
        "print(f\"  • 数据路径: /kaggle/input/rosenbrock-data-20d-800/rosenbrock_data_raw\")\n",
        "print(f\"  • 结果路径: /kaggle/working/results\")\n",
        "print()\n",
        "\n",
        "print(\"⚠️  重要提醒:\")\n",
        "print(\"  • 确保已上传Rosenbrock数据集到Kaggle\")\n",
        "print(\"  • P100 GPU推荐，16GB内存\")\n",
        "print(\"  • 完整400次迭代需要8-12小时\")\n",
        "print(\"  • 先运行快速测试验证环境\")\n",
        "print()\n",
        "\n",
        "print(\"🎓 实验建议:\")\n",
        "print(\"  1. 先运行 run_quick_experiment() 验证环境\")\n",
        "print(\"  2. 尝试不同的动态探索因子参数\")\n",
        "print(\"  3. 观察DUCB趋势图分析搜索行为\")\n",
        "print(\"  4. 使用参数扫描找到最优组合\")\n",
        "print(\"  5. 最后运行完整400次迭代实验\")\n",
        "print()\n",
        "\n",
        "print(\"准备开始优化实验！🚀\")\n",
        "\n",
        "# 运行环境验证\n",
        "try:\n",
        "    # 验证所有核心组件\n",
        "    test_rosenbrock_val = rosenbrock_function(np.ones(20))\n",
        "    test_log_transform = safe_power10(np.array([1.0, 2.0]))\n",
        "    print(f\"✅ 环境验证通过 - Rosenbrock(1^20) = {test_rosenbrock_val:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 环境验证失败: {e}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
