# =============================================================================
# Cell 8: ä¸»è®­ç»ƒå¾ªç¯ - DANTEä¼˜åŒ–ä¸dynamic_exploration_factorè°ƒè¯•
# =============================================================================

print("ğŸš€ å¼€å§‹DANTEä¸»è®­ç»ƒå¾ªç¯...")

def main_training_loop():
    """DANTEä¸»è®­ç»ƒå¾ªç¯ï¼Œä¸“æ³¨äºdynamic_exploration_factorè°ƒè¯•"""
    
    # === å‚æ•°é…ç½® ===
    ROSENBROCK_DIMENSION = 20
    domain_min, domain_max = -2.048, 2.048
    ROSENBROCK_DOMAIN = [(domain_min, domain_max)] * ROSENBROCK_DIMENSION
    
    # è®­ç»ƒå‚æ•°
    INITIAL_DATA_SIZE = 800
    N_AL_ITERATIONS = 400      # å¯è°ƒæ•´ä¸ºæ›´å°å€¼å¦‚100è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    CNN_TRAINING_EPOCHS = 200
    CNN_EARLY_STOPPING_PATIENCE = 30
    CNN_VALIDATION_SPLIT = 0.2
    CNN_BATCH_SIZE = 32
    CNN_DROPOUT_RATE = 0.2
    CNN_LEARNING_RATE = 1e-3
    
    # NTEå‚æ•°
    NTE_C0 = 1
    NTE_ROLLOUT_ROUNDS = 100
    VALIDATION_BATCH_SIZE = 20
    
    # å¯è§†åŒ–å‚æ•°
    viz_interval = 10  # æ¯10æ¬¡è¿­ä»£å¯è§†åŒ–ä¸€æ¬¡
    
    print(f"ğŸ“‹ DANTEé…ç½®:")
    print(f"   ç»´åº¦: {ROSENBROCK_DIMENSION}D")
    print(f"   åˆå§‹æ ·æœ¬: {INITIAL_DATA_SIZE}")
    print(f"   ALè¿­ä»£: {N_AL_ITERATIONS}")
    print(f"   CNN epochs: {CNN_TRAINING_EPOCHS}")
    print(f"   å¯è§†åŒ–é—´éš”: {viz_interval}")
    
    # === æ•°æ®åŠ è½½ ===
    try:
        print("\nğŸ“ åŠ è½½åˆå§‹æ•°æ®...")
        x_train_path = os.path.join(DATA_PATH, "Rosenbrock_x_train.npy")
        y_train_path = os.path.join(DATA_PATH, "Rosenbrock_y_train.npy")
        x_test_path = os.path.join(DATA_PATH, "Rosenbrock_x_test.npy")
        y_test_path = os.path.join(DATA_PATH, "Rosenbrock_y_test.npy")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        initial_X = np.load(x_train_path)
        initial_y = np.load(y_train_path)
        if initial_y.ndim == 1:
            initial_y = initial_y.reshape(-1, 1)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        X_test = np.load(x_test_path) if os.path.exists(x_test_path) else None
        y_test = np.load(y_test_path) if os.path.exists(y_test_path) else None
        if y_test is not None and y_test.ndim == 1:
            y_test = y_test.reshape(-1, 1)
            
        print(f"âœ… è®­ç»ƒæ•°æ®: {initial_X.shape}, {initial_y.shape}")
        if X_test is not None:
            print(f"âœ… æµ‹è¯•æ•°æ®: {X_test.shape}, {y_test.shape}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # === åˆå§‹åŒ–ç»„ä»¶ ===
    print("\nğŸ”§ åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
    
    # åˆ›å»ºç»“æœç›®å½•
    results_dir = create_results_directory(RESULTS_PATH)
    
    # æ•°æ®ç®¡ç†å™¨
    data_manager = DataManager(
        initial_X=initial_X,
        initial_y=initial_y,
        dimension=ROSENBROCK_DIMENSION
    )
    
    # CNNæ¨¡å‹
    cnn_model = CNN1DSurrogate(input_dim=ROSENBROCK_DIMENSION)
    
    # å†å²è®°å½•
    history_iterations = []
    history_best_y = []
    history_pearson_all = []
    history_pearson_best_samples = []
    history_nte_candidates_count = []
    search_boundaries_history = []
    max_ducb_history = []
    cnn_performance_history = []
    
    # è¿ç»­æ— æ”¹è¿›è¿½è¸ª
    no_improvement_streak = 0
    previous_best_y = float('inf')
    previous_iteration_samples_x = None
    
    print(f"âœ… åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {data_manager.num_samples}")
    
    # === NTEæ›¿ä»£æ¨¡å‹åŒ…è£…å™¨ ===
    class NTESurrogateWrapper:
        def __init__(self, surrogate_model, data_manager_instance):
            self.surrogate_model = surrogate_model
            self.dm = data_manager_instance
            
        def predict(self, X_batch_orig: np.ndarray) -> np.ndarray:
            if X_batch_orig.shape[0] == 0:
                return np.array([])
            if X_batch_orig.ndim == 1:
                X_batch_orig = X_batch_orig.reshape(1, -1)
                
            try:
                pred_log = self.surrogate_model.predict(X_batch_orig)
                return pred_log[0] if X_batch_orig.shape[0] == 1 else pred_log
            except Exception as e:
                logger.error(f"NTEé¢„æµ‹é”™è¯¯: {e}")
                return np.ones(X_batch_orig.shape[0]) * np.log10(1e6 + 1.0)
    
    # === ä¸»è®­ç»ƒå¾ªç¯ ===
    print(f"\nğŸ¯ å¼€å§‹{N_AL_ITERATIONS}æ¬¡ä¸»åŠ¨å­¦ä¹ è¿­ä»£...")
    print("=" * 80)
    
    for al_iteration in range(N_AL_ITERATIONS):
        current_iteration_number = al_iteration + 1
        history_iterations.append(current_iteration_number)
        
        # å¯è§†åŒ–å†³ç­–
        should_generate_plots = (current_iteration_number % viz_interval == 0) or (current_iteration_number == N_AL_ITERATIONS)
        
        print(f"\n{'='*20} è¿­ä»£ {current_iteration_number}/{N_AL_ITERATIONS} {'='*20}")
        
        # === CNNè®­ç»ƒ ===
        logger.info(f"è®­ç»ƒCNNæ¨¡å‹ (è¿­ä»£ {current_iteration_number})")
        training_start_time = time.time()
        
        training_history = cnn_model.train(
            data_manager=data_manager,
            epochs=CNN_TRAINING_EPOCHS,
            batch_size=CNN_BATCH_SIZE,
            validation_split=CNN_VALIDATION_SPLIT,
            patience=CNN_EARLY_STOPPING_PATIENCE,
            verbose=0
        )
        
        training_time = time.time() - training_start_time
        logger.info(f"CNNè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.1f}ç§’")
        
        # === CNNæ€§èƒ½è¯„ä¼° ===
        try:
            # è®­ç»ƒé›†æ€§èƒ½
            X_train = data_manager.get_all_x_orig()
            y_train_orig = data_manager.get_all_y_orig().flatten()
            train_predictions_log = cnn_model.predict(X_train)
            train_predictions_orig = safe_power10(train_predictions_log)
            train_mse = np.mean((train_predictions_orig - y_train_orig) ** 2)
            train_rmse = np.sqrt(train_mse)
            
            # æµ‹è¯•é›†æ€§èƒ½
            if X_test is not None:
                test_predictions_log = cnn_model.predict(X_test)
                test_predictions_orig = safe_power10(test_predictions_log)
                test_ground_truth = y_test.flatten()
                test_mse = np.mean((test_predictions_orig - test_ground_truth) ** 2)
                test_rmse = np.sqrt(test_mse)
                test_pearson, _ = pearsonr(test_predictions_orig, test_ground_truth)
                test_pearson = 0.0 if np.isnan(test_pearson) else test_pearson
            else:
                test_rmse, test_pearson = 0.0, 0.0
            
            history_pearson_all.append(test_pearson)
            
            # æœ€ä½³æ ·æœ¬æ€§èƒ½
            best_pearson, min_y_best, max_y_best = calculate_pearson_on_best_samples(cnn_model, data_manager)
            history_pearson_best_samples.append(best_pearson)
            
            logger.info(f"CNNæ€§èƒ½ - è®­ç»ƒRMSE: {train_rmse:.4e}, æµ‹è¯•Pearson: {test_pearson:.4f}, æœ€ä½³æ ·æœ¬Pearson: {best_pearson:.4f}")
            
        except Exception as e:
            logger.error(f"CNNæ€§èƒ½è¯„ä¼°å¤±è´¥: {e}")
            history_pearson_all.append(0.0)
            history_pearson_best_samples.append(0.0)
        
        # === å…³é”®éƒ¨åˆ†ï¼šåŠ¨æ€æ¢ç´¢å› å­è®¡ç®—ä¸NTEæœç´¢ ===
        logger.info("å¼€å§‹NTEæœç´¢...")
        
        # è®¡ç®—åŠ¨æ€æ¢ç´¢å› å­
        dynamic_exploration_factor = 0.2 + 1 * min(20, 2 ** no_improvement_streak)
        logger.info(f"ğŸ¯ åŠ¨æ€æ¢ç´¢å› å­: {dynamic_exploration_factor:.2f} (è¿ç»­æ— æ”¹è¿›: {no_improvement_streak}æ¬¡)")
        
        # NTEæœç´¢
        try:
            nte_model_wrapper = NTESurrogateWrapper(cnn_model, data_manager)
            nte_searcher = NTESearcher(
                dimension=ROSENBROCK_DIMENSION,
                domain=ROSENBROCK_DOMAIN,
                c0=NTE_C0,
                rollout_rounds=NTE_ROLLOUT_ROUNDS,
                validation_batch_size=VALIDATION_BATCH_SIZE
            )
            
            current_best_x_orig, current_best_y_overall = data_manager.get_current_best()
            
            # æ‰§è¡ŒNTEæœç´¢
            search_start_time = time.time()
            nte_search_result = nte_searcher.search(
                surrogate_model=nte_model_wrapper,
                current_best_state=current_best_x_orig,
                min_y_observed=current_best_y_overall,
                dynamic_exploration_factor=dynamic_exploration_factor,
                all_states=data_manager.X_data,
                all_values=data_manager.get_all_y_orig(),
                previous_iteration_samples=previous_iteration_samples_x,
                current_iteration=al_iteration,
                return_ducb_trends=True
            )
            
            nte_selected_samples_x, ducb_trends_data = nte_search_result
            search_time = time.time() - search_start_time
            
            if isinstance(nte_selected_samples_x, list):
                nte_selected_samples_x = np.array(nte_selected_samples_x)
            
            logger.info(f"NTEæœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.1f}ç§’ï¼Œæ‰¾åˆ° {len(nte_selected_samples_x)} ä¸ªå€™é€‰ç‚¹")
            history_nte_candidates_count.append(len(nte_selected_samples_x))
            
            # === å€™é€‰æ ·æœ¬è¯„ä¼° ===
            if len(nte_selected_samples_x) > 0:
                logger.info(f"è¯„ä¼° {len(nte_selected_samples_x)} ä¸ªæ–°å€™é€‰æ ·æœ¬...")
                nte_selected_samples_y_true = np.array([
                    rosenbrock_evaluator(x) for x in nte_selected_samples_x
                ]).reshape(-1, 1)
                
                # æ·»åŠ åˆ°æ•°æ®é›†
                data_manager.add_samples(nte_selected_samples_x, nte_selected_samples_y_true, re_normalize=True)
                previous_iteration_samples_x = nte_selected_samples_x.copy()
                
                # æ›´æ–°æœ€ä½³å€¼
                current_best_x_orig, current_best_y_overall = data_manager.get_current_best()
                history_best_y.append(current_best_y_overall)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
                iteration_improved = current_best_y_overall < previous_best_y
                if iteration_improved:
                    logger.info(f"ğŸ‰ å‘ç°æ›´ä¼˜è§£: {current_best_y_overall:.6e} (æå‡: {previous_best_y - current_best_y_overall:.6e})")
                    no_improvement_streak = 0
                    previous_best_y = current_best_y_overall
                else:
                    no_improvement_streak += 1
                    logger.info(f"æœ¬è½®æ— æ”¹è¿›ï¼Œè¿ç»­æ— æ”¹è¿›æ¬¡æ•°: {no_improvement_streak}")
                
                # å¯è§†åŒ–
                if should_generate_plots:
                    try:
                        # DUCBè¶‹åŠ¿å›¾
                        plot_nte_ducb_trends(
                            ducb_trends_data=ducb_trends_data,
                            output_dir=results_dir,
                            iteration=current_iteration_number,
                            dynamic_exploration_factor=dynamic_exploration_factor,
                            new_global_best_value=current_best_y_overall if iteration_improved else None
                        )
                        
                        # å…¨å±€æœ€å°å€¼è¶‹åŠ¿
                        plot_global_min_value_trend(
                            history_iterations[:len(history_best_y)], 
                            history_best_y, 
                            results_dir,
                            filename=f"global_min_iter_{current_iteration_number}.png"
                        )
                        
                        # ä¿å­˜æ ·æœ¬
                        csv_path = os.path.join(results_dir, f"iteration_{current_iteration_number}_samples.csv")
                        data_dict = {}
                        for i in range(nte_selected_samples_x.shape[1]):
                            data_dict[f'x{i+1}'] = nte_selected_samples_x[:, i]
                        data_dict['y_true'] = nte_selected_samples_y_true.flatten()
                        data_dict['dynamic_factor'] = [dynamic_exploration_factor] * len(nte_selected_samples_y_true)
                        pd.DataFrame(data_dict).to_csv(csv_path, index=False)
                        
                    except Exception as e:
                        logger.error(f"å¯è§†åŒ–å¤±è´¥: {e}")
            else:
                history_nte_candidates_count.append(0)
                history_best_y.append(history_best_y[-1] if history_best_y else float('inf'))
                no_improvement_streak += 1
                
        except Exception as e:
            logger.error(f"NTEæœç´¢å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            history_nte_candidates_count.append(0)
            no_improvement_streak += 1
        
        # === è¿›åº¦æŠ¥å‘Š ===
        current_best_x_orig, current_best_y_overall = data_manager.get_current_best()
        logger.info(f"è¿­ä»£ {current_iteration_number} å®Œæˆ:")
        logger.info(f"  ğŸ“Š æ•°æ®é›†å¤§å°: {data_manager.num_samples}")
        logger.info(f"  ğŸ¯ å½“å‰æœ€ä¼˜å€¼: {current_best_y_overall:.6e}")
        logger.info(f"  ğŸ” åŠ¨æ€æ¢ç´¢å› å­: {dynamic_exploration_factor:.2f}")
        logger.info(f"  â±ï¸  è¿ç»­æ— æ”¹è¿›: {no_improvement_streak}æ¬¡")
        
        # å†…å­˜æ¸…ç†
        if current_iteration_number % 10 == 0:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    # === è®­ç»ƒå®Œæˆå¤„ç† ===
    print("\n" + "="*80)
    print("ğŸ‰ DANTEè®­ç»ƒå®Œæˆï¼")
    print("="*80)
    
    final_best_x, final_best_y = data_manager.get_current_best()
    print(f"ğŸ† æœ€ç»ˆç»“æœ:")
    print(f"   æœ€ä¼˜å‡½æ•°å€¼: {final_best_y:.6e}")
    print(f"   æ€»æ ·æœ¬æ•°: {data_manager.num_samples}")
    print(f"   æœ€ç»ˆæ•°æ®é›†å¤§å°: {INITIAL_DATA_SIZE + N_AL_ITERATIONS * VALIDATION_BATCH_SIZE}")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    try:
        # æ•°æ®ç®¡ç†å™¨çŠ¶æ€
        final_data_path = os.path.join(results_dir, "final_data_manager_state.npz")
        data_manager.save_data(final_data_path)
        
        # æ€§èƒ½å†å²
        final_history = {
            'iterations': history_iterations,
            'best_y_values': history_best_y,
            'pearson_all': history_pearson_all,
            'pearson_best_samples': history_pearson_best_samples,
            'nte_candidates_count': history_nte_candidates_count
        }
        history_df = pd.DataFrame(final_history)
        history_csv_path = os.path.join(results_dir, "training_history.csv")
        history_df.to_csv(history_csv_path, index=False)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {results_dir}")
        
        # æœ€ç»ˆå›¾è¡¨
        plot_global_min_value_trend(
            history_iterations[:len(history_best_y)], 
            history_best_y, 
            results_dir,
            filename="global_min_value_trend_final.png"
        )
        
    except Exception as e:
        logger.error(f"ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {e}")
    
    return results_dir, final_best_y, data_manager.num_samples

# è®¡ç®—æœ€ä½³æ ·æœ¬Pearsonç›¸å…³ç³»æ•°çš„è¾…åŠ©å‡½æ•°
def calculate_pearson_on_best_samples(model, data_manager, n_best=60):
    """è®¡ç®—æœ€ä½³æ ·æœ¬çš„Pearsonç›¸å…³ç³»æ•°"""
    try:
        all_X = data_manager.get_all_x_orig()
        all_y_orig = data_manager.get_all_y_orig()
        
        if all_X.shape[0] < 2:
            return 0.0, float('nan'), float('nan')
        
        sorted_indices = np.argsort(all_y_orig.flatten())
        sorted_X = all_X[sorted_indices[:n_best]]
        sorted_y_orig = all_y_orig[sorted_indices[:n_best]]
        
        min_y_orig = np.min(sorted_y_orig)
        max_y_orig = np.max(sorted_y_orig)
        
        predictions_log = model.predict(sorted_X)
        predictions_orig = safe_power10(predictions_log)
        true_values_orig = sorted_y_orig.flatten()
        
        if len(predictions_orig) < 2 or np.var(predictions_orig) == 0 or np.var(true_values_orig) == 0:
            return 0.0, min_y_orig, max_y_orig
        
        correlation, _ = pearsonr(predictions_orig, true_values_orig)
        return (correlation if not np.isnan(correlation) else 0.0), min_y_orig, max_y_orig
        
    except Exception as e:
        logger.error(f"è®¡ç®—æœ€ä½³æ ·æœ¬Pearsonå¤±è´¥: {e}")
        return 0.0, float('nan'), float('nan')

# === æ‰§è¡Œé€‰é¡¹ ===
print("\nğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ...")
print("âš¡ å¦‚éœ€å¿«é€Ÿæµ‹è¯•ï¼Œå¯ä»¥ä¿®æ”¹ N_AL_ITERATIONS = 50")
print("ğŸ”¥ å¦‚éœ€å®Œæ•´å®éªŒï¼Œä¿æŒ N_AL_ITERATIONS = 400")

# å¼€å§‹è®­ç»ƒ
results_directory, best_value, total_samples = main_training_loop()

print(f"\nğŸ¯ è®­ç»ƒå®Œæˆæ€»ç»“:")
print(f"   ç»“æœç›®å½•: {results_directory}")
print(f"   æœ€ä¼˜å€¼: {best_value:.6e}")
print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
print(f"   è¿è¡Œæ—¶é—´: {datetime.now()}") 